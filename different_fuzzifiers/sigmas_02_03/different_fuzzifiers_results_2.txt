Crisp_2dim, number of iterations: 10

Parameters info
Tradeoffs parameter: [  0.1   1.   10.  100. ]
Gaussian kernel sigmas: [0.2   0.225 0.25  0.275 0.3  ]
Number of principal dimension considerated: 2
Dataset length: 150

Clusterization info
Minimum size of a cluster: in perc: 0.01, in int: 2
Type of mu: Binary Muzzifier
Fuzzifier: Crisp
Cluster to label info
Force the number of cluster to be the number of different labels: False
Force the labels to be always represent by a cluster: False

Validation info
Conflict resolution: random


RESULTS

On TEST set

Accuracy on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 0.275): 0.333333333333
(1, 100.0, 0.2): 0.2
(2, 100.0, 0.2): 0.8
(3, 1.0, 0.25): 0.466666666667
(4, 1.0, 0.225): 0.466666666667
(5, 100.0, 0.25): 0.333333333333
(6, 1.0, 0.275): 0.533333333333
(7, 10.0, 0.3): 0.4
(8, 1.0, 0.25): 0.133333333333
(9, 10.0, 0.3): 0.533333333333

Loss on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 0.275): 0.666666666667
(1, 100.0, 0.2): 0.8
(2, 100.0, 0.2): 0.2
(3, 1.0, 0.25): 0.533333333333
(4, 1.0, 0.225): 0.533333333333
(5, 100.0, 0.25): 0.666666666667
(6, 1.0, 0.275): 0.466666666667
(7, 10.0, 0.3): 0.6
(8, 1.0, 0.25): 0.866666666667
(9, 10.0, 0.3): 0.466666666667

Accuracy mean :0.4200000000000001
Std deviation :0.17900962109463403
Loss mean :0.5799999999999998
Std deviation :0.17900962109463403

On TRAINING set

Accuracy on training set for every iteration (n_of_the_iter, c, sigma):
(0, 1.0, 0.275): 0.592592592593
(1, 100.0, 0.2): 0.348148148148
(2, 100.0, 0.2): 0.659259259259
(3, 1.0, 0.25): 0.525925925926
(4, 1.0, 0.225): 0.540740740741
(5, 100.0, 0.25): 0.540740740741
(6, 1.0, 0.275): 0.696296296296
(7, 10.0, 0.3): 0.607407407407
(8, 1.0, 0.25): 0.659259259259
(9, 10.0, 0.3): 0.585185185185
Loss on training set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 0.275): 0.407407407407
(1, 100.0, 0.2): 0.651851851852
(2, 100.0, 0.2): 0.340740740741
(3, 1.0, 0.25): 0.474074074074
(4, 1.0, 0.225): 0.459259259259
(5, 100.0, 0.25): 0.459259259259
(6, 1.0, 0.275): 0.303703703704
(7, 10.0, 0.3): 0.392592592593
(8, 1.0, 0.25): 0.340740740741
(9, 10.0, 0.3): 0.414814814815

Accuracy mean :0.5755555555555556
Std deviation :0.09305366683176307
Loss mean :0.42444444444444446
Std deviation :0.09305366683176308

On ALL the sets

Accuracy on all the set for every iteration, for every couple c,sigma (n_of_the_iter, c, sigma):
(0, 0.1, 0.275): 0.6
(0, 0.1, 0.3): 0.466666666667
(0, 1.0, 0.2): 0.666666666667
(0, 1.0, 0.225): 0.466666666667
(0, 1.0, 0.25): 0.4
(0, 1.0, 0.275): 0.733333333333
(0, 1.0, 0.3): 0.6
(0, 10.0, 0.2): 0.4
(0, 10.0, 0.225): 0.6
(0, 10.0, 0.25): 0.466666666667
(0, 10.0, 0.275): 0.6
(0, 10.0, 0.3): 0.666666666667
(0, 100.0, 0.2): 0.533333333333
(0, 100.0, 0.225): 0.6
(0, 100.0, 0.25): 0.4
(0, 100.0, 0.275): 0.666666666667
(0, 100.0, 0.3): 0.6
(1, 1.0, 0.2): 0.266666666667
(1, 1.0, 0.225): 0.466666666667
(1, 1.0, 0.25): 0.266666666667
(1, 1.0, 0.275): 0.6
(1, 1.0, 0.3): 0.4
(1, 10.0, 0.2): 0.266666666667
(1, 10.0, 0.225): 0.333333333333
(1, 10.0, 0.25): 0.333333333333
(1, 10.0, 0.275): 0.4
(1, 10.0, 0.3): 0.533333333333
(1, 100.0, 0.2): 0.733333333333
(1, 100.0, 0.225): 0.6
(1, 100.0, 0.25): 0.4
(1, 100.0, 0.275): 0.466666666667
(1, 100.0, 0.3): 0.266666666667
(2, 1.0, 0.2): 0.333333333333
(2, 1.0, 0.225): 0.6
(2, 1.0, 0.25): 0.4
(2, 1.0, 0.275): 0.666666666667
(2, 1.0, 0.3): 0.4
(2, 10.0, 0.2): 0.466666666667
(2, 10.0, 0.225): 0.333333333333
(2, 10.0, 0.25): 0.4
(2, 10.0, 0.275): 0.533333333333
(2, 10.0, 0.3): 0.266666666667
(2, 100.0, 0.2): 0.733333333333
(2, 100.0, 0.225): 0.6
(2, 100.0, 0.25): 0.4
(2, 100.0, 0.275): 0.666666666667
(2, 100.0, 0.3): 0.666666666667
(3, 1.0, 0.225): 0.466666666667
(3, 1.0, 0.25): 0.666666666667
(3, 1.0, 0.275): 0.533333333333
(3, 1.0, 0.3): 0.4
(3, 10.0, 0.225): 0.533333333333
(3, 10.0, 0.25): 0.6
(3, 10.0, 0.275): 0.466666666667
(3, 10.0, 0.3): 0.466666666667
(3, 100.0, 0.2): 0.466666666667
(3, 100.0, 0.225): 0.333333333333
(3, 100.0, 0.25): 0.466666666667
(3, 100.0, 0.275): 0.4
(3, 100.0, 0.3): 0.333333333333
(4, 1.0, 0.2): 0.533333333333
(4, 1.0, 0.225): 0.666666666667
(4, 1.0, 0.25): 0.4
(4, 1.0, 0.275): 0.466666666667
(4, 1.0, 0.3): 0.333333333333
(4, 10.0, 0.2): 0.333333333333
(4, 10.0, 0.225): 0.4
(4, 10.0, 0.25): 0.133333333333
(4, 10.0, 0.275): 0.4
(4, 10.0, 0.3): 0.2
(4, 100.0, 0.2): 0.4
(4, 100.0, 0.225): 0.6
(4, 100.0, 0.25): 0.133333333333
(4, 100.0, 0.275): 0.4
(4, 100.0, 0.3): 0.266666666667
(5, 1.0, 0.2): 0.4
(5, 1.0, 0.225): 0.333333333333
(5, 1.0, 0.25): 0.4
(5, 1.0, 0.275): 0.466666666667
(5, 1.0, 0.3): 0.533333333333
(5, 10.0, 0.2): 0.533333333333
(5, 10.0, 0.225): 0.466666666667
(5, 10.0, 0.25): 0.4
(5, 10.0, 0.275): 0.466666666667
(5, 10.0, 0.3): 0.466666666667
(5, 100.0, 0.2): 0.4
(5, 100.0, 0.225): 0.266666666667
(5, 100.0, 0.25): 0.6
(5, 100.0, 0.275): 0.333333333333
(5, 100.0, 0.3): 0.6
(6, 1.0, 0.2): 0.333333333333
(6, 1.0, 0.225): 0.2
(6, 1.0, 0.25): 0.533333333333
(6, 1.0, 0.275): 0.6
(6, 1.0, 0.3): 0.466666666667
(6, 10.0, 0.2): 0.333333333333
(6, 10.0, 0.225): 0.333333333333
(6, 10.0, 0.25): 0.266666666667
(6, 10.0, 0.275): 0.4
(6, 10.0, 0.3): 0.533333333333
(6, 100.0, 0.2): 0.333333333333
(6, 100.0, 0.225): 0.466666666667
(6, 100.0, 0.25): 0.533333333333
(6, 100.0, 0.275): 0.4
(6, 100.0, 0.3): 0.4
(7, 1.0, 0.2): 0.6
(7, 1.0, 0.225): 0.466666666667
(7, 1.0, 0.25): 0.533333333333
(7, 1.0, 0.275): 0.466666666667
(7, 1.0, 0.3): 0.6
(7, 10.0, 0.2): 0.466666666667
(7, 10.0, 0.225): 0.333333333333
(7, 10.0, 0.25): 0.4
(7, 10.0, 0.275): 0.466666666667
(7, 10.0, 0.3): 0.666666666667
(7, 100.0, 0.2): 0.533333333333
(7, 100.0, 0.225): 0.466666666667
(7, 100.0, 0.25): 0.333333333333
(7, 100.0, 0.275): 0.466666666667
(7, 100.0, 0.3): 0.533333333333
(8, 0.1, 0.3): 0.266666666667
(8, 1.0, 0.2): 0.4
(8, 1.0, 0.225): 0.466666666667
(8, 1.0, 0.25): 0.533333333333
(8, 1.0, 0.275): 0.333333333333
(8, 1.0, 0.3): 0.266666666667
(8, 10.0, 0.2): 0.266666666667
(8, 10.0, 0.225): 0.466666666667
(8, 10.0, 0.25): 0.133333333333
(8, 10.0, 0.275): 0.266666666667
(8, 10.0, 0.3): 0.2
(8, 100.0, 0.2): 0.533333333333
(8, 100.0, 0.225): 0.533333333333
(8, 100.0, 0.25): 0.266666666667
(8, 100.0, 0.275): 0.333333333333
(8, 100.0, 0.3): 0.333333333333
(9, 1.0, 0.2): 0.666666666667
(9, 1.0, 0.225): 0.533333333333
(9, 1.0, 0.25): 0.6
(9, 1.0, 0.275): 0.8
(9, 1.0, 0.3): 0.6
(9, 10.0, 0.2): 0.466666666667
(9, 10.0, 0.225): 0.666666666667
(9, 10.0, 0.25): 0.733333333333
(9, 10.0, 0.275): 0.533333333333
(9, 10.0, 0.3): 0.8
(9, 100.0, 0.2): 0.666666666667
(9, 100.0, 0.225): 0.6
(9, 100.0, 0.25): 0.733333333333
(9, 100.0, 0.275): 0.666666666667
(9, 100.0, 0.3): 0.6

Loss on all the set for every iteration, for every couple c,sigma (n_of_the_iter, best_c, best_sigma):
(0, 0.1, 0.275): 0.4
(0, 0.1, 0.3): 0.533333333333
(0, 1.0, 0.2): 0.333333333333
(0, 1.0, 0.225): 0.533333333333
(0, 1.0, 0.25): 0.6
(0, 1.0, 0.275): 0.266666666667
(0, 1.0, 0.3): 0.4
(0, 10.0, 0.2): 0.6
(0, 10.0, 0.225): 0.4
(0, 10.0, 0.25): 0.533333333333
(0, 10.0, 0.275): 0.4
(0, 10.0, 0.3): 0.333333333333
(0, 100.0, 0.2): 0.466666666667
(0, 100.0, 0.225): 0.4
(0, 100.0, 0.25): 0.6
(0, 100.0, 0.275): 0.333333333333
(0, 100.0, 0.3): 0.4
(1, 1.0, 0.2): 0.733333333333
(1, 1.0, 0.225): 0.533333333333
(1, 1.0, 0.25): 0.733333333333
(1, 1.0, 0.275): 0.4
(1, 1.0, 0.3): 0.6
(1, 10.0, 0.2): 0.733333333333
(1, 10.0, 0.225): 0.666666666667
(1, 10.0, 0.25): 0.666666666667
(1, 10.0, 0.275): 0.6
(1, 10.0, 0.3): 0.466666666667
(1, 100.0, 0.2): 0.266666666667
(1, 100.0, 0.225): 0.4
(1, 100.0, 0.25): 0.6
(1, 100.0, 0.275): 0.533333333333
(1, 100.0, 0.3): 0.733333333333
(2, 1.0, 0.2): 0.666666666667
(2, 1.0, 0.225): 0.4
(2, 1.0, 0.25): 0.6
(2, 1.0, 0.275): 0.333333333333
(2, 1.0, 0.3): 0.6
(2, 10.0, 0.2): 0.533333333333
(2, 10.0, 0.225): 0.666666666667
(2, 10.0, 0.25): 0.6
(2, 10.0, 0.275): 0.466666666667
(2, 10.0, 0.3): 0.733333333333
(2, 100.0, 0.2): 0.266666666667
(2, 100.0, 0.225): 0.4
(2, 100.0, 0.25): 0.6
(2, 100.0, 0.275): 0.333333333333
(2, 100.0, 0.3): 0.333333333333
(3, 1.0, 0.225): 0.533333333333
(3, 1.0, 0.25): 0.333333333333
(3, 1.0, 0.275): 0.466666666667
(3, 1.0, 0.3): 0.6
(3, 10.0, 0.225): 0.466666666667
(3, 10.0, 0.25): 0.4
(3, 10.0, 0.275): 0.533333333333
(3, 10.0, 0.3): 0.533333333333
(3, 100.0, 0.2): 0.533333333333
(3, 100.0, 0.225): 0.666666666667
(3, 100.0, 0.25): 0.533333333333
(3, 100.0, 0.275): 0.6
(3, 100.0, 0.3): 0.666666666667
(4, 1.0, 0.2): 0.466666666667
(4, 1.0, 0.225): 0.333333333333
(4, 1.0, 0.25): 0.6
(4, 1.0, 0.275): 0.533333333333
(4, 1.0, 0.3): 0.666666666667
(4, 10.0, 0.2): 0.666666666667
(4, 10.0, 0.225): 0.6
(4, 10.0, 0.25): 0.866666666667
(4, 10.0, 0.275): 0.6
(4, 10.0, 0.3): 0.8
(4, 100.0, 0.2): 0.6
(4, 100.0, 0.225): 0.4
(4, 100.0, 0.25): 0.866666666667
(4, 100.0, 0.275): 0.6
(4, 100.0, 0.3): 0.733333333333
(5, 1.0, 0.2): 0.6
(5, 1.0, 0.225): 0.666666666667
(5, 1.0, 0.25): 0.6
(5, 1.0, 0.275): 0.533333333333
(5, 1.0, 0.3): 0.466666666667
(5, 10.0, 0.2): 0.466666666667
(5, 10.0, 0.225): 0.533333333333
(5, 10.0, 0.25): 0.6
(5, 10.0, 0.275): 0.533333333333
(5, 10.0, 0.3): 0.533333333333
(5, 100.0, 0.2): 0.6
(5, 100.0, 0.225): 0.733333333333
(5, 100.0, 0.25): 0.4
(5, 100.0, 0.275): 0.666666666667
(5, 100.0, 0.3): 0.4
(6, 1.0, 0.2): 0.666666666667
(6, 1.0, 0.225): 0.8
(6, 1.0, 0.25): 0.466666666667
(6, 1.0, 0.275): 0.4
(6, 1.0, 0.3): 0.533333333333
(6, 10.0, 0.2): 0.666666666667
(6, 10.0, 0.225): 0.666666666667
(6, 10.0, 0.25): 0.733333333333
(6, 10.0, 0.275): 0.6
(6, 10.0, 0.3): 0.466666666667
(6, 100.0, 0.2): 0.666666666667
(6, 100.0, 0.225): 0.533333333333
(6, 100.0, 0.25): 0.466666666667
(6, 100.0, 0.275): 0.6
(6, 100.0, 0.3): 0.6
(7, 1.0, 0.2): 0.4
(7, 1.0, 0.225): 0.533333333333
(7, 1.0, 0.25): 0.466666666667
(7, 1.0, 0.275): 0.533333333333
(7, 1.0, 0.3): 0.4
(7, 10.0, 0.2): 0.533333333333
(7, 10.0, 0.225): 0.666666666667
(7, 10.0, 0.25): 0.6
(7, 10.0, 0.275): 0.533333333333
(7, 10.0, 0.3): 0.333333333333
(7, 100.0, 0.2): 0.466666666667
(7, 100.0, 0.225): 0.533333333333
(7, 100.0, 0.25): 0.666666666667
(7, 100.0, 0.275): 0.533333333333
(7, 100.0, 0.3): 0.466666666667
(8, 0.1, 0.3): 0.733333333333
(8, 1.0, 0.2): 0.6
(8, 1.0, 0.225): 0.533333333333
(8, 1.0, 0.25): 0.466666666667
(8, 1.0, 0.275): 0.666666666667
(8, 1.0, 0.3): 0.733333333333
(8, 10.0, 0.2): 0.733333333333
(8, 10.0, 0.225): 0.533333333333
(8, 10.0, 0.25): 0.866666666667
(8, 10.0, 0.275): 0.733333333333
(8, 10.0, 0.3): 0.8
(8, 100.0, 0.2): 0.466666666667
(8, 100.0, 0.225): 0.466666666667
(8, 100.0, 0.25): 0.733333333333
(8, 100.0, 0.275): 0.666666666667
(8, 100.0, 0.3): 0.666666666667
(9, 1.0, 0.2): 0.333333333333
(9, 1.0, 0.225): 0.466666666667
(9, 1.0, 0.25): 0.4
(9, 1.0, 0.275): 0.2
(9, 1.0, 0.3): 0.4
(9, 10.0, 0.2): 0.533333333333
(9, 10.0, 0.225): 0.333333333333
(9, 10.0, 0.25): 0.266666666667
(9, 10.0, 0.275): 0.466666666667
(9, 10.0, 0.3): 0.2
(9, 100.0, 0.2): 0.333333333333
(9, 100.0, 0.225): 0.4
(9, 100.0, 0.25): 0.266666666667
(9, 100.0, 0.275): 0.333333333333
(9, 100.0, 0.3): 0.4

Accuracy mean :0.4666666666666667
Std deviation :0.14271621346735847
Loss mean :0.5333333333333333
Std deviation :0.14271621346735847



Linear_2dim, number of iterations: 10

Parameters info
Tradeoffs parameter: [  0.1   1.   10.  100. ]
Gaussian kernel sigmas: [0.2   0.225 0.25  0.275 0.3  ]
Number of principal dimension considerated: 2
Dataset length: 150

Clusterization info
Minimum size of a cluster: in perc: 0.01, in int: 2
Type of mu: Binary Muzzifier
Fuzzifier: Linear
Cluster to label info
Force the number of cluster to be the number of different labels: False
Force the labels to be always represent by a cluster: False

Validation info
Conflict resolution: random


RESULTS

On TEST set

Accuracy on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 0.2): 0.933333333333
(1, 10.0, 0.2): 0.933333333333
(2, 10.0, 0.225): 1.0
(3, 10.0, 0.2): 0.933333333333
(4, 10.0, 0.2): 0.866666666667
(5, 10.0, 0.225): 0.866666666667
(6, 10.0, 0.2): 0.6
(7, 10.0, 0.25): 0.533333333333
(8, 100.0, 0.225): 0.866666666667
(9, 1.0, 0.225): 0.733333333333

Loss on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 0.2): 0.0666666666667
(1, 10.0, 0.2): 0.0666666666667
(2, 10.0, 0.225): 0.0
(3, 10.0, 0.2): 0.0666666666667
(4, 10.0, 0.2): 0.133333333333
(5, 10.0, 0.225): 0.133333333333
(6, 10.0, 0.2): 0.4
(7, 10.0, 0.25): 0.466666666667
(8, 100.0, 0.225): 0.133333333333
(9, 1.0, 0.225): 0.266666666667

Accuracy mean :0.8266666666666668
Std deviation :0.14666666666666667
Loss mean :0.17333333333333334
Std deviation :0.14666666666666667

On TRAINING set

Accuracy on training set for every iteration (n_of_the_iter, c, sigma):
(0, 1.0, 0.2): 0.866666666667
(1, 10.0, 0.2): 0.925925925926
(2, 10.0, 0.225): 0.874074074074
(3, 10.0, 0.2): 0.874074074074
(4, 10.0, 0.2): 0.859259259259
(5, 10.0, 0.225): 0.762962962963
(6, 10.0, 0.2): 0.762962962963
(7, 10.0, 0.25): 0.681481481481
(8, 100.0, 0.225): 0.918518518519
(9, 1.0, 0.225): 0.903703703704
Loss on training set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 0.2): 0.133333333333
(1, 10.0, 0.2): 0.0740740740741
(2, 10.0, 0.225): 0.125925925926
(3, 10.0, 0.2): 0.125925925926
(4, 10.0, 0.2): 0.140740740741
(5, 10.0, 0.225): 0.237037037037
(6, 10.0, 0.2): 0.237037037037
(7, 10.0, 0.25): 0.318518518519
(8, 100.0, 0.225): 0.0814814814815
(9, 1.0, 0.225): 0.0962962962963

Accuracy mean :0.842962962962963
Std deviation :0.07610547874987451
Loss mean :0.15703703703703703
Std deviation :0.07610547874987451

On ALL the sets

Accuracy on all the set for every iteration, for every couple c,sigma (n_of_the_iter, c, sigma):
(0, 1.0, 0.2): 0.933333333333
(0, 1.0, 0.225): 0.933333333333
(0, 1.0, 0.25): 0.133333333333
(0, 1.0, 0.275): 0.6
(0, 1.0, 0.3): 0.533333333333
(0, 10.0, 0.2): 0.933333333333
(0, 10.0, 0.225): 0.266666666667
(0, 10.0, 0.25): 0.466666666667
(0, 10.0, 0.275): 0.466666666667
(0, 10.0, 0.3): 0.533333333333
(0, 100.0, 0.2): 0.933333333333
(0, 100.0, 0.225): 0.266666666667
(0, 100.0, 0.25): 0.466666666667
(0, 100.0, 0.275): 0.6
(0, 100.0, 0.3): 0.533333333333
(1, 1.0, 0.2): 0.8
(1, 1.0, 0.225): 0.733333333333
(1, 1.0, 0.25): 0.666666666667
(1, 1.0, 0.275): 0.8
(1, 1.0, 0.3): 0.733333333333
(1, 10.0, 0.2): 0.8
(1, 10.0, 0.225): 0.733333333333
(1, 10.0, 0.25): 0.666666666667
(1, 10.0, 0.275): 0.733333333333
(1, 10.0, 0.3): 0.666666666667
(1, 100.0, 0.2): 0.8
(1, 100.0, 0.225): 0.8
(1, 100.0, 0.25): 0.733333333333
(1, 100.0, 0.275): 0.733333333333
(1, 100.0, 0.3): 0.733333333333
(2, 0.1, 0.25): 0.733333333333
(2, 1.0, 0.2): 0.933333333333
(2, 1.0, 0.225): 0.933333333333
(2, 1.0, 0.25): 0.733333333333
(2, 1.0, 0.275): 0.733333333333
(2, 1.0, 0.3): 0.733333333333
(2, 10.0, 0.2): 0.866666666667
(2, 10.0, 0.225): 0.933333333333
(2, 10.0, 0.25): 0.733333333333
(2, 10.0, 0.275): 0.866666666667
(2, 10.0, 0.3): 0.733333333333
(2, 100.0, 0.2): 0.933333333333
(2, 100.0, 0.225): 0.933333333333
(2, 100.0, 0.25): 0.733333333333
(2, 100.0, 0.275): 0.733333333333
(2, 100.0, 0.3): 0.733333333333
(3, 1.0, 0.2): 1.0
(3, 1.0, 0.225): 1.0
(3, 1.0, 0.25): 0.133333333333
(3, 1.0, 0.275): 1.0
(3, 1.0, 0.3): 1.0
(3, 10.0, 0.2): 1.0
(3, 10.0, 0.225): 0.133333333333
(3, 10.0, 0.25): 0.8
(3, 10.0, 0.275): 1.0
(3, 10.0, 0.3): 1.0
(3, 100.0, 0.2): 1.0
(3, 100.0, 0.225): 0.133333333333
(3, 100.0, 0.25): 0.133333333333
(3, 100.0, 0.275): 0.8
(3, 100.0, 0.3): 1.0
(4, 1.0, 0.2): 0.266666666667
(4, 1.0, 0.225): 0.866666666667
(4, 1.0, 0.25): 0.933333333333
(4, 1.0, 0.275): 0.533333333333
(4, 1.0, 0.3): 0.266666666667
(4, 10.0, 0.2): 0.933333333333
(4, 10.0, 0.225): 0.866666666667
(4, 10.0, 0.25): 0.933333333333
(4, 10.0, 0.275): 0.533333333333
(4, 10.0, 0.3): 0.466666666667
(4, 100.0, 0.2): 0.266666666667
(4, 100.0, 0.225): 0.866666666667
(4, 100.0, 0.25): 0.866666666667
(4, 100.0, 0.275): 0.6
(4, 100.0, 0.3): 0.266666666667
(5, 0.1, 0.25): 0.8
(5, 0.1, 0.275): 0.8
(5, 1.0, 0.2): 0.2
(5, 1.0, 0.225): 0.866666666667
(5, 1.0, 0.25): 0.8
(5, 1.0, 0.275): 0.8
(5, 1.0, 0.3): 0.8
(5, 10.0, 0.2): 0.2
(5, 10.0, 0.225): 0.866666666667
(5, 10.0, 0.25): 0.8
(5, 10.0, 0.275): 0.8
(5, 10.0, 0.3): 0.8
(5, 100.0, 0.2): 0.2
(5, 100.0, 0.225): 0.866666666667
(5, 100.0, 0.25): 0.866666666667
(5, 100.0, 0.275): 0.8
(5, 100.0, 0.3): 0.8
(6, 1.0, 0.2): 0.933333333333
(6, 1.0, 0.225): 0.866666666667
(6, 1.0, 0.25): 0.666666666667
(6, 1.0, 0.275): 0.733333333333
(6, 1.0, 0.3): 0.733333333333
(6, 10.0, 0.2): 0.933333333333
(6, 10.0, 0.225): 0.866666666667
(6, 10.0, 0.25): 0.666666666667
(6, 10.0, 0.275): 0.733333333333
(6, 10.0, 0.3): 0.666666666667
(6, 100.0, 0.2): 0.866666666667
(6, 100.0, 0.225): 0.666666666667
(6, 100.0, 0.25): 0.866666666667
(6, 100.0, 0.275): 0.733333333333
(6, 100.0, 0.3): 0.666666666667
(7, 1.0, 0.225): 0.866666666667
(7, 1.0, 0.25): 0.933333333333
(7, 1.0, 0.275): 0.933333333333
(7, 1.0, 0.3): 0.666666666667
(7, 10.0, 0.2): 0.333333333333
(7, 10.0, 0.225): 0.866666666667
(7, 10.0, 0.25): 1.0
(7, 10.0, 0.275): 0.933333333333
(7, 10.0, 0.3): 0.933333333333
(7, 100.0, 0.2): 0.866666666667
(7, 100.0, 0.225): 0.866666666667
(7, 100.0, 0.25): 0.933333333333
(7, 100.0, 0.275): 0.933333333333
(7, 100.0, 0.3): 0.666666666667
(8, 1.0, 0.2): 0.866666666667
(8, 1.0, 0.225): 0.933333333333
(8, 1.0, 0.25): 0.733333333333
(8, 1.0, 0.275): 0.933333333333
(8, 1.0, 0.3): 0.733333333333
(8, 10.0, 0.2): 0.8
(8, 10.0, 0.225): 0.933333333333
(8, 10.0, 0.25): 0.733333333333
(8, 10.0, 0.275): 0.933333333333
(8, 10.0, 0.3): 0.733333333333
(8, 100.0, 0.2): 0.866666666667
(8, 100.0, 0.225): 0.933333333333
(8, 100.0, 0.25): 0.733333333333
(8, 100.0, 0.275): 0.933333333333
(8, 100.0, 0.3): 0.733333333333
(9, 1.0, 0.2): 0.666666666667
(9, 1.0, 0.225): 1.0
(9, 1.0, 0.25): 0.866666666667
(9, 1.0, 0.275): 0.8
(9, 1.0, 0.3): 0.666666666667
(9, 10.0, 0.2): 0.733333333333
(9, 10.0, 0.225): 1.0
(9, 10.0, 0.25): 0.733333333333
(9, 10.0, 0.275): 0.866666666667
(9, 10.0, 0.3): 0.666666666667
(9, 100.0, 0.2): 0.733333333333
(9, 100.0, 0.225): 1.0
(9, 100.0, 0.25): 0.866666666667
(9, 100.0, 0.275): 0.866666666667
(9, 100.0, 0.3): 0.666666666667

Loss on all the set for every iteration, for every couple c,sigma (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 0.2): 0.0666666666667
(0, 1.0, 0.225): 0.0666666666667
(0, 1.0, 0.25): 0.866666666667
(0, 1.0, 0.275): 0.4
(0, 1.0, 0.3): 0.466666666667
(0, 10.0, 0.2): 0.0666666666667
(0, 10.0, 0.225): 0.733333333333
(0, 10.0, 0.25): 0.533333333333
(0, 10.0, 0.275): 0.533333333333
(0, 10.0, 0.3): 0.466666666667
(0, 100.0, 0.2): 0.0666666666667
(0, 100.0, 0.225): 0.733333333333
(0, 100.0, 0.25): 0.533333333333
(0, 100.0, 0.275): 0.4
(0, 100.0, 0.3): 0.466666666667
(1, 1.0, 0.2): 0.2
(1, 1.0, 0.225): 0.266666666667
(1, 1.0, 0.25): 0.333333333333
(1, 1.0, 0.275): 0.2
(1, 1.0, 0.3): 0.266666666667
(1, 10.0, 0.2): 0.2
(1, 10.0, 0.225): 0.266666666667
(1, 10.0, 0.25): 0.333333333333
(1, 10.0, 0.275): 0.266666666667
(1, 10.0, 0.3): 0.333333333333
(1, 100.0, 0.2): 0.2
(1, 100.0, 0.225): 0.2
(1, 100.0, 0.25): 0.266666666667
(1, 100.0, 0.275): 0.266666666667
(1, 100.0, 0.3): 0.266666666667
(2, 0.1, 0.25): 0.266666666667
(2, 1.0, 0.2): 0.0666666666667
(2, 1.0, 0.225): 0.0666666666667
(2, 1.0, 0.25): 0.266666666667
(2, 1.0, 0.275): 0.266666666667
(2, 1.0, 0.3): 0.266666666667
(2, 10.0, 0.2): 0.133333333333
(2, 10.0, 0.225): 0.0666666666667
(2, 10.0, 0.25): 0.266666666667
(2, 10.0, 0.275): 0.133333333333
(2, 10.0, 0.3): 0.266666666667
(2, 100.0, 0.2): 0.0666666666667
(2, 100.0, 0.225): 0.0666666666667
(2, 100.0, 0.25): 0.266666666667
(2, 100.0, 0.275): 0.266666666667
(2, 100.0, 0.3): 0.266666666667
(3, 1.0, 0.2): 0.0
(3, 1.0, 0.225): 0.0
(3, 1.0, 0.25): 0.866666666667
(3, 1.0, 0.275): 0.0
(3, 1.0, 0.3): 0.0
(3, 10.0, 0.2): 0.0
(3, 10.0, 0.225): 0.866666666667
(3, 10.0, 0.25): 0.2
(3, 10.0, 0.275): 0.0
(3, 10.0, 0.3): 0.0
(3, 100.0, 0.2): 0.0
(3, 100.0, 0.225): 0.866666666667
(3, 100.0, 0.25): 0.866666666667
(3, 100.0, 0.275): 0.2
(3, 100.0, 0.3): 0.0
(4, 1.0, 0.2): 0.733333333333
(4, 1.0, 0.225): 0.133333333333
(4, 1.0, 0.25): 0.0666666666667
(4, 1.0, 0.275): 0.466666666667
(4, 1.0, 0.3): 0.733333333333
(4, 10.0, 0.2): 0.0666666666667
(4, 10.0, 0.225): 0.133333333333
(4, 10.0, 0.25): 0.0666666666667
(4, 10.0, 0.275): 0.466666666667
(4, 10.0, 0.3): 0.533333333333
(4, 100.0, 0.2): 0.733333333333
(4, 100.0, 0.225): 0.133333333333
(4, 100.0, 0.25): 0.133333333333
(4, 100.0, 0.275): 0.4
(4, 100.0, 0.3): 0.733333333333
(5, 0.1, 0.25): 0.2
(5, 0.1, 0.275): 0.2
(5, 1.0, 0.2): 0.8
(5, 1.0, 0.225): 0.133333333333
(5, 1.0, 0.25): 0.2
(5, 1.0, 0.275): 0.2
(5, 1.0, 0.3): 0.2
(5, 10.0, 0.2): 0.8
(5, 10.0, 0.225): 0.133333333333
(5, 10.0, 0.25): 0.2
(5, 10.0, 0.275): 0.2
(5, 10.0, 0.3): 0.2
(5, 100.0, 0.2): 0.8
(5, 100.0, 0.225): 0.133333333333
(5, 100.0, 0.25): 0.133333333333
(5, 100.0, 0.275): 0.2
(5, 100.0, 0.3): 0.2
(6, 1.0, 0.2): 0.0666666666667
(6, 1.0, 0.225): 0.133333333333
(6, 1.0, 0.25): 0.333333333333
(6, 1.0, 0.275): 0.266666666667
(6, 1.0, 0.3): 0.266666666667
(6, 10.0, 0.2): 0.0666666666667
(6, 10.0, 0.225): 0.133333333333
(6, 10.0, 0.25): 0.333333333333
(6, 10.0, 0.275): 0.266666666667
(6, 10.0, 0.3): 0.333333333333
(6, 100.0, 0.2): 0.133333333333
(6, 100.0, 0.225): 0.333333333333
(6, 100.0, 0.25): 0.133333333333
(6, 100.0, 0.275): 0.266666666667
(6, 100.0, 0.3): 0.333333333333
(7, 1.0, 0.225): 0.133333333333
(7, 1.0, 0.25): 0.0666666666667
(7, 1.0, 0.275): 0.0666666666667
(7, 1.0, 0.3): 0.333333333333
(7, 10.0, 0.2): 0.666666666667
(7, 10.0, 0.225): 0.133333333333
(7, 10.0, 0.25): 0.0
(7, 10.0, 0.275): 0.0666666666667
(7, 10.0, 0.3): 0.0666666666667
(7, 100.0, 0.2): 0.133333333333
(7, 100.0, 0.225): 0.133333333333
(7, 100.0, 0.25): 0.0666666666667
(7, 100.0, 0.275): 0.0666666666667
(7, 100.0, 0.3): 0.333333333333
(8, 1.0, 0.2): 0.133333333333
(8, 1.0, 0.225): 0.0666666666667
(8, 1.0, 0.25): 0.266666666667
(8, 1.0, 0.275): 0.0666666666667
(8, 1.0, 0.3): 0.266666666667
(8, 10.0, 0.2): 0.2
(8, 10.0, 0.225): 0.0666666666667
(8, 10.0, 0.25): 0.266666666667
(8, 10.0, 0.275): 0.0666666666667
(8, 10.0, 0.3): 0.266666666667
(8, 100.0, 0.2): 0.133333333333
(8, 100.0, 0.225): 0.0666666666667
(8, 100.0, 0.25): 0.266666666667
(8, 100.0, 0.275): 0.0666666666667
(8, 100.0, 0.3): 0.266666666667
(9, 1.0, 0.2): 0.333333333333
(9, 1.0, 0.225): 0.0
(9, 1.0, 0.25): 0.133333333333
(9, 1.0, 0.275): 0.2
(9, 1.0, 0.3): 0.333333333333
(9, 10.0, 0.2): 0.266666666667
(9, 10.0, 0.225): 0.0
(9, 10.0, 0.25): 0.266666666667
(9, 10.0, 0.275): 0.133333333333
(9, 10.0, 0.3): 0.333333333333
(9, 100.0, 0.2): 0.266666666667
(9, 100.0, 0.225): 0.0
(9, 100.0, 0.25): 0.133333333333
(9, 100.0, 0.275): 0.133333333333
(9, 100.0, 0.3): 0.333333333333

Accuracy mean :0.7460526315789474
Std deviation :0.2172035928309862
Loss mean :0.25394736842105264
Std deviation :0.2172035928309862



QuantileConstPiecewise_2dim, number of iterations: 10

Parameters info
Tradeoffs parameter: [  0.1   1.   10.  100. ]
Gaussian kernel sigmas: [0.2   0.225 0.25  0.275 0.3  ]
Number of principal dimension considerated: 2
Dataset length: 150

Clusterization info
Minimum size of a cluster: in perc: 0.01, in int: 2
Type of mu: Binary Muzzifier
Fuzzifier: QuantileConstPiecewise
Cluster to label info
Force the number of cluster to be the number of different labels: False
Force the labels to be always represent by a cluster: False

Validation info
Conflict resolution: random


RESULTS

On TEST set

Accuracy on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 10.0, 0.3): 0.8
(1, 1.0, 0.3): 0.866666666667
(2, 10.0, 0.275): 0.533333333333
(3, 1.0, 0.3): 0.6
(4, 100.0, 0.3): 0.933333333333
(5, 10.0, 0.25): 0.533333333333
(6, 100.0, 0.275): 0.533333333333
(7, 100.0, 0.225): 0.733333333333
(8, 1.0, 0.3): 0.733333333333
(9, 1.0, 0.275): 0.933333333333

Loss on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 10.0, 0.3): 0.2
(1, 1.0, 0.3): 0.133333333333
(2, 10.0, 0.275): 0.466666666667
(3, 1.0, 0.3): 0.4
(4, 100.0, 0.3): 0.0666666666667
(5, 10.0, 0.25): 0.466666666667
(6, 100.0, 0.275): 0.466666666667
(7, 100.0, 0.225): 0.266666666667
(8, 1.0, 0.3): 0.266666666667
(9, 1.0, 0.275): 0.0666666666667

Accuracy mean :0.72
Std deviation :0.15434449203720302
Loss mean :0.28
Std deviation :0.15434449203720302

On TRAINING set

Accuracy on training set for every iteration (n_of_the_iter, c, sigma):
(0, 10.0, 0.3): 0.777777777778
(1, 1.0, 0.3): 0.792592592593
(2, 10.0, 0.275): 0.77037037037
(3, 1.0, 0.3): 0.814814814815
(4, 100.0, 0.3): 0.807407407407
(5, 10.0, 0.25): 0.725925925926
(6, 100.0, 0.275): 0.666666666667
(7, 100.0, 0.225): 0.711111111111
(8, 1.0, 0.3): 0.792592592593
(9, 1.0, 0.275): 0.740740740741
Loss on training set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 10.0, 0.3): 0.222222222222
(1, 1.0, 0.3): 0.207407407407
(2, 10.0, 0.275): 0.22962962963
(3, 1.0, 0.3): 0.185185185185
(4, 100.0, 0.3): 0.192592592593
(5, 10.0, 0.25): 0.274074074074
(6, 100.0, 0.275): 0.333333333333
(7, 100.0, 0.225): 0.288888888889
(8, 1.0, 0.3): 0.207407407407
(9, 1.0, 0.275): 0.259259259259

Accuracy mean :0.76
Std deviation :0.045203396451532915
Loss mean :0.24
Std deviation :0.04520339645153291

On ALL the sets

Accuracy on all the set for every iteration, for every couple c,sigma (n_of_the_iter, c, sigma):
(0, 1.0, 0.2): 0.533333333333
(0, 1.0, 0.225): 0.8
(0, 1.0, 0.25): 0.533333333333
(0, 1.0, 0.275): 0.6
(0, 1.0, 0.3): 0.6
(0, 10.0, 0.2): 0.333333333333
(0, 10.0, 0.225): 0.6
(0, 10.0, 0.25): 0.666666666667
(0, 10.0, 0.275): 0.6
(0, 10.0, 0.3): 0.8
(0, 100.0, 0.2): 0.666666666667
(0, 100.0, 0.225): 0.333333333333
(0, 100.0, 0.25): 0.8
(0, 100.0, 0.275): 0.533333333333
(0, 100.0, 0.3): 0.733333333333
(1, 1.0, 0.2): 0.666666666667
(1, 1.0, 0.225): 0.666666666667
(1, 1.0, 0.25): 0.6
(1, 1.0, 0.275): 0.533333333333
(1, 1.0, 0.3): 0.866666666667
(1, 10.0, 0.2): 0.533333333333
(1, 10.0, 0.225): 0.733333333333
(1, 10.0, 0.25): 0.6
(1, 10.0, 0.275): 0.533333333333
(1, 10.0, 0.3): 0.8
(1, 100.0, 0.2): 0.466666666667
(1, 100.0, 0.225): 0.466666666667
(1, 100.0, 0.25): 0.8
(1, 100.0, 0.275): 0.6
(1, 100.0, 0.3): 0.666666666667
(2, 1.0, 0.2): 0.6
(2, 1.0, 0.225): 0.4
(2, 1.0, 0.25): 0.266666666667
(2, 1.0, 0.275): 0.733333333333
(2, 1.0, 0.3): 0.733333333333
(2, 10.0, 0.2): 0.6
(2, 10.0, 0.225): 0.666666666667
(2, 10.0, 0.25): 0.733333333333
(2, 10.0, 0.275): 0.733333333333
(2, 10.0, 0.3): 0.666666666667
(2, 100.0, 0.2): 0.466666666667
(2, 100.0, 0.225): 0.4
(2, 100.0, 0.25): 0.266666666667
(2, 100.0, 0.275): 0.733333333333
(2, 100.0, 0.3): 0.666666666667
(3, 1.0, 0.2): 0.466666666667
(3, 1.0, 0.225): 0.266666666667
(3, 1.0, 0.25): 0.8
(3, 1.0, 0.275): 0.866666666667
(3, 1.0, 0.3): 0.933333333333
(3, 10.0, 0.2): 0.666666666667
(3, 10.0, 0.225): 0.466666666667
(3, 10.0, 0.25): 0.6
(3, 10.0, 0.275): 0.933333333333
(3, 10.0, 0.3): 0.933333333333
(3, 100.0, 0.2): 0.533333333333
(3, 100.0, 0.225): 0.2
(3, 100.0, 0.25): 0.8
(3, 100.0, 0.275): 0.8
(3, 100.0, 0.3): 0.866666666667
(4, 1.0, 0.2): 0.2
(4, 1.0, 0.225): 0.666666666667
(4, 1.0, 0.25): 0.733333333333
(4, 1.0, 0.275): 0.266666666667
(4, 1.0, 0.3): 0.666666666667
(4, 10.0, 0.2): 0.0
(4, 10.0, 0.225): 0.133333333333
(4, 10.0, 0.25): 0.666666666667
(4, 10.0, 0.275): 0.133333333333
(4, 10.0, 0.3): 0.666666666667
(4, 100.0, 0.2): 0.266666666667
(4, 100.0, 0.225): 0.6
(4, 100.0, 0.25): 0.733333333333
(4, 100.0, 0.275): 0.133333333333
(4, 100.0, 0.3): 0.8
(5, 0.1, 0.25): 0.533333333333
(5, 0.1, 0.275): 0.533333333333
(5, 0.1, 0.3): 0.533333333333
(5, 1.0, 0.2): 0.466666666667
(5, 1.0, 0.225): 0.6
(5, 1.0, 0.25): 0.733333333333
(5, 1.0, 0.275): 0.533333333333
(5, 1.0, 0.3): 0.533333333333
(5, 10.0, 0.2): 0.666666666667
(5, 10.0, 0.225): 0.733333333333
(5, 10.0, 0.25): 0.8
(5, 10.0, 0.275): 0.533333333333
(5, 10.0, 0.3): 0.533333333333
(5, 100.0, 0.2): 0.333333333333
(5, 100.0, 0.225): 0.6
(5, 100.0, 0.25): 0.666666666667
(5, 100.0, 0.275): 0.533333333333
(5, 100.0, 0.3): 0.533333333333
(6, 0.1, 0.2): 0.466666666667
(6, 1.0, 0.2): 0.333333333333
(6, 1.0, 0.225): 0.6
(6, 1.0, 0.25): 0.6
(6, 1.0, 0.275): 0.8
(6, 1.0, 0.3): 0.733333333333
(6, 10.0, 0.2): 0.533333333333
(6, 10.0, 0.225): 0.333333333333
(6, 10.0, 0.25): 0.8
(6, 10.0, 0.275): 0.866666666667
(6, 10.0, 0.3): 0.666666666667
(6, 100.0, 0.2): 0.466666666667
(6, 100.0, 0.225): 0.333333333333
(6, 100.0, 0.25): 0.333333333333
(6, 100.0, 0.275): 0.866666666667
(6, 100.0, 0.3): 0.666666666667
(7, 0.1, 0.25): 0.333333333333
(7, 1.0, 0.2): 0.333333333333
(7, 1.0, 0.225): 0.4
(7, 1.0, 0.25): 0.533333333333
(7, 1.0, 0.275): 0.6
(7, 1.0, 0.3): 0.333333333333
(7, 10.0, 0.2): 0.733333333333
(7, 10.0, 0.225): 0.8
(7, 10.0, 0.275): 0.733333333333
(7, 10.0, 0.3): 0.333333333333
(7, 100.0, 0.2): 0.4
(7, 100.0, 0.225): 0.8
(7, 100.0, 0.25): 0.666666666667
(7, 100.0, 0.275): 0.6
(7, 100.0, 0.3): 0.333333333333
(8, 0.1, 0.25): 0.6
(8, 1.0, 0.2): 0.6
(8, 1.0, 0.225): 0.466666666667
(8, 1.0, 0.25): 0.6
(8, 1.0, 0.275): 0.6
(8, 1.0, 0.3): 0.733333333333
(8, 10.0, 0.2): 0.2
(8, 10.0, 0.225): 0.466666666667
(8, 10.0, 0.25): 0.6
(8, 10.0, 0.275): 0.333333333333
(8, 10.0, 0.3): 0.666666666667
(8, 100.0, 0.2): 0.333333333333
(8, 100.0, 0.225): 0.533333333333
(8, 100.0, 0.25): 0.6
(8, 100.0, 0.275): 0.333333333333
(8, 100.0, 0.3): 0.666666666667
(9, 1.0, 0.2): 0.333333333333
(9, 1.0, 0.225): 0.533333333333
(9, 1.0, 0.25): 0.666666666667
(9, 1.0, 0.275): 0.8
(9, 1.0, 0.3): 0.6
(9, 10.0, 0.2): 0.2
(9, 10.0, 0.225): 0.733333333333
(9, 10.0, 0.25): 0.666666666667
(9, 10.0, 0.275): 0.666666666667
(9, 10.0, 0.3): 0.733333333333
(9, 100.0, 0.2): 0.466666666667
(9, 100.0, 0.225): 0.466666666667
(9, 100.0, 0.25): 0.8
(9, 100.0, 0.275): 0.666666666667
(9, 100.0, 0.3): 0.466666666667

Loss on all the set for every iteration, for every couple c,sigma (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 0.2): 0.466666666667
(0, 1.0, 0.225): 0.2
(0, 1.0, 0.25): 0.466666666667
(0, 1.0, 0.275): 0.4
(0, 1.0, 0.3): 0.4
(0, 10.0, 0.2): 0.666666666667
(0, 10.0, 0.225): 0.4
(0, 10.0, 0.25): 0.333333333333
(0, 10.0, 0.275): 0.4
(0, 10.0, 0.3): 0.2
(0, 100.0, 0.2): 0.333333333333
(0, 100.0, 0.225): 0.666666666667
(0, 100.0, 0.25): 0.2
(0, 100.0, 0.275): 0.466666666667
(0, 100.0, 0.3): 0.266666666667
(1, 1.0, 0.2): 0.333333333333
(1, 1.0, 0.225): 0.333333333333
(1, 1.0, 0.25): 0.4
(1, 1.0, 0.275): 0.466666666667
(1, 1.0, 0.3): 0.133333333333
(1, 10.0, 0.2): 0.466666666667
(1, 10.0, 0.225): 0.266666666667
(1, 10.0, 0.25): 0.4
(1, 10.0, 0.275): 0.466666666667
(1, 10.0, 0.3): 0.2
(1, 100.0, 0.2): 0.533333333333
(1, 100.0, 0.225): 0.533333333333
(1, 100.0, 0.25): 0.2
(1, 100.0, 0.275): 0.4
(1, 100.0, 0.3): 0.333333333333
(2, 1.0, 0.2): 0.4
(2, 1.0, 0.225): 0.6
(2, 1.0, 0.25): 0.733333333333
(2, 1.0, 0.275): 0.266666666667
(2, 1.0, 0.3): 0.266666666667
(2, 10.0, 0.2): 0.4
(2, 10.0, 0.225): 0.333333333333
(2, 10.0, 0.25): 0.266666666667
(2, 10.0, 0.275): 0.266666666667
(2, 10.0, 0.3): 0.333333333333
(2, 100.0, 0.2): 0.533333333333
(2, 100.0, 0.225): 0.6
(2, 100.0, 0.25): 0.733333333333
(2, 100.0, 0.275): 0.266666666667
(2, 100.0, 0.3): 0.333333333333
(3, 1.0, 0.2): 0.533333333333
(3, 1.0, 0.225): 0.733333333333
(3, 1.0, 0.25): 0.2
(3, 1.0, 0.275): 0.133333333333
(3, 1.0, 0.3): 0.0666666666667
(3, 10.0, 0.2): 0.333333333333
(3, 10.0, 0.225): 0.533333333333
(3, 10.0, 0.25): 0.4
(3, 10.0, 0.275): 0.0666666666667
(3, 10.0, 0.3): 0.0666666666667
(3, 100.0, 0.2): 0.466666666667
(3, 100.0, 0.225): 0.8
(3, 100.0, 0.25): 0.2
(3, 100.0, 0.275): 0.2
(3, 100.0, 0.3): 0.133333333333
(4, 1.0, 0.2): 0.8
(4, 1.0, 0.225): 0.333333333333
(4, 1.0, 0.25): 0.266666666667
(4, 1.0, 0.275): 0.733333333333
(4, 1.0, 0.3): 0.333333333333
(4, 10.0, 0.2): 1.0
(4, 10.0, 0.225): 0.866666666667
(4, 10.0, 0.25): 0.333333333333
(4, 10.0, 0.275): 0.866666666667
(4, 10.0, 0.3): 0.333333333333
(4, 100.0, 0.2): 0.733333333333
(4, 100.0, 0.225): 0.4
(4, 100.0, 0.25): 0.266666666667
(4, 100.0, 0.275): 0.866666666667
(4, 100.0, 0.3): 0.2
(5, 0.1, 0.25): 0.466666666667
(5, 0.1, 0.275): 0.466666666667
(5, 0.1, 0.3): 0.466666666667
(5, 1.0, 0.2): 0.533333333333
(5, 1.0, 0.225): 0.4
(5, 1.0, 0.25): 0.266666666667
(5, 1.0, 0.275): 0.466666666667
(5, 1.0, 0.3): 0.466666666667
(5, 10.0, 0.2): 0.333333333333
(5, 10.0, 0.225): 0.266666666667
(5, 10.0, 0.25): 0.2
(5, 10.0, 0.275): 0.466666666667
(5, 10.0, 0.3): 0.466666666667
(5, 100.0, 0.2): 0.666666666667
(5, 100.0, 0.225): 0.4
(5, 100.0, 0.25): 0.333333333333
(5, 100.0, 0.275): 0.466666666667
(5, 100.0, 0.3): 0.466666666667
(6, 0.1, 0.2): 0.533333333333
(6, 1.0, 0.2): 0.666666666667
(6, 1.0, 0.225): 0.4
(6, 1.0, 0.25): 0.4
(6, 1.0, 0.275): 0.2
(6, 1.0, 0.3): 0.266666666667
(6, 10.0, 0.2): 0.466666666667
(6, 10.0, 0.225): 0.666666666667
(6, 10.0, 0.25): 0.2
(6, 10.0, 0.275): 0.133333333333
(6, 10.0, 0.3): 0.333333333333
(6, 100.0, 0.2): 0.533333333333
(6, 100.0, 0.225): 0.666666666667
(6, 100.0, 0.25): 0.666666666667
(6, 100.0, 0.275): 0.133333333333
(6, 100.0, 0.3): 0.333333333333
(7, 0.1, 0.25): 0.666666666667
(7, 1.0, 0.2): 0.666666666667
(7, 1.0, 0.225): 0.6
(7, 1.0, 0.25): 0.466666666667
(7, 1.0, 0.275): 0.4
(7, 1.0, 0.3): 0.666666666667
(7, 10.0, 0.2): 0.266666666667
(7, 10.0, 0.225): 0.2
(7, 10.0, 0.275): 0.266666666667
(7, 10.0, 0.3): 0.666666666667
(7, 100.0, 0.2): 0.6
(7, 100.0, 0.225): 0.2
(7, 100.0, 0.25): 0.333333333333
(7, 100.0, 0.275): 0.4
(7, 100.0, 0.3): 0.666666666667
(8, 0.1, 0.25): 0.4
(8, 1.0, 0.2): 0.4
(8, 1.0, 0.225): 0.533333333333
(8, 1.0, 0.25): 0.4
(8, 1.0, 0.275): 0.4
(8, 1.0, 0.3): 0.266666666667
(8, 10.0, 0.2): 0.8
(8, 10.0, 0.225): 0.533333333333
(8, 10.0, 0.25): 0.4
(8, 10.0, 0.275): 0.666666666667
(8, 10.0, 0.3): 0.333333333333
(8, 100.0, 0.2): 0.666666666667
(8, 100.0, 0.225): 0.466666666667
(8, 100.0, 0.25): 0.4
(8, 100.0, 0.275): 0.666666666667
(8, 100.0, 0.3): 0.333333333333
(9, 1.0, 0.2): 0.666666666667
(9, 1.0, 0.225): 0.466666666667
(9, 1.0, 0.25): 0.333333333333
(9, 1.0, 0.275): 0.2
(9, 1.0, 0.3): 0.4
(9, 10.0, 0.2): 0.8
(9, 10.0, 0.225): 0.266666666667
(9, 10.0, 0.25): 0.333333333333
(9, 10.0, 0.275): 0.333333333333
(9, 10.0, 0.3): 0.266666666667
(9, 100.0, 0.2): 0.533333333333
(9, 100.0, 0.225): 0.533333333333
(9, 100.0, 0.25): 0.2
(9, 100.0, 0.275): 0.333333333333
(9, 100.0, 0.3): 0.533333333333

Accuracy mean :0.5741935483870968
Std deviation :0.1877063361536094
Loss mean :0.4258064516129032
Std deviation :0.1877063361536094



Crisp_3dim, number of iterations: 10

Parameters info
Tradeoffs parameter: [  0.1   1.   10.  100. ]
Gaussian kernel sigmas: [0.2   0.225 0.25  0.275 0.3  ]
Number of principal dimension considerated: 3
Dataset length: 150

Clusterization info
Minimum size of a cluster: in perc: 0.01, in int: 2
Type of mu: Binary Muzzifier
Fuzzifier: Crisp
Cluster to label info
Force the number of cluster to be the number of different labels: False
Force the labels to be always represent by a cluster: False

Validation info
Conflict resolution: random


RESULTS

On TEST set

Accuracy on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 0.275): 0.266666666667
(1, 1.0, 0.3): 0.533333333333
(2, 1.0, 0.2): 0.333333333333
(3, 10.0, 0.275): 0.6
(4, 100.0, 0.275): 0.2
(5, 10.0, 0.25): 0.266666666667
(6, 100.0, 0.2): 0.266666666667
(7, 10.0, 0.25): 0.6
(8, 10.0, 0.275): 0.4
(9, 10.0, 0.3): 0.533333333333

Loss on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 0.275): 0.733333333333
(1, 1.0, 0.3): 0.466666666667
(2, 1.0, 0.2): 0.666666666667
(3, 10.0, 0.275): 0.4
(4, 100.0, 0.275): 0.8
(5, 10.0, 0.25): 0.733333333333
(6, 100.0, 0.2): 0.733333333333
(7, 10.0, 0.25): 0.4
(8, 10.0, 0.275): 0.6
(9, 10.0, 0.3): 0.466666666667

Accuracy mean :0.4
Std deviation :0.1460593486680443
Loss mean :0.6
Std deviation :0.1460593486680443

On TRAINING set

Accuracy on training set for every iteration (n_of_the_iter, c, sigma):
(0, 1.0, 0.275): 0.459259259259
(1, 1.0, 0.3): 0.437037037037
(2, 1.0, 0.2): 0.496296296296
(3, 10.0, 0.275): 0.42962962963
(4, 100.0, 0.275): 0.525925925926
(5, 10.0, 0.25): 0.496296296296
(6, 100.0, 0.2): 0.525925925926
(7, 10.0, 0.25): 0.4
(8, 10.0, 0.275): 0.444444444444
(9, 10.0, 0.3): 0.503703703704
Loss on training set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 0.275): 0.540740740741
(1, 1.0, 0.3): 0.562962962963
(2, 1.0, 0.2): 0.503703703704
(3, 10.0, 0.275): 0.57037037037
(4, 100.0, 0.275): 0.474074074074
(5, 10.0, 0.25): 0.503703703704
(6, 100.0, 0.2): 0.474074074074
(7, 10.0, 0.25): 0.6
(8, 10.0, 0.275): 0.555555555556
(9, 10.0, 0.3): 0.496296296296

Accuracy mean :0.47185185185185186
Std deviation :0.04138215621766938
Loss mean :0.5281481481481481
Std deviation :0.0413821562176694

On ALL the sets

Accuracy on all the set for every iteration, for every couple c,sigma (n_of_the_iter, c, sigma):
(0, 1.0, 0.2): 0.4
(0, 1.0, 0.225): 0.266666666667
(0, 1.0, 0.25): 0.466666666667
(0, 1.0, 0.275): 0.6
(0, 1.0, 0.3): 0.266666666667
(0, 10.0, 0.2): 0.333333333333
(0, 10.0, 0.225): 0.533333333333
(0, 10.0, 0.25): 0.4
(0, 10.0, 0.275): 0.333333333333
(0, 10.0, 0.3): 0.4
(0, 100.0, 0.2): 0.266666666667
(0, 100.0, 0.225): 0.266666666667
(0, 100.0, 0.25): 0.333333333333
(0, 100.0, 0.275): 0.333333333333
(0, 100.0, 0.3): 0.333333333333
(1, 1.0, 0.2): 0.466666666667
(1, 1.0, 0.225): 0.466666666667
(1, 1.0, 0.25): 0.333333333333
(1, 1.0, 0.275): 0.333333333333
(1, 1.0, 0.3): 0.533333333333
(1, 10.0, 0.2): 0.333333333333
(1, 10.0, 0.225): 0.2
(1, 10.0, 0.25): 0.333333333333
(1, 10.0, 0.275): 0.4
(1, 10.0, 0.3): 0.333333333333
(1, 100.0, 0.2): 0.333333333333
(1, 100.0, 0.225): 0.4
(1, 100.0, 0.25): 0.2
(1, 100.0, 0.275): 0.333333333333
(1, 100.0, 0.3): 0.533333333333
(2, 1.0, 0.2): 0.533333333333
(2, 1.0, 0.225): 0.4
(2, 1.0, 0.25): 0.2
(2, 1.0, 0.275): 0.466666666667
(2, 1.0, 0.3): 0.466666666667
(2, 10.0, 0.2): 0.2
(2, 10.0, 0.225): 0.466666666667
(2, 10.0, 0.25): 0.2
(2, 10.0, 0.275): 0.466666666667
(2, 10.0, 0.3): 0.466666666667
(2, 100.0, 0.2): 0.266666666667
(2, 100.0, 0.225): 0.4
(2, 100.0, 0.25): 0.2
(2, 100.0, 0.275): 0.466666666667
(2, 100.0, 0.3): 0.4
(3, 1.0, 0.2): 0.333333333333
(3, 1.0, 0.225): 0.266666666667
(3, 1.0, 0.25): 0.466666666667
(3, 1.0, 0.275): 0.4
(3, 1.0, 0.3): 0.4
(3, 10.0, 0.2): 0.4
(3, 10.0, 0.225): 0.4
(3, 10.0, 0.25): 0.266666666667
(3, 10.0, 0.275): 0.6
(3, 10.0, 0.3): 0.466666666667
(3, 100.0, 0.2): 0.466666666667
(3, 100.0, 0.225): 0.4
(3, 100.0, 0.25): 0.266666666667
(3, 100.0, 0.275): 0.466666666667
(3, 100.0, 0.3): 0.333333333333
(4, 1.0, 0.2): 0.266666666667
(4, 1.0, 0.225): 0.533333333333
(4, 1.0, 0.25): 0.266666666667
(4, 1.0, 0.275): 0.333333333333
(4, 1.0, 0.3): 0.466666666667
(4, 10.0, 0.2): 0.266666666667
(4, 10.0, 0.225): 0.2
(4, 10.0, 0.25): 0.266666666667
(4, 10.0, 0.275): 0.4
(4, 10.0, 0.3): 0.533333333333
(4, 100.0, 0.2): 0.266666666667
(4, 100.0, 0.225): 0.466666666667
(4, 100.0, 0.25): 0.466666666667
(4, 100.0, 0.275): 0.6
(4, 100.0, 0.3): 0.4
(5, 1.0, 0.2): 0.266666666667
(5, 1.0, 0.225): 0.466666666667
(5, 1.0, 0.25): 0.333333333333
(5, 1.0, 0.275): 0.533333333333
(5, 1.0, 0.3): 0.333333333333
(5, 10.0, 0.2): 0.533333333333
(5, 10.0, 0.225): 0.533333333333
(5, 10.0, 0.25): 0.666666666667
(5, 10.0, 0.275): 0.6
(5, 10.0, 0.3): 0.666666666667
(5, 100.0, 0.2): 0.333333333333
(5, 100.0, 0.225): 0.133333333333
(5, 100.0, 0.25): 0.466666666667
(5, 100.0, 0.275): 0.533333333333
(5, 100.0, 0.3): 0.533333333333
(6, 0.1, 0.3): 0.4
(6, 1.0, 0.2): 0.333333333333
(6, 1.0, 0.225): 0.2
(6, 1.0, 0.25): 0.333333333333
(6, 1.0, 0.275): 0.333333333333
(6, 1.0, 0.3): 0.333333333333
(6, 10.0, 0.2): 0.333333333333
(6, 10.0, 0.225): 0.2
(6, 10.0, 0.25): 0.466666666667
(6, 10.0, 0.275): 0.2
(6, 10.0, 0.3): 0.466666666667
(6, 100.0, 0.2): 0.533333333333
(6, 100.0, 0.225): 0.2
(6, 100.0, 0.25): 0.4
(6, 100.0, 0.275): 0.2
(6, 100.0, 0.3): 0.4
(7, 1.0, 0.2): 0.2
(7, 1.0, 0.225): 0.266666666667
(7, 1.0, 0.25): 0.466666666667
(7, 1.0, 0.275): 0.266666666667
(7, 1.0, 0.3): 0.4
(7, 10.0, 0.2): 0.266666666667
(7, 10.0, 0.225): 0.2
(7, 10.0, 0.25): 0.6
(7, 10.0, 0.3): 0.266666666667
(7, 100.0, 0.2): 0.4
(7, 100.0, 0.225): 0.4
(7, 100.0, 0.25): 0.533333333333
(7, 100.0, 0.3): 0.333333333333
(8, 1.0, 0.2): 0.333333333333
(8, 1.0, 0.225): 0.133333333333
(8, 1.0, 0.25): 0.2
(8, 1.0, 0.275): 0.2
(8, 1.0, 0.3): 0.266666666667
(8, 10.0, 0.2): 0.266666666667
(8, 10.0, 0.225): 0.333333333333
(8, 10.0, 0.25): 0.2
(8, 10.0, 0.275): 0.466666666667
(8, 10.0, 0.3): 0.333333333333
(8, 100.0, 0.2): 0.266666666667
(8, 100.0, 0.225): 0.266666666667
(8, 100.0, 0.25): 0.2
(8, 100.0, 0.275): 0.266666666667
(8, 100.0, 0.3): 0.333333333333
(9, 0.1, 0.25): 0.333333333333
(9, 1.0, 0.2): 0.533333333333
(9, 1.0, 0.225): 0.266666666667
(9, 1.0, 0.25): 0.333333333333
(9, 1.0, 0.275): 0.666666666667
(9, 1.0, 0.3): 0.333333333333
(9, 10.0, 0.2): 0.466666666667
(9, 10.0, 0.225): 0.333333333333
(9, 10.0, 0.25): 0.333333333333
(9, 10.0, 0.275): 0.533333333333
(9, 10.0, 0.3): 0.733333333333
(9, 100.0, 0.2): 0.466666666667
(9, 100.0, 0.225): 0.133333333333
(9, 100.0, 0.25): 0.333333333333
(9, 100.0, 0.275): 0.6
(9, 100.0, 0.3): 0.333333333333

Loss on all the set for every iteration, for every couple c,sigma (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 0.2): 0.6
(0, 1.0, 0.225): 0.733333333333
(0, 1.0, 0.25): 0.533333333333
(0, 1.0, 0.275): 0.4
(0, 1.0, 0.3): 0.733333333333
(0, 10.0, 0.2): 0.666666666667
(0, 10.0, 0.225): 0.466666666667
(0, 10.0, 0.25): 0.6
(0, 10.0, 0.275): 0.666666666667
(0, 10.0, 0.3): 0.6
(0, 100.0, 0.2): 0.733333333333
(0, 100.0, 0.225): 0.733333333333
(0, 100.0, 0.25): 0.666666666667
(0, 100.0, 0.275): 0.666666666667
(0, 100.0, 0.3): 0.666666666667
(1, 1.0, 0.2): 0.533333333333
(1, 1.0, 0.225): 0.533333333333
(1, 1.0, 0.25): 0.666666666667
(1, 1.0, 0.275): 0.666666666667
(1, 1.0, 0.3): 0.466666666667
(1, 10.0, 0.2): 0.666666666667
(1, 10.0, 0.225): 0.8
(1, 10.0, 0.25): 0.666666666667
(1, 10.0, 0.275): 0.6
(1, 10.0, 0.3): 0.666666666667
(1, 100.0, 0.2): 0.666666666667
(1, 100.0, 0.225): 0.6
(1, 100.0, 0.25): 0.8
(1, 100.0, 0.275): 0.666666666667
(1, 100.0, 0.3): 0.466666666667
(2, 1.0, 0.2): 0.466666666667
(2, 1.0, 0.225): 0.6
(2, 1.0, 0.25): 0.8
(2, 1.0, 0.275): 0.533333333333
(2, 1.0, 0.3): 0.533333333333
(2, 10.0, 0.2): 0.8
(2, 10.0, 0.225): 0.533333333333
(2, 10.0, 0.25): 0.8
(2, 10.0, 0.275): 0.533333333333
(2, 10.0, 0.3): 0.533333333333
(2, 100.0, 0.2): 0.733333333333
(2, 100.0, 0.225): 0.6
(2, 100.0, 0.25): 0.8
(2, 100.0, 0.275): 0.533333333333
(2, 100.0, 0.3): 0.6
(3, 1.0, 0.2): 0.666666666667
(3, 1.0, 0.225): 0.733333333333
(3, 1.0, 0.25): 0.533333333333
(3, 1.0, 0.275): 0.6
(3, 1.0, 0.3): 0.6
(3, 10.0, 0.2): 0.6
(3, 10.0, 0.225): 0.6
(3, 10.0, 0.25): 0.733333333333
(3, 10.0, 0.275): 0.4
(3, 10.0, 0.3): 0.533333333333
(3, 100.0, 0.2): 0.533333333333
(3, 100.0, 0.225): 0.6
(3, 100.0, 0.25): 0.733333333333
(3, 100.0, 0.275): 0.533333333333
(3, 100.0, 0.3): 0.666666666667
(4, 1.0, 0.2): 0.733333333333
(4, 1.0, 0.225): 0.466666666667
(4, 1.0, 0.25): 0.733333333333
(4, 1.0, 0.275): 0.666666666667
(4, 1.0, 0.3): 0.533333333333
(4, 10.0, 0.2): 0.733333333333
(4, 10.0, 0.225): 0.8
(4, 10.0, 0.25): 0.733333333333
(4, 10.0, 0.275): 0.6
(4, 10.0, 0.3): 0.466666666667
(4, 100.0, 0.2): 0.733333333333
(4, 100.0, 0.225): 0.533333333333
(4, 100.0, 0.25): 0.533333333333
(4, 100.0, 0.275): 0.4
(4, 100.0, 0.3): 0.6
(5, 1.0, 0.2): 0.733333333333
(5, 1.0, 0.225): 0.533333333333
(5, 1.0, 0.25): 0.666666666667
(5, 1.0, 0.275): 0.466666666667
(5, 1.0, 0.3): 0.666666666667
(5, 10.0, 0.2): 0.466666666667
(5, 10.0, 0.225): 0.466666666667
(5, 10.0, 0.25): 0.333333333333
(5, 10.0, 0.275): 0.4
(5, 10.0, 0.3): 0.333333333333
(5, 100.0, 0.2): 0.666666666667
(5, 100.0, 0.225): 0.866666666667
(5, 100.0, 0.25): 0.533333333333
(5, 100.0, 0.275): 0.466666666667
(5, 100.0, 0.3): 0.466666666667
(6, 0.1, 0.3): 0.6
(6, 1.0, 0.2): 0.666666666667
(6, 1.0, 0.225): 0.8
(6, 1.0, 0.25): 0.666666666667
(6, 1.0, 0.275): 0.666666666667
(6, 1.0, 0.3): 0.666666666667
(6, 10.0, 0.2): 0.666666666667
(6, 10.0, 0.225): 0.8
(6, 10.0, 0.25): 0.533333333333
(6, 10.0, 0.275): 0.8
(6, 10.0, 0.3): 0.533333333333
(6, 100.0, 0.2): 0.466666666667
(6, 100.0, 0.225): 0.8
(6, 100.0, 0.25): 0.6
(6, 100.0, 0.275): 0.8
(6, 100.0, 0.3): 0.6
(7, 1.0, 0.2): 0.8
(7, 1.0, 0.225): 0.733333333333
(7, 1.0, 0.25): 0.533333333333
(7, 1.0, 0.275): 0.733333333333
(7, 1.0, 0.3): 0.6
(7, 10.0, 0.2): 0.733333333333
(7, 10.0, 0.225): 0.8
(7, 10.0, 0.25): 0.4
(7, 10.0, 0.3): 0.733333333333
(7, 100.0, 0.2): 0.6
(7, 100.0, 0.225): 0.6
(7, 100.0, 0.25): 0.466666666667
(7, 100.0, 0.3): 0.666666666667
(8, 1.0, 0.2): 0.666666666667
(8, 1.0, 0.225): 0.866666666667
(8, 1.0, 0.25): 0.8
(8, 1.0, 0.275): 0.8
(8, 1.0, 0.3): 0.733333333333
(8, 10.0, 0.2): 0.733333333333
(8, 10.0, 0.225): 0.666666666667
(8, 10.0, 0.25): 0.8
(8, 10.0, 0.275): 0.533333333333
(8, 10.0, 0.3): 0.666666666667
(8, 100.0, 0.2): 0.733333333333
(8, 100.0, 0.225): 0.733333333333
(8, 100.0, 0.25): 0.8
(8, 100.0, 0.275): 0.733333333333
(8, 100.0, 0.3): 0.666666666667
(9, 0.1, 0.25): 0.666666666667
(9, 1.0, 0.2): 0.466666666667
(9, 1.0, 0.225): 0.733333333333
(9, 1.0, 0.25): 0.666666666667
(9, 1.0, 0.275): 0.333333333333
(9, 1.0, 0.3): 0.666666666667
(9, 10.0, 0.2): 0.533333333333
(9, 10.0, 0.225): 0.666666666667
(9, 10.0, 0.25): 0.666666666667
(9, 10.0, 0.275): 0.466666666667
(9, 10.0, 0.3): 0.266666666667
(9, 100.0, 0.2): 0.533333333333
(9, 100.0, 0.225): 0.866666666667
(9, 100.0, 0.25): 0.666666666667
(9, 100.0, 0.275): 0.4
(9, 100.0, 0.3): 0.666666666667

Accuracy mean :0.37333333333333335
Std deviation :0.12507775359529144
Loss mean :0.6266666666666667
Std deviation :0.12507775359529144



Linear_3dim, number of iterations: 10

Parameters info
Tradeoffs parameter: [  0.1   1.   10.  100. ]
Gaussian kernel sigmas: [0.2   0.225 0.25  0.275 0.3  ]
Number of principal dimension considerated: 3
Dataset length: 150

Clusterization info
Minimum size of a cluster: in perc: 0.01, in int: 2
Type of mu: Binary Muzzifier
Fuzzifier: Linear
Cluster to label info
Force the number of cluster to be the number of different labels: False
Force the labels to be always represent by a cluster: False

Validation info
Conflict resolution: random


RESULTS

On TEST set

Accuracy on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 0.225): 0.733333333333
(1, 100.0, 0.2): 0.8
(2, 10.0, 0.25): 0.8
(3, 1.0, 0.2): 0.933333333333
(4, 10.0, 0.225): 1.0
(5, 100.0, 0.225): 0.866666666667
(6, 100.0, 0.275): 0.6
(7, 1.0, 0.225): 0.866666666667
(8, 1.0, 0.25): 0.2
(9, 100.0, 0.2): 0.8

Loss on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 0.225): 0.266666666667
(1, 100.0, 0.2): 0.2
(2, 10.0, 0.25): 0.2
(3, 1.0, 0.2): 0.0666666666667
(4, 10.0, 0.225): 0.0
(5, 100.0, 0.225): 0.133333333333
(6, 100.0, 0.275): 0.4
(7, 1.0, 0.225): 0.133333333333
(8, 1.0, 0.25): 0.8
(9, 100.0, 0.2): 0.2

Accuracy mean :0.76
Std deviation :0.21333333333333335
Loss mean :0.24
Std deviation :0.21333333333333335

On TRAINING set

Accuracy on training set for every iteration (n_of_the_iter, c, sigma):
(0, 1.0, 0.225): 0.903703703704
(1, 100.0, 0.2): 0.940740740741
(2, 10.0, 0.25): 0.696296296296
(3, 1.0, 0.2): 0.925925925926
(4, 10.0, 0.225): 0.925925925926
(5, 100.0, 0.225): 0.851851851852
(6, 100.0, 0.275): 0.674074074074
(7, 1.0, 0.225): 0.933333333333
(8, 1.0, 0.25): 0.348148148148
(9, 100.0, 0.2): 0.933333333333
Loss on training set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 0.225): 0.0962962962963
(1, 100.0, 0.2): 0.0592592592593
(2, 10.0, 0.25): 0.303703703704
(3, 1.0, 0.2): 0.0740740740741
(4, 10.0, 0.225): 0.0740740740741
(5, 100.0, 0.225): 0.148148148148
(6, 100.0, 0.275): 0.325925925926
(7, 1.0, 0.225): 0.0666666666667
(8, 1.0, 0.25): 0.651851851852
(9, 100.0, 0.2): 0.0666666666667

Accuracy mean :0.8133333333333332
Std deviation :0.18152833862212284
Loss mean :0.18666666666666668
Std deviation :0.18152833862212286

On ALL the sets

Accuracy on all the set for every iteration, for every couple c,sigma (n_of_the_iter, c, sigma):
(0, 1.0, 0.2): 0.933333333333
(0, 1.0, 0.225): 0.933333333333
(0, 1.0, 0.25): 0.333333333333
(0, 1.0, 0.275): 0.733333333333
(0, 1.0, 0.3): 0.733333333333
(0, 10.0, 0.2): 0.333333333333
(0, 10.0, 0.225): 0.933333333333
(0, 10.0, 0.25): 0.333333333333
(0, 10.0, 0.275): 0.733333333333
(0, 10.0, 0.3): 0.733333333333
(0, 100.0, 0.2): 0.933333333333
(0, 100.0, 0.225): 0.933333333333
(0, 100.0, 0.25): 0.666666666667
(0, 100.0, 0.275): 0.733333333333
(0, 100.0, 0.3): 0.733333333333
(1, 1.0, 0.2): 0.866666666667
(1, 1.0, 0.225): 0.933333333333
(1, 1.0, 0.25): 0.666666666667
(1, 1.0, 0.275): 0.666666666667
(1, 1.0, 0.3): 0.6
(1, 10.0, 0.2): 0.866666666667
(1, 10.0, 0.225): 0.866666666667
(1, 10.0, 0.25): 0.733333333333
(1, 10.0, 0.275): 0.666666666667
(1, 10.0, 0.3): 0.6
(1, 100.0, 0.2): 0.933333333333
(1, 100.0, 0.225): 0.866666666667
(1, 100.0, 0.25): 0.866666666667
(1, 100.0, 0.275): 0.666666666667
(1, 100.0, 0.3): 0.6
(2, 1.0, 0.2): 0.733333333333
(2, 1.0, 0.225): 0.666666666667
(2, 1.0, 0.25): 1.0
(2, 1.0, 0.275): 0.666666666667
(2, 1.0, 0.3): 0.533333333333
(2, 10.0, 0.2): 0.733333333333
(2, 10.0, 0.225): 0.6
(2, 10.0, 0.25): 1.0
(2, 10.0, 0.275): 0.666666666667
(2, 10.0, 0.3): 0.533333333333
(2, 100.0, 0.2): 0.8
(2, 100.0, 0.225): 0.733333333333
(2, 100.0, 0.25): 1.0
(2, 100.0, 0.275): 0.733333333333
(2, 100.0, 0.3): 0.533333333333
(3, 0.1, 0.225): 0.933333333333
(3, 0.1, 0.25): 0.866666666667
(3, 0.1, 0.275): 0.866666666667
(3, 1.0, 0.2): 0.933333333333
(3, 1.0, 0.225): 0.866666666667
(3, 1.0, 0.25): 0.333333333333
(3, 1.0, 0.275): 0.333333333333
(3, 1.0, 0.3): 0.733333333333
(3, 10.0, 0.2): 0.933333333333
(3, 10.0, 0.225): 0.866666666667
(3, 10.0, 0.25): 0.666666666667
(3, 10.0, 0.275): 0.666666666667
(3, 10.0, 0.3): 0.733333333333
(3, 100.0, 0.2): 0.866666666667
(3, 100.0, 0.225): 0.866666666667
(3, 100.0, 0.25): 0.333333333333
(3, 100.0, 0.275): 0.333333333333
(3, 100.0, 0.3): 0.733333333333
(4, 0.1, 0.3): 0.733333333333
(4, 1.0, 0.2): 0.933333333333
(4, 1.0, 0.225): 0.933333333333
(4, 1.0, 0.25): 0.0
(4, 1.0, 0.275): 0.733333333333
(4, 1.0, 0.3): 0.733333333333
(4, 10.0, 0.2): 0.866666666667
(4, 10.0, 0.225): 0.933333333333
(4, 10.0, 0.25): 0.0
(4, 10.0, 0.275): 0.733333333333
(4, 10.0, 0.3): 0.733333333333
(4, 100.0, 0.2): 0.933333333333
(4, 100.0, 0.225): 0.933333333333
(4, 100.0, 0.25): 0.0
(4, 100.0, 0.275): 0.733333333333
(4, 100.0, 0.3): 0.733333333333
(5, 1.0, 0.2): 0.8
(5, 1.0, 0.225): 0.866666666667
(5, 1.0, 0.25): 0.933333333333
(5, 1.0, 0.275): 0.933333333333
(5, 1.0, 0.3): 0.733333333333
(5, 10.0, 0.2): 0.8
(5, 10.0, 0.225): 0.933333333333
(5, 10.0, 0.25): 0.933333333333
(5, 10.0, 0.275): 0.933333333333
(5, 10.0, 0.3): 0.733333333333
(5, 100.0, 0.2): 0.866666666667
(5, 100.0, 0.225): 0.933333333333
(5, 100.0, 0.25): 0.933333333333
(5, 100.0, 0.275): 0.933333333333
(5, 100.0, 0.3): 0.733333333333
(6, 0.1, 0.3): 0.533333333333
(6, 1.0, 0.2): 0.866666666667
(6, 1.0, 0.225): 0.866666666667
(6, 1.0, 0.25): 0.866666666667
(6, 1.0, 0.275): 0.533333333333
(6, 1.0, 0.3): 0.8
(6, 10.0, 0.2): 0.866666666667
(6, 10.0, 0.25): 0.866666666667
(6, 10.0, 0.275): 0.533333333333
(6, 10.0, 0.3): 0.533333333333
(6, 100.0, 0.2): 0.866666666667
(6, 100.0, 0.225): 0.866666666667
(6, 100.0, 0.25): 0.866666666667
(6, 100.0, 0.275): 0.866666666667
(6, 100.0, 0.3): 0.133333333333
(7, 1.0, 0.2): 0.866666666667
(7, 1.0, 0.225): 0.866666666667
(7, 1.0, 0.25): 0.666666666667
(7, 1.0, 0.275): 0.733333333333
(7, 1.0, 0.3): 0.666666666667
(7, 10.0, 0.2): 0.866666666667
(7, 10.0, 0.225): 0.266666666667
(7, 10.0, 0.25): 0.266666666667
(7, 10.0, 0.275): 0.733333333333
(7, 10.0, 0.3): 0.666666666667
(7, 100.0, 0.225): 0.866666666667
(7, 100.0, 0.25): 0.666666666667
(7, 100.0, 0.275): 0.733333333333
(7, 100.0, 0.3): 0.666666666667
(8, 1.0, 0.2): 1.0
(8, 1.0, 0.225): 0.866666666667
(8, 1.0, 0.25): 1.0
(8, 1.0, 0.275): 0.0666666666667
(8, 1.0, 0.3): 0.666666666667
(8, 10.0, 0.225): 0.866666666667
(8, 10.0, 0.25): 0.933333333333
(8, 10.0, 0.275): 0.0666666666667
(8, 10.0, 0.3): 0.666666666667
(8, 100.0, 0.225): 0.866666666667
(8, 100.0, 0.25): 1.0
(8, 100.0, 0.275): 0.666666666667
(8, 100.0, 0.3): 0.666666666667
(9, 1.0, 0.2): 0.933333333333
(9, 1.0, 0.225): 0.266666666667
(9, 1.0, 0.25): 0.733333333333
(9, 1.0, 0.275): 0.6
(9, 1.0, 0.3): 0.266666666667
(9, 10.0, 0.2): 1.0
(9, 10.0, 0.225): 0.8
(9, 10.0, 0.25): 0.733333333333
(9, 10.0, 0.275): 0.6
(9, 10.0, 0.3): 0.533333333333
(9, 100.0, 0.2): 1.0
(9, 100.0, 0.225): 0.8
(9, 100.0, 0.25): 0.733333333333
(9, 100.0, 0.275): 0.6
(9, 100.0, 0.3): 0.533333333333

Loss on all the set for every iteration, for every couple c,sigma (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 0.2): 0.0666666666667
(0, 1.0, 0.225): 0.0666666666667
(0, 1.0, 0.25): 0.666666666667
(0, 1.0, 0.275): 0.266666666667
(0, 1.0, 0.3): 0.266666666667
(0, 10.0, 0.2): 0.666666666667
(0, 10.0, 0.225): 0.0666666666667
(0, 10.0, 0.25): 0.666666666667
(0, 10.0, 0.275): 0.266666666667
(0, 10.0, 0.3): 0.266666666667
(0, 100.0, 0.2): 0.0666666666667
(0, 100.0, 0.225): 0.0666666666667
(0, 100.0, 0.25): 0.333333333333
(0, 100.0, 0.275): 0.266666666667
(0, 100.0, 0.3): 0.266666666667
(1, 1.0, 0.2): 0.133333333333
(1, 1.0, 0.225): 0.0666666666667
(1, 1.0, 0.25): 0.333333333333
(1, 1.0, 0.275): 0.333333333333
(1, 1.0, 0.3): 0.4
(1, 10.0, 0.2): 0.133333333333
(1, 10.0, 0.225): 0.133333333333
(1, 10.0, 0.25): 0.266666666667
(1, 10.0, 0.275): 0.333333333333
(1, 10.0, 0.3): 0.4
(1, 100.0, 0.2): 0.0666666666667
(1, 100.0, 0.225): 0.133333333333
(1, 100.0, 0.25): 0.133333333333
(1, 100.0, 0.275): 0.333333333333
(1, 100.0, 0.3): 0.4
(2, 1.0, 0.2): 0.266666666667
(2, 1.0, 0.225): 0.333333333333
(2, 1.0, 0.25): 0.0
(2, 1.0, 0.275): 0.333333333333
(2, 1.0, 0.3): 0.466666666667
(2, 10.0, 0.2): 0.266666666667
(2, 10.0, 0.225): 0.4
(2, 10.0, 0.25): 0.0
(2, 10.0, 0.275): 0.333333333333
(2, 10.0, 0.3): 0.466666666667
(2, 100.0, 0.2): 0.2
(2, 100.0, 0.225): 0.266666666667
(2, 100.0, 0.25): 0.0
(2, 100.0, 0.275): 0.266666666667
(2, 100.0, 0.3): 0.466666666667
(3, 0.1, 0.225): 0.0666666666667
(3, 0.1, 0.25): 0.133333333333
(3, 0.1, 0.275): 0.133333333333
(3, 1.0, 0.2): 0.0666666666667
(3, 1.0, 0.225): 0.133333333333
(3, 1.0, 0.25): 0.666666666667
(3, 1.0, 0.275): 0.666666666667
(3, 1.0, 0.3): 0.266666666667
(3, 10.0, 0.2): 0.0666666666667
(3, 10.0, 0.225): 0.133333333333
(3, 10.0, 0.25): 0.333333333333
(3, 10.0, 0.275): 0.333333333333
(3, 10.0, 0.3): 0.266666666667
(3, 100.0, 0.2): 0.133333333333
(3, 100.0, 0.225): 0.133333333333
(3, 100.0, 0.25): 0.666666666667
(3, 100.0, 0.275): 0.666666666667
(3, 100.0, 0.3): 0.266666666667
(4, 0.1, 0.3): 0.266666666667
(4, 1.0, 0.2): 0.0666666666667
(4, 1.0, 0.225): 0.0666666666667
(4, 1.0, 0.25): 1.0
(4, 1.0, 0.275): 0.266666666667
(4, 1.0, 0.3): 0.266666666667
(4, 10.0, 0.2): 0.133333333333
(4, 10.0, 0.225): 0.0666666666667
(4, 10.0, 0.25): 1.0
(4, 10.0, 0.275): 0.266666666667
(4, 10.0, 0.3): 0.266666666667
(4, 100.0, 0.2): 0.0666666666667
(4, 100.0, 0.225): 0.0666666666667
(4, 100.0, 0.25): 1.0
(4, 100.0, 0.275): 0.266666666667
(4, 100.0, 0.3): 0.266666666667
(5, 1.0, 0.2): 0.2
(5, 1.0, 0.225): 0.133333333333
(5, 1.0, 0.25): 0.0666666666667
(5, 1.0, 0.275): 0.0666666666667
(5, 1.0, 0.3): 0.266666666667
(5, 10.0, 0.2): 0.2
(5, 10.0, 0.225): 0.0666666666667
(5, 10.0, 0.25): 0.0666666666667
(5, 10.0, 0.275): 0.0666666666667
(5, 10.0, 0.3): 0.266666666667
(5, 100.0, 0.2): 0.133333333333
(5, 100.0, 0.225): 0.0666666666667
(5, 100.0, 0.25): 0.0666666666667
(5, 100.0, 0.275): 0.0666666666667
(5, 100.0, 0.3): 0.266666666667
(6, 0.1, 0.3): 0.466666666667
(6, 1.0, 0.2): 0.133333333333
(6, 1.0, 0.225): 0.133333333333
(6, 1.0, 0.25): 0.133333333333
(6, 1.0, 0.275): 0.466666666667
(6, 1.0, 0.3): 0.2
(6, 10.0, 0.2): 0.133333333333
(6, 10.0, 0.25): 0.133333333333
(6, 10.0, 0.275): 0.466666666667
(6, 10.0, 0.3): 0.466666666667
(6, 100.0, 0.2): 0.133333333333
(6, 100.0, 0.225): 0.133333333333
(6, 100.0, 0.25): 0.133333333333
(6, 100.0, 0.275): 0.133333333333
(6, 100.0, 0.3): 0.866666666667
(7, 1.0, 0.2): 0.133333333333
(7, 1.0, 0.225): 0.133333333333
(7, 1.0, 0.25): 0.333333333333
(7, 1.0, 0.275): 0.266666666667
(7, 1.0, 0.3): 0.333333333333
(7, 10.0, 0.2): 0.133333333333
(7, 10.0, 0.225): 0.733333333333
(7, 10.0, 0.25): 0.733333333333
(7, 10.0, 0.275): 0.266666666667
(7, 10.0, 0.3): 0.333333333333
(7, 100.0, 0.225): 0.133333333333
(7, 100.0, 0.25): 0.333333333333
(7, 100.0, 0.275): 0.266666666667
(7, 100.0, 0.3): 0.333333333333
(8, 1.0, 0.2): 0.0
(8, 1.0, 0.225): 0.133333333333
(8, 1.0, 0.25): 0.0
(8, 1.0, 0.275): 0.933333333333
(8, 1.0, 0.3): 0.333333333333
(8, 10.0, 0.225): 0.133333333333
(8, 10.0, 0.25): 0.0666666666667
(8, 10.0, 0.275): 0.933333333333
(8, 10.0, 0.3): 0.333333333333
(8, 100.0, 0.225): 0.133333333333
(8, 100.0, 0.25): 0.0
(8, 100.0, 0.275): 0.333333333333
(8, 100.0, 0.3): 0.333333333333
(9, 1.0, 0.2): 0.0666666666667
(9, 1.0, 0.225): 0.733333333333
(9, 1.0, 0.25): 0.266666666667
(9, 1.0, 0.275): 0.4
(9, 1.0, 0.3): 0.733333333333
(9, 10.0, 0.2): 0.0
(9, 10.0, 0.225): 0.2
(9, 10.0, 0.25): 0.266666666667
(9, 10.0, 0.275): 0.4
(9, 10.0, 0.3): 0.466666666667
(9, 100.0, 0.2): 0.0
(9, 100.0, 0.225): 0.2
(9, 100.0, 0.25): 0.266666666667
(9, 100.0, 0.275): 0.4
(9, 100.0, 0.3): 0.466666666667

Accuracy mean :0.7249448123620309
Std deviation :0.2234658610786964
Loss mean :0.2750551876379691
Std deviation :0.22346586107869637



QuantileConstPiecewise_3dim, number of iterations: 10

Parameters info
Tradeoffs parameter: [  0.1   1.   10.  100. ]
Gaussian kernel sigmas: [0.2   0.225 0.25  0.275 0.3  ]
Number of principal dimension considerated: 3
Dataset length: 150

Clusterization info
Minimum size of a cluster: in perc: 0.01, in int: 2
Type of mu: Binary Muzzifier
Fuzzifier: QuantileConstPiecewise
Cluster to label info
Force the number of cluster to be the number of different labels: False
Force the labels to be always represent by a cluster: False

Validation info
Conflict resolution: random


RESULTS

On TEST set

Accuracy on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 100.0, 0.3): 0.4
(1, 100.0, 0.3): 0.466666666667
(2, 10.0, 0.225): 0.266666666667
(3, 10.0, 0.3): 0.466666666667
(4, 100.0, 0.25): 0.4
(5, 1.0, 0.3): 0.466666666667
(6, 1.0, 0.275): 0.4
(7, 1.0, 0.275): 0.266666666667
(8, 1.0, 0.25): 0.333333333333
(9, 1.0, 0.25): 0.4

Loss on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 100.0, 0.3): 0.6
(1, 100.0, 0.3): 0.533333333333
(2, 10.0, 0.225): 0.733333333333
(3, 10.0, 0.3): 0.533333333333
(4, 100.0, 0.25): 0.6
(5, 1.0, 0.3): 0.533333333333
(6, 1.0, 0.275): 0.6
(7, 1.0, 0.275): 0.733333333333
(8, 1.0, 0.25): 0.666666666667
(9, 1.0, 0.25): 0.6

Accuracy mean :0.38666666666666666
Std deviation :0.07180219742846006
Loss mean :0.6133333333333333
Std deviation :0.07180219742846003

On TRAINING set

Accuracy on training set for every iteration (n_of_the_iter, c, sigma):
(0, 100.0, 0.3): 0.577777777778
(1, 100.0, 0.3): 0.562962962963
(2, 10.0, 0.225): 0.488888888889
(3, 10.0, 0.3): 0.540740740741
(4, 100.0, 0.25): 0.444444444444
(5, 1.0, 0.3): 0.533333333333
(6, 1.0, 0.275): 0.518518518519
(7, 1.0, 0.275): 0.318518518519
(8, 1.0, 0.25): 0.481481481481
(9, 1.0, 0.25): 0.511111111111
Loss on training set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 100.0, 0.3): 0.422222222222
(1, 100.0, 0.3): 0.437037037037
(2, 10.0, 0.225): 0.511111111111
(3, 10.0, 0.3): 0.459259259259
(4, 100.0, 0.25): 0.555555555556
(5, 1.0, 0.3): 0.466666666667
(6, 1.0, 0.275): 0.481481481481
(7, 1.0, 0.275): 0.681481481481
(8, 1.0, 0.25): 0.518518518519
(9, 1.0, 0.25): 0.488888888889

Accuracy mean :0.49777777777777776
Std deviation :0.0704911244644167
Loss mean :0.5022222222222222
Std deviation :0.0704911244644167

On ALL the sets

Accuracy on all the set for every iteration, for every couple c,sigma (n_of_the_iter, c, sigma):
(0, 1.0, 0.2): 0.333333333333
(0, 1.0, 0.225): 0.133333333333
(0, 1.0, 0.25): 0.266666666667
(0, 1.0, 0.275): 0.4
(0, 1.0, 0.3): 0.333333333333
(0, 10.0, 0.2): 0.133333333333
(0, 10.0, 0.225): 0.333333333333
(0, 10.0, 0.25): 0.266666666667
(0, 10.0, 0.275): 0.4
(0, 10.0, 0.3): 0.333333333333
(0, 100.0, 0.2): 0.266666666667
(0, 100.0, 0.225): 0.4
(0, 100.0, 0.25): 0.266666666667
(0, 100.0, 0.275): 0.266666666667
(0, 100.0, 0.3): 0.466666666667
(1, 0.1, 0.25): 0.4
(1, 0.1, 0.275): 0.6
(1, 1.0, 0.2): 0.4
(1, 1.0, 0.225): 0.4
(1, 1.0, 0.25): 0.533333333333
(1, 1.0, 0.275): 0.266666666667
(1, 1.0, 0.3): 0.4
(1, 10.0, 0.2): 0.533333333333
(1, 10.0, 0.225): 0.466666666667
(1, 10.0, 0.25): 0.4
(1, 10.0, 0.275): 0.466666666667
(1, 10.0, 0.3): 0.333333333333
(1, 100.0, 0.2): 0.4
(1, 100.0, 0.225): 0.4
(1, 100.0, 0.25): 0.466666666667
(1, 100.0, 0.275): 0.533333333333
(1, 100.0, 0.3): 0.6
(2, 1.0, 0.2): 0.266666666667
(2, 1.0, 0.225): 0.266666666667
(2, 1.0, 0.25): 0.333333333333
(2, 1.0, 0.275): 0.2
(2, 1.0, 0.3): 0.133333333333
(2, 10.0, 0.2): 0.0666666666667
(2, 10.0, 0.225): 0.6
(2, 10.0, 0.25): 0.333333333333
(2, 10.0, 0.275): 0.333333333333
(2, 10.0, 0.3): 0.133333333333
(2, 100.0, 0.2): 0.533333333333
(2, 100.0, 0.225): 0.333333333333
(2, 100.0, 0.25): 0.266666666667
(2, 100.0, 0.275): 0.266666666667
(2, 100.0, 0.3): 0.4
(3, 1.0, 0.2): 0.466666666667
(3, 1.0, 0.225): 0.333333333333
(3, 1.0, 0.25): 0.466666666667
(3, 1.0, 0.275): 0.466666666667
(3, 1.0, 0.3): 0.466666666667
(3, 10.0, 0.2): 0.266666666667
(3, 10.0, 0.225): 0.333333333333
(3, 10.0, 0.25): 0.533333333333
(3, 10.0, 0.275): 0.2
(3, 10.0, 0.3): 0.733333333333
(3, 100.0, 0.2): 0.2
(3, 100.0, 0.225): 0.333333333333
(3, 100.0, 0.25): 0.466666666667
(3, 100.0, 0.275): 0.266666666667
(3, 100.0, 0.3): 0.466666666667
(4, 1.0, 0.2): 0.266666666667
(4, 1.0, 0.225): 0.266666666667
(4, 1.0, 0.25): 0.4
(4, 1.0, 0.275): 0.333333333333
(4, 1.0, 0.3): 0.333333333333
(4, 10.0, 0.2): 0.2
(4, 10.0, 0.225): 0.2
(4, 10.0, 0.25): 0.4
(4, 10.0, 0.275): 0.333333333333
(4, 10.0, 0.3): 0.266666666667
(4, 100.0, 0.2): 0.333333333333
(4, 100.0, 0.225): 0.333333333333
(4, 100.0, 0.25): 0.466666666667
(4, 100.0, 0.275): 0.333333333333
(4, 100.0, 0.3): 0.466666666667
(5, 1.0, 0.2): 0.266666666667
(5, 1.0, 0.225): 0.0666666666667
(5, 1.0, 0.25): 0.266666666667
(5, 1.0, 0.275): 0.466666666667
(5, 1.0, 0.3): 0.666666666667
(5, 10.0, 0.2): 0.133333333333
(5, 10.0, 0.225): 0.2
(5, 10.0, 0.25): 0.266666666667
(5, 10.0, 0.275): 0.466666666667
(5, 10.0, 0.3): 0.6
(5, 100.0, 0.2): 0.266666666667
(5, 100.0, 0.225): 0.266666666667
(5, 100.0, 0.25): 0.266666666667
(5, 100.0, 0.275): 0.4
(5, 100.0, 0.3): 0.6
(6, 0.1, 0.3): 0.466666666667
(6, 1.0, 0.2): 0.133333333333
(6, 1.0, 0.225): 0.333333333333
(6, 1.0, 0.25): 0.466666666667
(6, 1.0, 0.275): 0.6
(6, 1.0, 0.3): 0.466666666667
(6, 10.0, 0.2): 0.266666666667
(6, 10.0, 0.225): 0.133333333333
(6, 10.0, 0.25): 0.4
(6, 10.0, 0.275): 0.333333333333
(6, 10.0, 0.3): 0.466666666667
(6, 100.0, 0.2): 0.333333333333
(6, 100.0, 0.225): 0.133333333333
(6, 100.0, 0.25): 0.333333333333
(6, 100.0, 0.275): 0.466666666667
(6, 100.0, 0.3): 0.466666666667
(7, 0.1, 0.3): 0.6
(7, 1.0, 0.2): 0.533333333333
(7, 1.0, 0.225): 0.333333333333
(7, 1.0, 0.25): 0.4
(7, 1.0, 0.275): 0.666666666667
(7, 1.0, 0.3): 0.6
(7, 10.0, 0.2): 0.466666666667
(7, 10.0, 0.225): 0.333333333333
(7, 10.0, 0.25): 0.333333333333
(7, 10.0, 0.275): 0.466666666667
(7, 10.0, 0.3): 0.466666666667
(7, 100.0, 0.2): 0.266666666667
(7, 100.0, 0.225): 0.466666666667
(7, 100.0, 0.25): 0.466666666667
(7, 100.0, 0.275): 0.4
(7, 100.0, 0.3): 0.533333333333
(8, 1.0, 0.2): 0.4
(8, 1.0, 0.225): 0.466666666667
(8, 1.0, 0.25): 0.6
(8, 1.0, 0.275): 0.333333333333
(8, 1.0, 0.3): 0.466666666667
(8, 10.0, 0.2): 0.533333333333
(8, 10.0, 0.225): 0.4
(8, 10.0, 0.25): 0.466666666667
(8, 10.0, 0.275): 0.333333333333
(8, 10.0, 0.3): 0.4
(8, 100.0, 0.2): 0.466666666667
(8, 100.0, 0.225): 0.333333333333
(8, 100.0, 0.25): 0.533333333333
(8, 100.0, 0.275): 0.4
(8, 100.0, 0.3): 0.4
(9, 1.0, 0.2): 0.533333333333
(9, 1.0, 0.225): 0.333333333333
(9, 1.0, 0.25): 0.6
(9, 1.0, 0.275): 0.4
(9, 1.0, 0.3): 0.333333333333
(9, 10.0, 0.2): 0.533333333333
(9, 10.0, 0.225): 0.266666666667
(9, 10.0, 0.25): 0.533333333333
(9, 10.0, 0.275): 0.4
(9, 10.0, 0.3): 0.533333333333
(9, 100.0, 0.2): 0.266666666667
(9, 100.0, 0.225): 0.4
(9, 100.0, 0.25): 0.4
(9, 100.0, 0.275): 0.4
(9, 100.0, 0.3): 0.2

Loss on all the set for every iteration, for every couple c,sigma (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 0.2): 0.666666666667
(0, 1.0, 0.225): 0.866666666667
(0, 1.0, 0.25): 0.733333333333
(0, 1.0, 0.275): 0.6
(0, 1.0, 0.3): 0.666666666667
(0, 10.0, 0.2): 0.866666666667
(0, 10.0, 0.225): 0.666666666667
(0, 10.0, 0.25): 0.733333333333
(0, 10.0, 0.275): 0.6
(0, 10.0, 0.3): 0.666666666667
(0, 100.0, 0.2): 0.733333333333
(0, 100.0, 0.225): 0.6
(0, 100.0, 0.25): 0.733333333333
(0, 100.0, 0.275): 0.733333333333
(0, 100.0, 0.3): 0.533333333333
(1, 0.1, 0.25): 0.6
(1, 0.1, 0.275): 0.4
(1, 1.0, 0.2): 0.6
(1, 1.0, 0.225): 0.6
(1, 1.0, 0.25): 0.466666666667
(1, 1.0, 0.275): 0.733333333333
(1, 1.0, 0.3): 0.6
(1, 10.0, 0.2): 0.466666666667
(1, 10.0, 0.225): 0.533333333333
(1, 10.0, 0.25): 0.6
(1, 10.0, 0.275): 0.533333333333
(1, 10.0, 0.3): 0.666666666667
(1, 100.0, 0.2): 0.6
(1, 100.0, 0.225): 0.6
(1, 100.0, 0.25): 0.533333333333
(1, 100.0, 0.275): 0.466666666667
(1, 100.0, 0.3): 0.4
(2, 1.0, 0.2): 0.733333333333
(2, 1.0, 0.225): 0.733333333333
(2, 1.0, 0.25): 0.666666666667
(2, 1.0, 0.275): 0.8
(2, 1.0, 0.3): 0.866666666667
(2, 10.0, 0.2): 0.933333333333
(2, 10.0, 0.225): 0.4
(2, 10.0, 0.25): 0.666666666667
(2, 10.0, 0.275): 0.666666666667
(2, 10.0, 0.3): 0.866666666667
(2, 100.0, 0.2): 0.466666666667
(2, 100.0, 0.225): 0.666666666667
(2, 100.0, 0.25): 0.733333333333
(2, 100.0, 0.275): 0.733333333333
(2, 100.0, 0.3): 0.6
(3, 1.0, 0.2): 0.533333333333
(3, 1.0, 0.225): 0.666666666667
(3, 1.0, 0.25): 0.533333333333
(3, 1.0, 0.275): 0.533333333333
(3, 1.0, 0.3): 0.533333333333
(3, 10.0, 0.2): 0.733333333333
(3, 10.0, 0.225): 0.666666666667
(3, 10.0, 0.25): 0.466666666667
(3, 10.0, 0.275): 0.8
(3, 10.0, 0.3): 0.266666666667
(3, 100.0, 0.2): 0.8
(3, 100.0, 0.225): 0.666666666667
(3, 100.0, 0.25): 0.533333333333
(3, 100.0, 0.275): 0.733333333333
(3, 100.0, 0.3): 0.533333333333
(4, 1.0, 0.2): 0.733333333333
(4, 1.0, 0.225): 0.733333333333
(4, 1.0, 0.25): 0.6
(4, 1.0, 0.275): 0.666666666667
(4, 1.0, 0.3): 0.666666666667
(4, 10.0, 0.2): 0.8
(4, 10.0, 0.225): 0.8
(4, 10.0, 0.25): 0.6
(4, 10.0, 0.275): 0.666666666667
(4, 10.0, 0.3): 0.733333333333
(4, 100.0, 0.2): 0.666666666667
(4, 100.0, 0.225): 0.666666666667
(4, 100.0, 0.25): 0.533333333333
(4, 100.0, 0.275): 0.666666666667
(4, 100.0, 0.3): 0.533333333333
(5, 1.0, 0.2): 0.733333333333
(5, 1.0, 0.225): 0.933333333333
(5, 1.0, 0.25): 0.733333333333
(5, 1.0, 0.275): 0.533333333333
(5, 1.0, 0.3): 0.333333333333
(5, 10.0, 0.2): 0.866666666667
(5, 10.0, 0.225): 0.8
(5, 10.0, 0.25): 0.733333333333
(5, 10.0, 0.275): 0.533333333333
(5, 10.0, 0.3): 0.4
(5, 100.0, 0.2): 0.733333333333
(5, 100.0, 0.225): 0.733333333333
(5, 100.0, 0.25): 0.733333333333
(5, 100.0, 0.275): 0.6
(5, 100.0, 0.3): 0.4
(6, 0.1, 0.3): 0.533333333333
(6, 1.0, 0.2): 0.866666666667
(6, 1.0, 0.225): 0.666666666667
(6, 1.0, 0.25): 0.533333333333
(6, 1.0, 0.275): 0.4
(6, 1.0, 0.3): 0.533333333333
(6, 10.0, 0.2): 0.733333333333
(6, 10.0, 0.225): 0.866666666667
(6, 10.0, 0.25): 0.6
(6, 10.0, 0.275): 0.666666666667
(6, 10.0, 0.3): 0.533333333333
(6, 100.0, 0.2): 0.666666666667
(6, 100.0, 0.225): 0.866666666667
(6, 100.0, 0.25): 0.666666666667
(6, 100.0, 0.275): 0.533333333333
(6, 100.0, 0.3): 0.533333333333
(7, 0.1, 0.3): 0.4
(7, 1.0, 0.2): 0.466666666667
(7, 1.0, 0.225): 0.666666666667
(7, 1.0, 0.25): 0.6
(7, 1.0, 0.275): 0.333333333333
(7, 1.0, 0.3): 0.4
(7, 10.0, 0.2): 0.533333333333
(7, 10.0, 0.225): 0.666666666667
(7, 10.0, 0.25): 0.666666666667
(7, 10.0, 0.275): 0.533333333333
(7, 10.0, 0.3): 0.533333333333
(7, 100.0, 0.2): 0.733333333333
(7, 100.0, 0.225): 0.533333333333
(7, 100.0, 0.25): 0.533333333333
(7, 100.0, 0.275): 0.6
(7, 100.0, 0.3): 0.466666666667
(8, 1.0, 0.2): 0.6
(8, 1.0, 0.225): 0.533333333333
(8, 1.0, 0.25): 0.4
(8, 1.0, 0.275): 0.666666666667
(8, 1.0, 0.3): 0.533333333333
(8, 10.0, 0.2): 0.466666666667
(8, 10.0, 0.225): 0.6
(8, 10.0, 0.25): 0.533333333333
(8, 10.0, 0.275): 0.666666666667
(8, 10.0, 0.3): 0.6
(8, 100.0, 0.2): 0.533333333333
(8, 100.0, 0.225): 0.666666666667
(8, 100.0, 0.25): 0.466666666667
(8, 100.0, 0.275): 0.6
(8, 100.0, 0.3): 0.6
(9, 1.0, 0.2): 0.466666666667
(9, 1.0, 0.225): 0.666666666667
(9, 1.0, 0.25): 0.4
(9, 1.0, 0.275): 0.6
(9, 1.0, 0.3): 0.666666666667
(9, 10.0, 0.2): 0.466666666667
(9, 10.0, 0.225): 0.733333333333
(9, 10.0, 0.25): 0.466666666667
(9, 10.0, 0.275): 0.6
(9, 10.0, 0.3): 0.466666666667
(9, 100.0, 0.2): 0.733333333333
(9, 100.0, 0.225): 0.6
(9, 100.0, 0.25): 0.6
(9, 100.0, 0.275): 0.6
(9, 100.0, 0.3): 0.8

Accuracy mean :0.3805194805194805
Std deviation :0.1300294837639771
Loss mean :0.6194805194805195
Std deviation :0.1300294837639771



Crisp_4dim, number of iterations: 10

Parameters info
Tradeoffs parameter: [  0.1   1.   10.  100. ]
Gaussian kernel sigmas: [0.2   0.225 0.25  0.275 0.3  ]
Number of principal dimension considerated: 4
Dataset length: 150

Clusterization info
Minimum size of a cluster: in perc: 0.01, in int: 2
Type of mu: Binary Muzzifier
Fuzzifier: Crisp
Cluster to label info
Force the number of cluster to be the number of different labels: False
Force the labels to be always represent by a cluster: False

Validation info
Conflict resolution: random


RESULTS

On TEST set

Accuracy on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 0.1, 0.25): 0.4
(1, 1.0, 0.275): 0.266666666667
(2, 10.0, 0.3): 0.333333333333
(3, 100.0, 0.25): 0.533333333333
(4, 10.0, 0.3): 0.266666666667
(5, 10.0, 0.275): 0.4
(6, 1.0, 0.25): 0.333333333333
(7, 0.1, 0.3): 0.333333333333
(8, 10.0, 0.225): 0.4
(9, 0.1, 0.3): 0.266666666667

Loss on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 0.1, 0.25): 0.6
(1, 1.0, 0.275): 0.733333333333
(2, 10.0, 0.3): 0.666666666667
(3, 100.0, 0.25): 0.466666666667
(4, 10.0, 0.3): 0.733333333333
(5, 10.0, 0.275): 0.6
(6, 1.0, 0.25): 0.666666666667
(7, 0.1, 0.3): 0.666666666667
(8, 10.0, 0.225): 0.6
(9, 0.1, 0.3): 0.733333333333

Accuracy mean :0.35333333333333333
Std deviation :0.07916228058025279
Loss mean :0.6466666666666667
Std deviation :0.07916228058025276

On TRAINING set

Accuracy on training set for every iteration (n_of_the_iter, c, sigma):
(0, 0.1, 0.25): 0.385185185185
(1, 1.0, 0.275): 0.385185185185
(2, 10.0, 0.3): 0.525925925926
(3, 100.0, 0.25): 0.540740740741
(4, 10.0, 0.3): 0.503703703704
(5, 10.0, 0.275): 0.407407407407
(6, 1.0, 0.25): 0.407407407407
(7, 0.1, 0.3): 0.562962962963
(8, 10.0, 0.225): 0.422222222222
(9, 0.1, 0.3): 0.57037037037
Loss on training set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 0.1, 0.25): 0.614814814815
(1, 1.0, 0.275): 0.614814814815
(2, 10.0, 0.3): 0.474074074074
(3, 100.0, 0.25): 0.459259259259
(4, 10.0, 0.3): 0.496296296296
(5, 10.0, 0.275): 0.592592592593
(6, 1.0, 0.25): 0.592592592593
(7, 0.1, 0.3): 0.437037037037
(8, 10.0, 0.225): 0.577777777778
(9, 0.1, 0.3): 0.42962962963

Accuracy mean :0.47111111111111115
Std deviation :0.0724412632793935
Loss mean :0.5288888888888889
Std deviation :0.07244126327939347

On ALL the sets

Accuracy on all the set for every iteration, for every couple c,sigma (n_of_the_iter, c, sigma):
(0, 0.1, 0.225): 0.466666666667
(0, 0.1, 0.25): 0.533333333333
(0, 0.1, 0.275): 0.466666666667
(0, 0.1, 0.3): 0.4
(0, 1.0, 0.2): 0.4
(0, 1.0, 0.225): 0.466666666667
(0, 1.0, 0.25): 0.266666666667
(0, 1.0, 0.275): 0.333333333333
(0, 1.0, 0.3): 0.2
(0, 10.0, 0.2): 0.4
(0, 10.0, 0.225): 0.466666666667
(0, 10.0, 0.25): 0.133333333333
(0, 10.0, 0.275): 0.333333333333
(0, 10.0, 0.3): 0.266666666667
(0, 100.0, 0.2): 0.466666666667
(0, 100.0, 0.225): 0.333333333333
(0, 100.0, 0.25): 0.266666666667
(0, 100.0, 0.275): 0.4
(0, 100.0, 0.3): 0.4
(1, 0.1, 0.25): 0.266666666667
(1, 0.1, 0.3): 0.466666666667
(1, 1.0, 0.2): 0.0
(1, 1.0, 0.225): 0.266666666667
(1, 1.0, 0.25): 0.2
(1, 1.0, 0.275): 0.733333333333
(1, 1.0, 0.3): 0.0
(1, 10.0, 0.225): 0.466666666667
(1, 10.0, 0.25): 0.333333333333
(1, 10.0, 0.275): 0.0
(1, 10.0, 0.3): 0.0
(1, 100.0, 0.2): 0.0
(1, 100.0, 0.225): 0.333333333333
(1, 100.0, 0.25): 0.266666666667
(1, 100.0, 0.275): 0.333333333333
(1, 100.0, 0.3): 0.0
(2, 0.1, 0.275): 0.2
(2, 1.0, 0.2): 0.333333333333
(2, 1.0, 0.225): 0.133333333333
(2, 1.0, 0.25): 0.2
(2, 1.0, 0.275): 0.4
(2, 1.0, 0.3): 0.4
(2, 10.0, 0.2): 0.333333333333
(2, 10.0, 0.225): 0.333333333333
(2, 10.0, 0.25): 0.266666666667
(2, 10.0, 0.275): 0.4
(2, 10.0, 0.3): 0.466666666667
(2, 100.0, 0.2): 0.133333333333
(2, 100.0, 0.225): 0.266666666667
(2, 100.0, 0.25): 0.266666666667
(2, 100.0, 0.275): 0.133333333333
(2, 100.0, 0.3): 0.333333333333
(3, 1.0, 0.2): 0.2
(3, 1.0, 0.225): 0.2
(3, 1.0, 0.25): 0.466666666667
(3, 1.0, 0.275): 0.4
(3, 1.0, 0.3): 0.466666666667
(3, 10.0, 0.2): 0.266666666667
(3, 10.0, 0.225): 0.0666666666667
(3, 10.0, 0.25): 0.4
(3, 10.0, 0.275): 0.466666666667
(3, 10.0, 0.3): 0.333333333333
(3, 100.0, 0.2): 0.266666666667
(3, 100.0, 0.225): 0.333333333333
(3, 100.0, 0.25): 0.466666666667
(3, 100.0, 0.275): 0.333333333333
(3, 100.0, 0.3): 0.266666666667
(4, 0.1, 0.3): 0.333333333333
(4, 1.0, 0.225): 0.4
(4, 1.0, 0.25): 0.2
(4, 1.0, 0.275): 0.266666666667
(4, 1.0, 0.3): 0.466666666667
(4, 10.0, 0.225): 0.466666666667
(4, 10.0, 0.25): 0.4
(4, 10.0, 0.275): 0.333333333333
(4, 10.0, 0.3): 0.533333333333
(4, 100.0, 0.225): 0.466666666667
(4, 100.0, 0.25): 0.333333333333
(4, 100.0, 0.275): 0.2
(4, 100.0, 0.3): 0.466666666667
(5, 0.1, 0.3): 0.4
(5, 1.0, 0.2): 0.466666666667
(5, 1.0, 0.225): 0.466666666667
(5, 1.0, 0.25): 0.2
(5, 1.0, 0.275): 0.466666666667
(5, 1.0, 0.3): 0.2
(5, 10.0, 0.2): 0.333333333333
(5, 10.0, 0.225): 0.2
(5, 10.0, 0.25): 0.2
(5, 10.0, 0.275): 0.6
(5, 10.0, 0.3): 0.2
(5, 100.0, 0.2): 0.333333333333
(5, 100.0, 0.225): 0.133333333333
(5, 100.0, 0.25): 0.2
(5, 100.0, 0.275): 0.466666666667
(5, 100.0, 0.3): 0.2
(6, 0.1, 0.275): 0.466666666667
(6, 0.1, 0.3): 0.333333333333
(6, 1.0, 0.2): 0.533333333333
(6, 1.0, 0.225): 0.266666666667
(6, 1.0, 0.25): 0.6
(6, 1.0, 0.275): 0.266666666667
(6, 1.0, 0.3): 0.466666666667
(6, 10.0, 0.2): 0.266666666667
(6, 10.0, 0.225): 0.266666666667
(6, 10.0, 0.25): 0.466666666667
(6, 10.0, 0.275): 0.266666666667
(6, 10.0, 0.3): 0.4
(6, 100.0, 0.2): 0.2
(6, 100.0, 0.225): 0.266666666667
(6, 100.0, 0.25): 0.333333333333
(6, 100.0, 0.275): 0.333333333333
(6, 100.0, 0.3): 0.333333333333
(7, 0.1, 0.3): 0.533333333333
(7, 1.0, 0.2): 0.333333333333
(7, 1.0, 0.225): 0.4
(7, 1.0, 0.25): 0.533333333333
(7, 1.0, 0.275): 0.333333333333
(7, 1.0, 0.3): 0.466666666667
(7, 10.0, 0.2): 0.333333333333
(7, 10.0, 0.225): 0.2
(7, 10.0, 0.25): 0.266666666667
(7, 10.0, 0.275): 0.333333333333
(7, 10.0, 0.3): 0.333333333333
(7, 100.0, 0.225): 0.2
(7, 100.0, 0.25): 0.266666666667
(7, 100.0, 0.275): 0.333333333333
(7, 100.0, 0.3): 0.266666666667
(8, 0.1, 0.225): 0.333333333333
(8, 0.1, 0.3): 0.333333333333
(8, 1.0, 0.225): 0.4
(8, 1.0, 0.25): 0.2
(8, 1.0, 0.275): 0.2
(8, 1.0, 0.3): 0.466666666667
(8, 10.0, 0.225): 0.533333333333
(8, 10.0, 0.25): 0.266666666667
(8, 10.0, 0.275): 0.4
(8, 10.0, 0.3): 0.533333333333
(8, 100.0, 0.225): 0.466666666667
(8, 100.0, 0.25): 0.466666666667
(8, 100.0, 0.275): 0.4
(8, 100.0, 0.3): 0.333333333333
(9, 0.1, 0.3): 0.666666666667
(9, 1.0, 0.2): 0.266666666667
(9, 1.0, 0.225): 0.666666666667
(9, 1.0, 0.25): 0.333333333333
(9, 1.0, 0.3): 0.533333333333
(9, 10.0, 0.2): 0.333333333333
(9, 10.0, 0.225): 0.466666666667
(9, 10.0, 0.25): 0.533333333333
(9, 10.0, 0.3): 0.533333333333
(9, 100.0, 0.225): 0.666666666667
(9, 100.0, 0.25): 0.466666666667
(9, 100.0, 0.3): 0.533333333333

Loss on all the set for every iteration, for every couple c,sigma (n_of_the_iter, best_c, best_sigma):
(0, 0.1, 0.225): 0.533333333333
(0, 0.1, 0.25): 0.466666666667
(0, 0.1, 0.275): 0.533333333333
(0, 0.1, 0.3): 0.6
(0, 1.0, 0.2): 0.6
(0, 1.0, 0.225): 0.533333333333
(0, 1.0, 0.25): 0.733333333333
(0, 1.0, 0.275): 0.666666666667
(0, 1.0, 0.3): 0.8
(0, 10.0, 0.2): 0.6
(0, 10.0, 0.225): 0.533333333333
(0, 10.0, 0.25): 0.866666666667
(0, 10.0, 0.275): 0.666666666667
(0, 10.0, 0.3): 0.733333333333
(0, 100.0, 0.2): 0.533333333333
(0, 100.0, 0.225): 0.666666666667
(0, 100.0, 0.25): 0.733333333333
(0, 100.0, 0.275): 0.6
(0, 100.0, 0.3): 0.6
(1, 0.1, 0.25): 0.733333333333
(1, 0.1, 0.3): 0.533333333333
(1, 1.0, 0.2): 1.0
(1, 1.0, 0.225): 0.733333333333
(1, 1.0, 0.25): 0.8
(1, 1.0, 0.275): 0.266666666667
(1, 1.0, 0.3): 1.0
(1, 10.0, 0.225): 0.533333333333
(1, 10.0, 0.25): 0.666666666667
(1, 10.0, 0.275): 1.0
(1, 10.0, 0.3): 1.0
(1, 100.0, 0.2): 1.0
(1, 100.0, 0.225): 0.666666666667
(1, 100.0, 0.25): 0.733333333333
(1, 100.0, 0.275): 0.666666666667
(1, 100.0, 0.3): 1.0
(2, 0.1, 0.275): 0.8
(2, 1.0, 0.2): 0.666666666667
(2, 1.0, 0.225): 0.866666666667
(2, 1.0, 0.25): 0.8
(2, 1.0, 0.275): 0.6
(2, 1.0, 0.3): 0.6
(2, 10.0, 0.2): 0.666666666667
(2, 10.0, 0.225): 0.666666666667
(2, 10.0, 0.25): 0.733333333333
(2, 10.0, 0.275): 0.6
(2, 10.0, 0.3): 0.533333333333
(2, 100.0, 0.2): 0.866666666667
(2, 100.0, 0.225): 0.733333333333
(2, 100.0, 0.25): 0.733333333333
(2, 100.0, 0.275): 0.866666666667
(2, 100.0, 0.3): 0.666666666667
(3, 1.0, 0.2): 0.8
(3, 1.0, 0.225): 0.8
(3, 1.0, 0.25): 0.533333333333
(3, 1.0, 0.275): 0.6
(3, 1.0, 0.3): 0.533333333333
(3, 10.0, 0.2): 0.733333333333
(3, 10.0, 0.225): 0.933333333333
(3, 10.0, 0.25): 0.6
(3, 10.0, 0.275): 0.533333333333
(3, 10.0, 0.3): 0.666666666667
(3, 100.0, 0.2): 0.733333333333
(3, 100.0, 0.225): 0.666666666667
(3, 100.0, 0.25): 0.533333333333
(3, 100.0, 0.275): 0.666666666667
(3, 100.0, 0.3): 0.733333333333
(4, 0.1, 0.3): 0.666666666667
(4, 1.0, 0.225): 0.6
(4, 1.0, 0.25): 0.8
(4, 1.0, 0.275): 0.733333333333
(4, 1.0, 0.3): 0.533333333333
(4, 10.0, 0.225): 0.533333333333
(4, 10.0, 0.25): 0.6
(4, 10.0, 0.275): 0.666666666667
(4, 10.0, 0.3): 0.466666666667
(4, 100.0, 0.225): 0.533333333333
(4, 100.0, 0.25): 0.666666666667
(4, 100.0, 0.275): 0.8
(4, 100.0, 0.3): 0.533333333333
(5, 0.1, 0.3): 0.6
(5, 1.0, 0.2): 0.533333333333
(5, 1.0, 0.225): 0.533333333333
(5, 1.0, 0.25): 0.8
(5, 1.0, 0.275): 0.533333333333
(5, 1.0, 0.3): 0.8
(5, 10.0, 0.2): 0.666666666667
(5, 10.0, 0.225): 0.8
(5, 10.0, 0.25): 0.8
(5, 10.0, 0.275): 0.4
(5, 10.0, 0.3): 0.8
(5, 100.0, 0.2): 0.666666666667
(5, 100.0, 0.225): 0.866666666667
(5, 100.0, 0.25): 0.8
(5, 100.0, 0.275): 0.533333333333
(5, 100.0, 0.3): 0.8
(6, 0.1, 0.275): 0.533333333333
(6, 0.1, 0.3): 0.666666666667
(6, 1.0, 0.2): 0.466666666667
(6, 1.0, 0.225): 0.733333333333
(6, 1.0, 0.25): 0.4
(6, 1.0, 0.275): 0.733333333333
(6, 1.0, 0.3): 0.533333333333
(6, 10.0, 0.2): 0.733333333333
(6, 10.0, 0.225): 0.733333333333
(6, 10.0, 0.25): 0.533333333333
(6, 10.0, 0.275): 0.733333333333
(6, 10.0, 0.3): 0.6
(6, 100.0, 0.2): 0.8
(6, 100.0, 0.225): 0.733333333333
(6, 100.0, 0.25): 0.666666666667
(6, 100.0, 0.275): 0.666666666667
(6, 100.0, 0.3): 0.666666666667
(7, 0.1, 0.3): 0.466666666667
(7, 1.0, 0.2): 0.666666666667
(7, 1.0, 0.225): 0.6
(7, 1.0, 0.25): 0.466666666667
(7, 1.0, 0.275): 0.666666666667
(7, 1.0, 0.3): 0.533333333333
(7, 10.0, 0.2): 0.666666666667
(7, 10.0, 0.225): 0.8
(7, 10.0, 0.25): 0.733333333333
(7, 10.0, 0.275): 0.666666666667
(7, 10.0, 0.3): 0.666666666667
(7, 100.0, 0.225): 0.8
(7, 100.0, 0.25): 0.733333333333
(7, 100.0, 0.275): 0.666666666667
(7, 100.0, 0.3): 0.733333333333
(8, 0.1, 0.225): 0.666666666667
(8, 0.1, 0.3): 0.666666666667
(8, 1.0, 0.225): 0.6
(8, 1.0, 0.25): 0.8
(8, 1.0, 0.275): 0.8
(8, 1.0, 0.3): 0.533333333333
(8, 10.0, 0.225): 0.466666666667
(8, 10.0, 0.25): 0.733333333333
(8, 10.0, 0.275): 0.6
(8, 10.0, 0.3): 0.466666666667
(8, 100.0, 0.225): 0.533333333333
(8, 100.0, 0.25): 0.533333333333
(8, 100.0, 0.275): 0.6
(8, 100.0, 0.3): 0.666666666667
(9, 0.1, 0.3): 0.333333333333
(9, 1.0, 0.2): 0.733333333333
(9, 1.0, 0.225): 0.333333333333
(9, 1.0, 0.25): 0.666666666667
(9, 1.0, 0.3): 0.466666666667
(9, 10.0, 0.2): 0.666666666667
(9, 10.0, 0.225): 0.533333333333
(9, 10.0, 0.25): 0.466666666667
(9, 10.0, 0.3): 0.466666666667
(9, 100.0, 0.225): 0.333333333333
(9, 100.0, 0.25): 0.533333333333
(9, 100.0, 0.3): 0.466666666667

Accuracy mean :0.3442265795206972
Std deviation :0.14125848232495836
Loss mean :0.6557734204793029
Std deviation :0.14125848232495836



Linear_4dim, number of iterations: 10

Parameters info
Tradeoffs parameter: [  0.1   1.   10.  100. ]
Gaussian kernel sigmas: [0.2   0.225 0.25  0.275 0.3  ]
Number of principal dimension considerated: 4
Dataset length: 150

Clusterization info
Minimum size of a cluster: in perc: 0.01, in int: 2
Type of mu: Binary Muzzifier
Fuzzifier: Linear
Cluster to label info
Force the number of cluster to be the number of different labels: False
Force the labels to be always represent by a cluster: False

Validation info
Conflict resolution: random


RESULTS

On TEST set

Accuracy on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 0.225): 0.8
(1, 100.0, 0.225): 0.666666666667
(2, 100.0, 0.25): 0.8
(3, 10.0, 0.225): 0.866666666667
(4, 10.0, 0.225): 0.333333333333
(5, 100.0, 0.275): 0.533333333333
(6, 1.0, 0.25): 0.666666666667
(7, 1.0, 0.25): 0.6
(8, 100.0, 0.225): 0.933333333333
(9, 1.0, 0.225): 0.933333333333

Loss on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 0.225): 0.2
(1, 100.0, 0.225): 0.333333333333
(2, 100.0, 0.25): 0.2
(3, 10.0, 0.225): 0.133333333333
(4, 10.0, 0.225): 0.666666666667
(5, 100.0, 0.275): 0.466666666667
(6, 1.0, 0.25): 0.333333333333
(7, 1.0, 0.25): 0.4
(8, 100.0, 0.225): 0.0666666666667
(9, 1.0, 0.225): 0.0666666666667

Accuracy mean :0.7133333333333333
Std deviation :0.18147543451754936
Loss mean :0.2866666666666667
Std deviation :0.18147543451754933

On TRAINING set

Accuracy on training set for every iteration (n_of_the_iter, c, sigma):
(0, 1.0, 0.225): 0.874074074074
(1, 100.0, 0.225): 0.851851851852
(2, 100.0, 0.25): 0.940740740741
(3, 10.0, 0.225): 0.807407407407
(4, 10.0, 0.225): 0.385185185185
(5, 100.0, 0.275): 0.681481481481
(6, 1.0, 0.25): 0.666666666667
(7, 1.0, 0.25): 0.674074074074
(8, 100.0, 0.225): 0.807407407407
(9, 1.0, 0.225): 0.851851851852
Loss on training set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 0.225): 0.125925925926
(1, 100.0, 0.225): 0.148148148148
(2, 100.0, 0.25): 0.0592592592593
(3, 10.0, 0.225): 0.192592592593
(4, 10.0, 0.225): 0.614814814815
(5, 100.0, 0.275): 0.318518518519
(6, 1.0, 0.25): 0.333333333333
(7, 1.0, 0.25): 0.325925925926
(8, 100.0, 0.225): 0.192592592593
(9, 1.0, 0.225): 0.148148148148

Accuracy mean :0.754074074074074
Std deviation :0.1514737841021779
Loss mean :0.24592592592592594
Std deviation :0.15147378410217788

On ALL the sets

Accuracy on all the set for every iteration, for every couple c,sigma (n_of_the_iter, c, sigma):
(0, 0.1, 0.25): 0.6
(0, 0.1, 0.275): 0.6
(0, 0.1, 0.3): 0.6
(0, 1.0, 0.2): 0.6
(0, 1.0, 0.225): 0.666666666667
(0, 1.0, 0.25): 0.6
(0, 1.0, 0.275): 0.6
(0, 1.0, 0.3): 0.6
(0, 10.0, 0.2): 0.6
(0, 10.0, 0.225): 0.666666666667
(0, 10.0, 0.25): 0.6
(0, 10.0, 0.275): 0.6
(0, 10.0, 0.3): 0.6
(0, 100.0, 0.2): 0.2
(0, 100.0, 0.225): 0.666666666667
(0, 100.0, 0.25): 0.6
(0, 100.0, 0.275): 0.6
(0, 100.0, 0.3): 0.6
(1, 0.1, 0.25): 0.333333333333
(1, 0.1, 0.275): 0.533333333333
(1, 0.1, 0.3): 0.533333333333
(1, 1.0, 0.2): 0.533333333333
(1, 1.0, 0.225): 0.6
(1, 1.0, 0.25): 0.333333333333
(1, 1.0, 0.275): 0.533333333333
(1, 1.0, 0.3): 0.533333333333
(1, 10.0, 0.2): 0.533333333333
(1, 10.0, 0.225): 0.333333333333
(1, 10.0, 0.25): 0.333333333333
(1, 10.0, 0.275): 0.533333333333
(1, 10.0, 0.3): 0.533333333333
(1, 100.0, 0.2): 0.533333333333
(1, 100.0, 0.225): 0.733333333333
(1, 100.0, 0.25): 0.333333333333
(1, 100.0, 0.275): 0.533333333333
(1, 100.0, 0.3): 0.533333333333
(2, 0.1, 0.275): 0.4
(2, 0.1, 0.3): 0.4
(2, 1.0, 0.2): 0.133333333333
(2, 1.0, 0.225): 0.333333333333
(2, 1.0, 0.25): 0.8
(2, 1.0, 0.275): 0.4
(2, 1.0, 0.3): 0.4
(2, 10.0, 0.2): 0.133333333333
(2, 10.0, 0.225): 0.4
(2, 10.0, 0.25): 0.8
(2, 10.0, 0.275): 0.4
(2, 10.0, 0.3): 0.4
(2, 100.0, 0.2): 0.133333333333
(2, 100.0, 0.225): 0.333333333333
(2, 100.0, 0.25): 0.8
(2, 100.0, 0.275): 0.4
(2, 100.0, 0.3): 0.4
(3, 0.1, 0.25): 0.6
(3, 0.1, 0.275): 0.6
(3, 0.1, 0.3): 0.333333333333
(3, 1.0, 0.2): 0.333333333333
(3, 1.0, 0.225): 0.8
(3, 1.0, 0.25): 0.6
(3, 1.0, 0.275): 0.6
(3, 1.0, 0.3): 0.6
(3, 10.0, 0.2): 0.666666666667
(3, 10.0, 0.225): 0.866666666667
(3, 10.0, 0.25): 0.6
(3, 10.0, 0.275): 0.6
(3, 10.0, 0.3): 0.6
(3, 100.0, 0.2): 0.666666666667
(3, 100.0, 0.225): 0.8
(3, 100.0, 0.25): 0.6
(3, 100.0, 0.275): 0.6
(3, 100.0, 0.3): 0.333333333333
(4, 0.1, 0.275): 0.666666666667
(4, 0.1, 0.3): 0.666666666667
(4, 1.0, 0.2): 0.133333333333
(4, 1.0, 0.225): 0.933333333333
(4, 1.0, 0.25): 0.733333333333
(4, 1.0, 0.275): 0.666666666667
(4, 1.0, 0.3): 0.666666666667
(4, 10.0, 0.2): 0.133333333333
(4, 10.0, 0.225): 0.933333333333
(4, 10.0, 0.25): 0.333333333333
(4, 10.0, 0.275): 0.666666666667
(4, 10.0, 0.3): 0.666666666667
(4, 100.0, 0.225): 0.933333333333
(4, 100.0, 0.25): 0.333333333333
(4, 100.0, 0.275): 0.666666666667
(4, 100.0, 0.3): 0.666666666667
(5, 0.1, 0.25): 0.533333333333
(5, 0.1, 0.3): 0.533333333333
(5, 1.0, 0.2): 0.733333333333
(5, 1.0, 0.225): 0.266666666667
(5, 1.0, 0.25): 0.533333333333
(5, 1.0, 0.275): 0.933333333333
(5, 1.0, 0.3): 0.533333333333
(5, 10.0, 0.2): 0.733333333333
(5, 10.0, 0.225): 0.266666666667
(5, 10.0, 0.25): 0.533333333333
(5, 10.0, 0.275): 0.533333333333
(5, 10.0, 0.3): 0.533333333333
(5, 100.0, 0.2): 0.733333333333
(5, 100.0, 0.225): 0.266666666667
(5, 100.0, 0.25): 0.533333333333
(5, 100.0, 0.275): 0.933333333333
(5, 100.0, 0.3): 0.533333333333
(6, 0.1, 0.25): 0.8
(6, 0.1, 0.275): 0.533333333333
(6, 0.1, 0.3): 0.533333333333
(6, 1.0, 0.2): 0.4
(6, 1.0, 0.225): 0.533333333333
(6, 1.0, 0.25): 0.866666666667
(6, 1.0, 0.275): 0.533333333333
(6, 1.0, 0.3): 0.533333333333
(6, 10.0, 0.2): 0.466666666667
(6, 10.0, 0.225): 0.533333333333
(6, 10.0, 0.25): 0.866666666667
(6, 10.0, 0.275): 0.533333333333
(6, 10.0, 0.3): 0.533333333333
(6, 100.0, 0.2): 0.4
(6, 100.0, 0.225): 0.533333333333
(6, 100.0, 0.25): 0.866666666667
(6, 100.0, 0.275): 0.533333333333
(6, 100.0, 0.3): 0.533333333333
(7, 0.1, 0.275): 0.466666666667
(7, 1.0, 0.2): 0.133333333333
(7, 1.0, 0.225): 0.866666666667
(7, 1.0, 0.25): 0.866666666667
(7, 1.0, 0.275): 0.466666666667
(7, 1.0, 0.3): 0.133333333333
(7, 10.0, 0.2): 0.133333333333
(7, 10.0, 0.225): 0.466666666667
(7, 10.0, 0.25): 0.866666666667
(7, 10.0, 0.275): 0.466666666667
(7, 10.0, 0.3): 0.133333333333
(7, 100.0, 0.2): 0.133333333333
(7, 100.0, 0.225): 0.466666666667
(7, 100.0, 0.25): 0.866666666667
(7, 100.0, 0.275): 0.466666666667
(7, 100.0, 0.3): 0.133333333333
(8, 0.1, 0.3): 0.533333333333
(8, 1.0, 0.2): 0.533333333333
(8, 1.0, 0.225): 0.8
(8, 1.0, 0.25): 0.2
(8, 1.0, 0.275): 0.533333333333
(8, 1.0, 0.3): 0.533333333333
(8, 10.0, 0.2): 0.533333333333
(8, 10.0, 0.225): 0.8
(8, 10.0, 0.25): 0.2
(8, 10.0, 0.275): 0.533333333333
(8, 10.0, 0.3): 0.533333333333
(8, 100.0, 0.2): 0.533333333333
(8, 100.0, 0.225): 0.8
(8, 100.0, 0.25): 0.2
(8, 100.0, 0.275): 0.533333333333
(8, 100.0, 0.3): 0.533333333333
(9, 0.1, 0.275): 0.666666666667
(9, 0.1, 0.3): 0.666666666667
(9, 1.0, 0.2): 0.4
(9, 1.0, 0.225): 0.666666666667
(9, 1.0, 0.25): 0.4
(9, 1.0, 0.275): 0.666666666667
(9, 1.0, 0.3): 0.666666666667
(9, 10.0, 0.2): 0.4
(9, 10.0, 0.225): 0.666666666667
(9, 10.0, 0.25): 0.4
(9, 10.0, 0.275): 0.666666666667
(9, 10.0, 0.3): 0.666666666667
(9, 100.0, 0.25): 0.4
(9, 100.0, 0.275): 0.666666666667
(9, 100.0, 0.3): 0.666666666667

Loss on all the set for every iteration, for every couple c,sigma (n_of_the_iter, best_c, best_sigma):
(0, 0.1, 0.25): 0.4
(0, 0.1, 0.275): 0.4
(0, 0.1, 0.3): 0.4
(0, 1.0, 0.2): 0.4
(0, 1.0, 0.225): 0.333333333333
(0, 1.0, 0.25): 0.4
(0, 1.0, 0.275): 0.4
(0, 1.0, 0.3): 0.4
(0, 10.0, 0.2): 0.4
(0, 10.0, 0.225): 0.333333333333
(0, 10.0, 0.25): 0.4
(0, 10.0, 0.275): 0.4
(0, 10.0, 0.3): 0.4
(0, 100.0, 0.2): 0.8
(0, 100.0, 0.225): 0.333333333333
(0, 100.0, 0.25): 0.4
(0, 100.0, 0.275): 0.4
(0, 100.0, 0.3): 0.4
(1, 0.1, 0.25): 0.666666666667
(1, 0.1, 0.275): 0.466666666667
(1, 0.1, 0.3): 0.466666666667
(1, 1.0, 0.2): 0.466666666667
(1, 1.0, 0.225): 0.4
(1, 1.0, 0.25): 0.666666666667
(1, 1.0, 0.275): 0.466666666667
(1, 1.0, 0.3): 0.466666666667
(1, 10.0, 0.2): 0.466666666667
(1, 10.0, 0.225): 0.666666666667
(1, 10.0, 0.25): 0.666666666667
(1, 10.0, 0.275): 0.466666666667
(1, 10.0, 0.3): 0.466666666667
(1, 100.0, 0.2): 0.466666666667
(1, 100.0, 0.225): 0.266666666667
(1, 100.0, 0.25): 0.666666666667
(1, 100.0, 0.275): 0.466666666667
(1, 100.0, 0.3): 0.466666666667
(2, 0.1, 0.275): 0.6
(2, 0.1, 0.3): 0.6
(2, 1.0, 0.2): 0.866666666667
(2, 1.0, 0.225): 0.666666666667
(2, 1.0, 0.25): 0.2
(2, 1.0, 0.275): 0.6
(2, 1.0, 0.3): 0.6
(2, 10.0, 0.2): 0.866666666667
(2, 10.0, 0.225): 0.6
(2, 10.0, 0.25): 0.2
(2, 10.0, 0.275): 0.6
(2, 10.0, 0.3): 0.6
(2, 100.0, 0.2): 0.866666666667
(2, 100.0, 0.225): 0.666666666667
(2, 100.0, 0.25): 0.2
(2, 100.0, 0.275): 0.6
(2, 100.0, 0.3): 0.6
(3, 0.1, 0.25): 0.4
(3, 0.1, 0.275): 0.4
(3, 0.1, 0.3): 0.666666666667
(3, 1.0, 0.2): 0.666666666667
(3, 1.0, 0.225): 0.2
(3, 1.0, 0.25): 0.4
(3, 1.0, 0.275): 0.4
(3, 1.0, 0.3): 0.4
(3, 10.0, 0.2): 0.333333333333
(3, 10.0, 0.225): 0.133333333333
(3, 10.0, 0.25): 0.4
(3, 10.0, 0.275): 0.4
(3, 10.0, 0.3): 0.4
(3, 100.0, 0.2): 0.333333333333
(3, 100.0, 0.225): 0.2
(3, 100.0, 0.25): 0.4
(3, 100.0, 0.275): 0.4
(3, 100.0, 0.3): 0.666666666667
(4, 0.1, 0.275): 0.333333333333
(4, 0.1, 0.3): 0.333333333333
(4, 1.0, 0.2): 0.866666666667
(4, 1.0, 0.225): 0.0666666666667
(4, 1.0, 0.25): 0.266666666667
(4, 1.0, 0.275): 0.333333333333
(4, 1.0, 0.3): 0.333333333333
(4, 10.0, 0.2): 0.866666666667
(4, 10.0, 0.225): 0.0666666666667
(4, 10.0, 0.25): 0.666666666667
(4, 10.0, 0.275): 0.333333333333
(4, 10.0, 0.3): 0.333333333333
(4, 100.0, 0.225): 0.0666666666667
(4, 100.0, 0.25): 0.666666666667
(4, 100.0, 0.275): 0.333333333333
(4, 100.0, 0.3): 0.333333333333
(5, 0.1, 0.25): 0.466666666667
(5, 0.1, 0.3): 0.466666666667
(5, 1.0, 0.2): 0.266666666667
(5, 1.0, 0.225): 0.733333333333
(5, 1.0, 0.25): 0.466666666667
(5, 1.0, 0.275): 0.0666666666667
(5, 1.0, 0.3): 0.466666666667
(5, 10.0, 0.2): 0.266666666667
(5, 10.0, 0.225): 0.733333333333
(5, 10.0, 0.25): 0.466666666667
(5, 10.0, 0.275): 0.466666666667
(5, 10.0, 0.3): 0.466666666667
(5, 100.0, 0.2): 0.266666666667
(5, 100.0, 0.225): 0.733333333333
(5, 100.0, 0.25): 0.466666666667
(5, 100.0, 0.275): 0.0666666666667
(5, 100.0, 0.3): 0.466666666667
(6, 0.1, 0.25): 0.2
(6, 0.1, 0.275): 0.466666666667
(6, 0.1, 0.3): 0.466666666667
(6, 1.0, 0.2): 0.6
(6, 1.0, 0.225): 0.466666666667
(6, 1.0, 0.25): 0.133333333333
(6, 1.0, 0.275): 0.466666666667
(6, 1.0, 0.3): 0.466666666667
(6, 10.0, 0.2): 0.533333333333
(6, 10.0, 0.225): 0.466666666667
(6, 10.0, 0.25): 0.133333333333
(6, 10.0, 0.275): 0.466666666667
(6, 10.0, 0.3): 0.466666666667
(6, 100.0, 0.2): 0.6
(6, 100.0, 0.225): 0.466666666667
(6, 100.0, 0.25): 0.133333333333
(6, 100.0, 0.275): 0.466666666667
(6, 100.0, 0.3): 0.466666666667
(7, 0.1, 0.275): 0.533333333333
(7, 1.0, 0.2): 0.866666666667
(7, 1.0, 0.225): 0.133333333333
(7, 1.0, 0.25): 0.133333333333
(7, 1.0, 0.275): 0.533333333333
(7, 1.0, 0.3): 0.866666666667
(7, 10.0, 0.2): 0.866666666667
(7, 10.0, 0.225): 0.533333333333
(7, 10.0, 0.25): 0.133333333333
(7, 10.0, 0.275): 0.533333333333
(7, 10.0, 0.3): 0.866666666667
(7, 100.0, 0.2): 0.866666666667
(7, 100.0, 0.225): 0.533333333333
(7, 100.0, 0.25): 0.133333333333
(7, 100.0, 0.275): 0.533333333333
(7, 100.0, 0.3): 0.866666666667
(8, 0.1, 0.3): 0.466666666667
(8, 1.0, 0.2): 0.466666666667
(8, 1.0, 0.225): 0.2
(8, 1.0, 0.25): 0.8
(8, 1.0, 0.275): 0.466666666667
(8, 1.0, 0.3): 0.466666666667
(8, 10.0, 0.2): 0.466666666667
(8, 10.0, 0.225): 0.2
(8, 10.0, 0.25): 0.8
(8, 10.0, 0.275): 0.466666666667
(8, 10.0, 0.3): 0.466666666667
(8, 100.0, 0.2): 0.466666666667
(8, 100.0, 0.225): 0.2
(8, 100.0, 0.25): 0.8
(8, 100.0, 0.275): 0.466666666667
(8, 100.0, 0.3): 0.466666666667
(9, 0.1, 0.275): 0.333333333333
(9, 0.1, 0.3): 0.333333333333
(9, 1.0, 0.2): 0.6
(9, 1.0, 0.225): 0.333333333333
(9, 1.0, 0.25): 0.6
(9, 1.0, 0.275): 0.333333333333
(9, 1.0, 0.3): 0.333333333333
(9, 10.0, 0.2): 0.6
(9, 10.0, 0.225): 0.333333333333
(9, 10.0, 0.25): 0.6
(9, 10.0, 0.275): 0.333333333333
(9, 10.0, 0.3): 0.333333333333
(9, 100.0, 0.25): 0.6
(9, 100.0, 0.275): 0.333333333333
(9, 100.0, 0.3): 0.333333333333

Accuracy mean :0.5408284023668638
Std deviation :0.19234966415994673
Loss mean :0.45917159763313614
Std deviation :0.1923496641599467



QuantileConstPiecewise_4dim, number of iterations: 10

Parameters info
Tradeoffs parameter: [  0.1   1.   10.  100. ]
Gaussian kernel sigmas: [0.2   0.225 0.25  0.275 0.3  ]
Number of principal dimension considerated: 4
Dataset length: 150

Clusterization info
Minimum size of a cluster: in perc: 0.01, in int: 2
Type of mu: Binary Muzzifier
Fuzzifier: QuantileConstPiecewise
Cluster to label info
Force the number of cluster to be the number of different labels: False
Force the labels to be always represent by a cluster: False

Validation info
Conflict resolution: random


RESULTS

On TEST set

Accuracy on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 100.0, 0.275): 0.2
(1, 100.0, 0.3): 0.266666666667
(2, 0.1, 0.3): 0.4
(3, 100.0, 0.3): 0.0666666666667
(4, 10.0, 0.225): 0.266666666667
(5, 10.0, 0.2): 0.266666666667
(6, 100.0, 0.25): 0.333333333333
(7, 100.0, 0.3): 0.466666666667
(8, 1.0, 0.3): 0.266666666667
(9, 100.0, 0.275): 0.266666666667

Loss on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 100.0, 0.275): 0.8
(1, 100.0, 0.3): 0.733333333333
(2, 0.1, 0.3): 0.6
(3, 100.0, 0.3): 0.933333333333
(4, 10.0, 0.225): 0.733333333333
(5, 10.0, 0.2): 0.733333333333
(6, 100.0, 0.25): 0.666666666667
(7, 100.0, 0.3): 0.533333333333
(8, 1.0, 0.3): 0.733333333333
(9, 100.0, 0.275): 0.733333333333

Accuracy mean :0.27999999999999997
Std deviation :0.1024152766382481
Loss mean :0.72
Std deviation :0.10241527663824812

On TRAINING set

Accuracy on training set for every iteration (n_of_the_iter, c, sigma):
(0, 100.0, 0.275): 0.496296296296
(1, 100.0, 0.3): 0.525925925926
(2, 0.1, 0.3): 0.42962962963
(3, 100.0, 0.3): 0.362962962963
(4, 10.0, 0.225): 0.437037037037
(5, 10.0, 0.2): 0.362962962963
(6, 100.0, 0.25): 0.525925925926
(7, 100.0, 0.3): 0.533333333333
(8, 1.0, 0.3): 0.340740740741
(9, 100.0, 0.275): 0.548148148148
Loss on training set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 100.0, 0.275): 0.503703703704
(1, 100.0, 0.3): 0.474074074074
(2, 0.1, 0.3): 0.57037037037
(3, 100.0, 0.3): 0.637037037037
(4, 10.0, 0.225): 0.562962962963
(5, 10.0, 0.2): 0.637037037037
(6, 100.0, 0.25): 0.474074074074
(7, 100.0, 0.3): 0.466666666667
(8, 1.0, 0.3): 0.659259259259
(9, 100.0, 0.275): 0.451851851852

Accuracy mean :0.45629629629629626
Std deviation :0.07584548584397158
Loss mean :0.5437037037037038
Std deviation :0.07584548584397158

On ALL the sets

Accuracy on all the set for every iteration, for every couple c,sigma (n_of_the_iter, c, sigma):
(0, 1.0, 0.2): 0.266666666667
(0, 1.0, 0.225): 0.2
(0, 1.0, 0.25): 0.4
(0, 1.0, 0.275): 0.266666666667
(0, 1.0, 0.3): 0.4
(0, 10.0, 0.2): 0.333333333333
(0, 10.0, 0.225): 0.4
(0, 10.0, 0.25): 0.4
(0, 10.0, 0.275): 0.4
(0, 10.0, 0.3): 0.266666666667
(0, 100.0, 0.2): 0.266666666667
(0, 100.0, 0.225): 0.2
(0, 100.0, 0.25): 0.266666666667
(0, 100.0, 0.275): 0.533333333333
(0, 100.0, 0.3): 0.4
(1, 0.1, 0.3): 0.466666666667
(1, 1.0, 0.225): 0.4
(1, 1.0, 0.25): 0.333333333333
(1, 1.0, 0.275): 0.266666666667
(1, 1.0, 0.3): 0.466666666667
(1, 10.0, 0.225): 0.333333333333
(1, 10.0, 0.25): 0.266666666667
(1, 10.0, 0.275): 0.4
(1, 10.0, 0.3): 0.466666666667
(1, 100.0, 0.2): 0.266666666667
(1, 100.0, 0.225): 0.466666666667
(1, 100.0, 0.25): 0.333333333333
(1, 100.0, 0.275): 0.4
(1, 100.0, 0.3): 0.533333333333
(2, 0.1, 0.25): 0.466666666667
(2, 0.1, 0.3): 0.666666666667
(2, 1.0, 0.225): 0.266666666667
(2, 1.0, 0.25): 0.533333333333
(2, 1.0, 0.275): 0.333333333333
(2, 1.0, 0.3): 0.6
(2, 10.0, 0.225): 0.4
(2, 10.0, 0.25): 0.666666666667
(2, 10.0, 0.275): 0.466666666667
(2, 10.0, 0.3): 0.6
(2, 100.0, 0.2): 0.533333333333
(2, 100.0, 0.225): 0.4
(2, 100.0, 0.25): 0.333333333333
(2, 100.0, 0.275): 0.2
(2, 100.0, 0.3): 0.666666666667
(3, 0.1, 0.25): 0.133333333333
(3, 0.1, 0.275): 0.533333333333
(3, 0.1, 0.3): 0.6
(3, 1.0, 0.2): 0.333333333333
(3, 1.0, 0.225): 0.4
(3, 1.0, 0.25): 0.2
(3, 1.0, 0.275): 0.266666666667
(3, 1.0, 0.3): 0.4
(3, 10.0, 0.2): 0.133333333333
(3, 10.0, 0.225): 0.2
(3, 10.0, 0.25): 0.4
(3, 10.0, 0.275): 0.333333333333
(3, 10.0, 0.3): 0.333333333333
(3, 100.0, 0.2): 0.266666666667
(3, 100.0, 0.225): 0.266666666667
(3, 100.0, 0.25): 0.333333333333
(3, 100.0, 0.275): 0.4
(3, 100.0, 0.3): 0.6
(4, 0.1, 0.275): 0.266666666667
(4, 0.1, 0.3): 0.4
(4, 1.0, 0.2): 0.2
(4, 1.0, 0.225): 0.266666666667
(4, 1.0, 0.25): 0.4
(4, 1.0, 0.275): 0.4
(4, 1.0, 0.3): 0.2
(4, 10.0, 0.2): 0.333333333333
(4, 10.0, 0.225): 0.466666666667
(4, 10.0, 0.25): 0.466666666667
(4, 10.0, 0.275): 0.333333333333
(4, 10.0, 0.3): 0.0666666666667
(4, 100.0, 0.2): 0.2
(4, 100.0, 0.225): 0.4
(4, 100.0, 0.25): 0.4
(4, 100.0, 0.275): 0.333333333333
(4, 100.0, 0.3): 0.2
(5, 1.0, 0.2): 0.333333333333
(5, 1.0, 0.25): 0.266666666667
(5, 1.0, 0.275): 0.133333333333
(5, 1.0, 0.3): 0.4
(5, 10.0, 0.2): 0.4
(5, 10.0, 0.25): 0.2
(5, 10.0, 0.275): 0.133333333333
(5, 10.0, 0.3): 0.266666666667
(5, 100.0, 0.2): 0.2
(5, 100.0, 0.225): 0.4
(5, 100.0, 0.25): 0.4
(5, 100.0, 0.275): 0.133333333333
(5, 100.0, 0.3): 0.133333333333
(6, 0.1, 0.25): 0.533333333333
(6, 0.1, 0.275): 0.4
(6, 0.1, 0.3): 0.466666666667
(6, 1.0, 0.2): 0.133333333333
(6, 1.0, 0.225): 0.466666666667
(6, 1.0, 0.25): 0.333333333333
(6, 1.0, 0.275): 0.4
(6, 1.0, 0.3): 0.533333333333
(6, 10.0, 0.2): 0.133333333333
(6, 10.0, 0.225): 0.333333333333
(6, 10.0, 0.25): 0.266666666667
(6, 10.0, 0.275): 0.133333333333
(6, 10.0, 0.3): 0.333333333333
(6, 100.0, 0.2): 0.133333333333
(6, 100.0, 0.225): 0.4
(6, 100.0, 0.25): 0.6
(6, 100.0, 0.275): 0.2
(6, 100.0, 0.3): 0.6
(7, 1.0, 0.225): 0.2
(7, 1.0, 0.25): 0.466666666667
(7, 1.0, 0.275): 0.266666666667
(7, 1.0, 0.3): 0.533333333333
(7, 10.0, 0.225): 0.266666666667
(7, 10.0, 0.25): 0.533333333333
(7, 10.0, 0.275): 0.466666666667
(7, 10.0, 0.3): 0.4
(7, 100.0, 0.225): 0.266666666667
(7, 100.0, 0.25): 0.266666666667
(7, 100.0, 0.275): 0.466666666667
(7, 100.0, 0.3): 0.533333333333
(8, 0.1, 0.275): 0.4
(8, 0.1, 0.3): 0.2
(8, 1.0, 0.2): 0.133333333333
(8, 1.0, 0.225): 0.4
(8, 1.0, 0.25): 0.2
(8, 1.0, 0.275): 0.4
(8, 1.0, 0.3): 0.533333333333
(8, 10.0, 0.2): 0.333333333333
(8, 10.0, 0.225): 0.4
(8, 10.0, 0.25): 0.266666666667
(8, 10.0, 0.275): 0.333333333333
(8, 10.0, 0.3): 0.2
(8, 100.0, 0.2): 0.266666666667
(8, 100.0, 0.225): 0.466666666667
(8, 100.0, 0.25): 0.266666666667
(8, 100.0, 0.275): 0.333333333333
(8, 100.0, 0.3): 0.333333333333
(9, 0.1, 0.25): 0.266666666667
(9, 0.1, 0.275): 0.4
(9, 0.1, 0.3): 0.4
(9, 1.0, 0.2): 0.2
(9, 1.0, 0.225): 0.2
(9, 1.0, 0.25): 0.466666666667
(9, 1.0, 0.275): 0.333333333333
(9, 1.0, 0.3): 0.4
(9, 10.0, 0.2): 0.2
(9, 10.0, 0.225): 0.333333333333
(9, 10.0, 0.25): 0.266666666667
(9, 10.0, 0.275): 0.6
(9, 10.0, 0.3): 0.4
(9, 100.0, 0.2): 0.2
(9, 100.0, 0.225): 0.333333333333
(9, 100.0, 0.25): 0.266666666667
(9, 100.0, 0.275): 0.666666666667
(9, 100.0, 0.3): 0.333333333333

Loss on all the set for every iteration, for every couple c,sigma (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 0.2): 0.733333333333
(0, 1.0, 0.225): 0.8
(0, 1.0, 0.25): 0.6
(0, 1.0, 0.275): 0.733333333333
(0, 1.0, 0.3): 0.6
(0, 10.0, 0.2): 0.666666666667
(0, 10.0, 0.225): 0.6
(0, 10.0, 0.25): 0.6
(0, 10.0, 0.275): 0.6
(0, 10.0, 0.3): 0.733333333333
(0, 100.0, 0.2): 0.733333333333
(0, 100.0, 0.225): 0.8
(0, 100.0, 0.25): 0.733333333333
(0, 100.0, 0.275): 0.466666666667
(0, 100.0, 0.3): 0.6
(1, 0.1, 0.3): 0.533333333333
(1, 1.0, 0.225): 0.6
(1, 1.0, 0.25): 0.666666666667
(1, 1.0, 0.275): 0.733333333333
(1, 1.0, 0.3): 0.533333333333
(1, 10.0, 0.225): 0.666666666667
(1, 10.0, 0.25): 0.733333333333
(1, 10.0, 0.275): 0.6
(1, 10.0, 0.3): 0.533333333333
(1, 100.0, 0.2): 0.733333333333
(1, 100.0, 0.225): 0.533333333333
(1, 100.0, 0.25): 0.666666666667
(1, 100.0, 0.275): 0.6
(1, 100.0, 0.3): 0.466666666667
(2, 0.1, 0.25): 0.533333333333
(2, 0.1, 0.3): 0.333333333333
(2, 1.0, 0.225): 0.733333333333
(2, 1.0, 0.25): 0.466666666667
(2, 1.0, 0.275): 0.666666666667
(2, 1.0, 0.3): 0.4
(2, 10.0, 0.225): 0.6
(2, 10.0, 0.25): 0.333333333333
(2, 10.0, 0.275): 0.533333333333
(2, 10.0, 0.3): 0.4
(2, 100.0, 0.2): 0.466666666667
(2, 100.0, 0.225): 0.6
(2, 100.0, 0.25): 0.666666666667
(2, 100.0, 0.275): 0.8
(2, 100.0, 0.3): 0.333333333333
(3, 0.1, 0.25): 0.866666666667
(3, 0.1, 0.275): 0.466666666667
(3, 0.1, 0.3): 0.4
(3, 1.0, 0.2): 0.666666666667
(3, 1.0, 0.225): 0.6
(3, 1.0, 0.25): 0.8
(3, 1.0, 0.275): 0.733333333333
(3, 1.0, 0.3): 0.6
(3, 10.0, 0.2): 0.866666666667
(3, 10.0, 0.225): 0.8
(3, 10.0, 0.25): 0.6
(3, 10.0, 0.275): 0.666666666667
(3, 10.0, 0.3): 0.666666666667
(3, 100.0, 0.2): 0.733333333333
(3, 100.0, 0.225): 0.733333333333
(3, 100.0, 0.25): 0.666666666667
(3, 100.0, 0.275): 0.6
(3, 100.0, 0.3): 0.4
(4, 0.1, 0.275): 0.733333333333
(4, 0.1, 0.3): 0.6
(4, 1.0, 0.2): 0.8
(4, 1.0, 0.225): 0.733333333333
(4, 1.0, 0.25): 0.6
(4, 1.0, 0.275): 0.6
(4, 1.0, 0.3): 0.8
(4, 10.0, 0.2): 0.666666666667
(4, 10.0, 0.225): 0.533333333333
(4, 10.0, 0.25): 0.533333333333
(4, 10.0, 0.275): 0.666666666667
(4, 10.0, 0.3): 0.933333333333
(4, 100.0, 0.2): 0.8
(4, 100.0, 0.225): 0.6
(4, 100.0, 0.25): 0.6
(4, 100.0, 0.275): 0.666666666667
(4, 100.0, 0.3): 0.8
(5, 1.0, 0.2): 0.666666666667
(5, 1.0, 0.25): 0.733333333333
(5, 1.0, 0.275): 0.866666666667
(5, 1.0, 0.3): 0.6
(5, 10.0, 0.2): 0.6
(5, 10.0, 0.25): 0.8
(5, 10.0, 0.275): 0.866666666667
(5, 10.0, 0.3): 0.733333333333
(5, 100.0, 0.2): 0.8
(5, 100.0, 0.225): 0.6
(5, 100.0, 0.25): 0.6
(5, 100.0, 0.275): 0.866666666667
(5, 100.0, 0.3): 0.866666666667
(6, 0.1, 0.25): 0.466666666667
(6, 0.1, 0.275): 0.6
(6, 0.1, 0.3): 0.533333333333
(6, 1.0, 0.2): 0.866666666667
(6, 1.0, 0.225): 0.533333333333
(6, 1.0, 0.25): 0.666666666667
(6, 1.0, 0.275): 0.6
(6, 1.0, 0.3): 0.466666666667
(6, 10.0, 0.2): 0.866666666667
(6, 10.0, 0.225): 0.666666666667
(6, 10.0, 0.25): 0.733333333333
(6, 10.0, 0.275): 0.866666666667
(6, 10.0, 0.3): 0.666666666667
(6, 100.0, 0.2): 0.866666666667
(6, 100.0, 0.225): 0.6
(6, 100.0, 0.25): 0.4
(6, 100.0, 0.275): 0.8
(6, 100.0, 0.3): 0.4
(7, 1.0, 0.225): 0.8
(7, 1.0, 0.25): 0.533333333333
(7, 1.0, 0.275): 0.733333333333
(7, 1.0, 0.3): 0.466666666667
(7, 10.0, 0.225): 0.733333333333
(7, 10.0, 0.25): 0.466666666667
(7, 10.0, 0.275): 0.533333333333
(7, 10.0, 0.3): 0.6
(7, 100.0, 0.225): 0.733333333333
(7, 100.0, 0.25): 0.733333333333
(7, 100.0, 0.275): 0.533333333333
(7, 100.0, 0.3): 0.466666666667
(8, 0.1, 0.275): 0.6
(8, 0.1, 0.3): 0.8
(8, 1.0, 0.2): 0.866666666667
(8, 1.0, 0.225): 0.6
(8, 1.0, 0.25): 0.8
(8, 1.0, 0.275): 0.6
(8, 1.0, 0.3): 0.466666666667
(8, 10.0, 0.2): 0.666666666667
(8, 10.0, 0.225): 0.6
(8, 10.0, 0.25): 0.733333333333
(8, 10.0, 0.275): 0.666666666667
(8, 10.0, 0.3): 0.8
(8, 100.0, 0.2): 0.733333333333
(8, 100.0, 0.225): 0.533333333333
(8, 100.0, 0.25): 0.733333333333
(8, 100.0, 0.275): 0.666666666667
(8, 100.0, 0.3): 0.666666666667
(9, 0.1, 0.25): 0.733333333333
(9, 0.1, 0.275): 0.6
(9, 0.1, 0.3): 0.6
(9, 1.0, 0.2): 0.8
(9, 1.0, 0.225): 0.8
(9, 1.0, 0.25): 0.533333333333
(9, 1.0, 0.275): 0.666666666667
(9, 1.0, 0.3): 0.6
(9, 10.0, 0.2): 0.8
(9, 10.0, 0.225): 0.666666666667
(9, 10.0, 0.25): 0.733333333333
(9, 10.0, 0.275): 0.4
(9, 10.0, 0.3): 0.6
(9, 100.0, 0.2): 0.8
(9, 100.0, 0.225): 0.666666666667
(9, 100.0, 0.25): 0.733333333333
(9, 100.0, 0.275): 0.333333333333
(9, 100.0, 0.3): 0.666666666667

Accuracy mean :0.3515923566878981
Std deviation :0.13218431982370543
Loss mean :0.6484076433121018
Std deviation :0.13218431982370543



