Crisp_LinearMu_2dim, number of iterations: 10

Parameters info
Tradeoffs parameter: [  0.1   1.   10.  100. ]
Using same parameters for all the clusterization: True
Gaussian kernel sigmas: [0.2   0.225 0.25  0.275 0.3  ]
Number of principal dimension considerated: 2
Dataset length: 150

Clusterization info
Minimum size of a cluster: in perc: 0.01, in int: 2
Type of mu: Linear Muzzifier
Fuzzifier: Crisp
Cluster graph uses all connections: False
Cluster to label info
Force the number of cluster to be the number of different labels: False
Force the labels to be always represent by a cluster: False

Validation info
Conflict resolution: random
Number of cluster considerated for validation (top_k): None


RESULTS

On TEST set

Accuracy on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 1.0, 0.225): 0.466666666667
(1, 1.0, 1.0, 0.225): 0.6
(2, 100.0, 100.0, 0.25): 0.4
(3, 1.0, 1.0, 0.225): 0.8
(4, 100.0, 100.0, 0.3): 0.466666666667
(5, 1.0, 1.0, 0.275): 0.4
(6, 100.0, 100.0, 0.225): 0.6
(7, 10.0, 10.0, 0.25): 0.533333333333
(8, 100.0, 100.0, 0.225): 0.6
(9, 10.0, 10.0, 0.2): 0.333333333333

Loss on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 1.0, 0.225): 0.533333333333
(1, 1.0, 1.0, 0.225): 0.4
(2, 100.0, 100.0, 0.25): 0.6
(3, 1.0, 1.0, 0.225): 0.2
(4, 100.0, 100.0, 0.3): 0.533333333333
(5, 1.0, 1.0, 0.275): 0.6
(6, 100.0, 100.0, 0.225): 0.4
(7, 10.0, 10.0, 0.25): 0.466666666667
(8, 100.0, 100.0, 0.225): 0.4
(9, 10.0, 10.0, 0.2): 0.666666666667

Accuracy mean :0.52
Std deviation :0.12927146286443544
Loss mean :0.48
Std deviation :0.12927146286443542

On TRAINING set

Accuracy on training set for every iteration (n_of_the_iter, c, sigma):
(0, 1.0, 1.0, 0.225): 0.644444444444
(1, 1.0, 1.0, 0.225): 0.651851851852
(2, 100.0, 100.0, 0.25): 0.622222222222
(3, 1.0, 1.0, 0.225): 0.607407407407
(4, 100.0, 100.0, 0.3): 0.585185185185
(5, 1.0, 1.0, 0.275): 0.607407407407
(6, 100.0, 100.0, 0.225): 0.562962962963
(7, 10.0, 10.0, 0.25): 0.725925925926
(8, 100.0, 100.0, 0.225): 0.777777777778
(9, 10.0, 10.0, 0.2): 0.6
Loss on training set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 1.0, 0.225): 0.355555555556
(1, 1.0, 1.0, 0.225): 0.348148148148
(2, 100.0, 100.0, 0.25): 0.377777777778
(3, 1.0, 1.0, 0.225): 0.392592592593
(4, 100.0, 100.0, 0.3): 0.414814814815
(5, 1.0, 1.0, 0.275): 0.392592592593
(6, 100.0, 100.0, 0.225): 0.437037037037
(7, 10.0, 10.0, 0.25): 0.274074074074
(8, 100.0, 100.0, 0.225): 0.222222222222
(9, 10.0, 10.0, 0.2): 0.4

Accuracy mean :0.6385185185185185
Std deviation :0.06283647425318074
Loss mean :0.3614814814814814
Std deviation :0.06283647425318074

On ALL the sets

Accuracy on all the set for every iteration, for every couple c,sigma (n_of_the_iter, c, sigma):
(0, 1.0, 1.0, 0.2): 0.6
(0, 1.0, 1.0, 0.225): 0.666666666667
(0, 1.0, 1.0, 0.25): 0.6
(0, 1.0, 1.0, 0.275): 0.466666666667
(0, 1.0, 1.0, 0.3): 0.4
(0, 10.0, 10.0, 0.2): 0.466666666667
(0, 10.0, 10.0, 0.225): 0.4
(0, 10.0, 10.0, 0.25): 0.266666666667
(0, 10.0, 10.0, 0.275): 0.533333333333
(0, 10.0, 10.0, 0.3): 0.466666666667
(0, 100.0, 100.0, 0.2): 0.533333333333
(0, 100.0, 100.0, 0.225): 0.333333333333
(0, 100.0, 100.0, 0.25): 0.4
(0, 100.0, 100.0, 0.275): 0.466666666667
(0, 100.0, 100.0, 0.3): 0.333333333333
(1, 0.1, 0.1, 0.3): 0.4
(1, 1.0, 1.0, 0.2): 0.8
(1, 1.0, 1.0, 0.225): 0.866666666667
(1, 1.0, 1.0, 0.25): 0.266666666667
(1, 1.0, 1.0, 0.275): 0.4
(1, 1.0, 1.0, 0.3): 0.533333333333
(1, 10.0, 10.0, 0.2): 0.8
(1, 10.0, 10.0, 0.225): 0.333333333333
(1, 10.0, 10.0, 0.25): 0.333333333333
(1, 10.0, 10.0, 0.275): 0.333333333333
(1, 10.0, 10.0, 0.3): 0.466666666667
(1, 100.0, 100.0, 0.2): 0.6
(1, 100.0, 100.0, 0.225): 0.333333333333
(1, 100.0, 100.0, 0.25): 0.333333333333
(1, 100.0, 100.0, 0.275): 0.4
(1, 100.0, 100.0, 0.3): 0.466666666667
(2, 0.1, 0.1, 0.275): 0.6
(2, 0.1, 0.1, 0.3): 0.533333333333
(2, 1.0, 1.0, 0.2): 0.466666666667
(2, 1.0, 1.0, 0.225): 0.6
(2, 1.0, 1.0, 0.25): 0.8
(2, 1.0, 1.0, 0.275): 0.533333333333
(2, 1.0, 1.0, 0.3): 0.6
(2, 10.0, 10.0, 0.2): 0.466666666667
(2, 10.0, 10.0, 0.225): 0.666666666667
(2, 10.0, 10.0, 0.25): 0.533333333333
(2, 10.0, 10.0, 0.275): 0.6
(2, 10.0, 10.0, 0.3): 0.6
(2, 100.0, 100.0, 0.2): 0.6
(2, 100.0, 100.0, 0.225): 0.533333333333
(2, 100.0, 100.0, 0.25): 0.8
(2, 100.0, 100.0, 0.275): 0.533333333333
(2, 100.0, 100.0, 0.3): 0.6
(3, 0.1, 0.1, 0.25): 0.4
(3, 1.0, 1.0, 0.2): 0.466666666667
(3, 1.0, 1.0, 0.225): 0.733333333333
(3, 1.0, 1.0, 0.25): 0.333333333333
(3, 1.0, 1.0, 0.275): 0.466666666667
(3, 1.0, 1.0, 0.3): 0.2
(3, 10.0, 10.0, 0.2): 0.466666666667
(3, 10.0, 10.0, 0.225): 0.466666666667
(3, 10.0, 10.0, 0.25): 0.733333333333
(3, 10.0, 10.0, 0.275): 0.466666666667
(3, 10.0, 10.0, 0.3): 0.266666666667
(3, 100.0, 100.0, 0.2): 0.6
(3, 100.0, 100.0, 0.225): 0.466666666667
(3, 100.0, 100.0, 0.25): 0.466666666667
(3, 100.0, 100.0, 0.275): 0.466666666667
(3, 100.0, 100.0, 0.3): 0.6
(4, 1.0, 1.0, 0.2): 0.533333333333
(4, 1.0, 1.0, 0.225): 0.4
(4, 1.0, 1.0, 0.25): 0.333333333333
(4, 1.0, 1.0, 0.275): 0.466666666667
(4, 1.0, 1.0, 0.3): 0.733333333333
(4, 10.0, 10.0, 0.2): 0.533333333333
(4, 10.0, 10.0, 0.225): 0.533333333333
(4, 10.0, 10.0, 0.25): 0.466666666667
(4, 10.0, 10.0, 0.275): 0.666666666667
(4, 10.0, 10.0, 0.3): 0.666666666667
(4, 100.0, 100.0, 0.2): 0.466666666667
(4, 100.0, 100.0, 0.225): 0.4
(4, 100.0, 100.0, 0.25): 0.533333333333
(4, 100.0, 100.0, 0.275): 0.533333333333
(4, 100.0, 100.0, 0.3): 0.733333333333
(5, 1.0, 1.0, 0.2): 0.6
(5, 1.0, 1.0, 0.225): 0.6
(5, 1.0, 1.0, 0.25): 0.0666666666667
(5, 1.0, 1.0, 0.275): 0.733333333333
(5, 1.0, 1.0, 0.3): 0.6
(5, 10.0, 10.0, 0.2): 0.533333333333
(5, 10.0, 10.0, 0.225): 0.533333333333
(5, 10.0, 10.0, 0.25): 0.133333333333
(5, 10.0, 10.0, 0.275): 0.6
(5, 10.0, 10.0, 0.3): 0.733333333333
(5, 100.0, 100.0, 0.2): 0.533333333333
(5, 100.0, 100.0, 0.225): 0.533333333333
(5, 100.0, 100.0, 0.25): 0.0666666666667
(5, 100.0, 100.0, 0.275): 0.6
(5, 100.0, 100.0, 0.3): 0.533333333333
(6, 0.1, 0.1, 0.3): 0.266666666667
(6, 1.0, 1.0, 0.2): 0.333333333333
(6, 1.0, 1.0, 0.225): 0.466666666667
(6, 1.0, 1.0, 0.25): 0.4
(6, 1.0, 1.0, 0.275): 0.466666666667
(6, 1.0, 1.0, 0.3): 0.0666666666667
(6, 10.0, 10.0, 0.2): 0.333333333333
(6, 10.0, 10.0, 0.225): 0.666666666667
(6, 10.0, 10.0, 0.25): 0.666666666667
(6, 10.0, 10.0, 0.275): 0.466666666667
(6, 10.0, 10.0, 0.3): 0.333333333333
(6, 100.0, 100.0, 0.2): 0.333333333333
(6, 100.0, 100.0, 0.225): 0.733333333333
(6, 100.0, 100.0, 0.25): 0.533333333333
(6, 100.0, 100.0, 0.275): 0.466666666667
(6, 100.0, 100.0, 0.3): 0.333333333333
(7, 1.0, 1.0, 0.2): 0.2
(7, 1.0, 1.0, 0.225): 0.4
(7, 1.0, 1.0, 0.25): 0.466666666667
(7, 1.0, 1.0, 0.275): 0.333333333333
(7, 1.0, 1.0, 0.3): 0.466666666667
(7, 10.0, 10.0, 0.2): 0.266666666667
(7, 10.0, 10.0, 0.225): 0.466666666667
(7, 10.0, 10.0, 0.25): 0.666666666667
(7, 10.0, 10.0, 0.275): 0.466666666667
(7, 10.0, 10.0, 0.3): 0.4
(7, 100.0, 100.0, 0.2): 0.333333333333
(7, 100.0, 100.0, 0.225): 0.466666666667
(7, 100.0, 100.0, 0.25): 0.266666666667
(7, 100.0, 100.0, 0.275): 0.4
(7, 100.0, 100.0, 0.3): 0.6
(8, 1.0, 1.0, 0.2): 0.4
(8, 1.0, 1.0, 0.225): 0.333333333333
(8, 1.0, 1.0, 0.25): 0.266666666667
(8, 1.0, 1.0, 0.275): 0.466666666667
(8, 1.0, 1.0, 0.3): 0.466666666667
(8, 10.0, 10.0, 0.2): 0.6
(8, 10.0, 10.0, 0.225): 0.4
(8, 10.0, 10.0, 0.25): 0.4
(8, 10.0, 10.0, 0.275): 0.333333333333
(8, 10.0, 10.0, 0.3): 0.4
(8, 100.0, 100.0, 0.2): 0.533333333333
(8, 100.0, 100.0, 0.225): 0.6
(8, 100.0, 100.0, 0.25): 0.2
(8, 100.0, 100.0, 0.275): 0.4
(8, 100.0, 100.0, 0.3): 0.333333333333
(9, 0.1, 0.1, 0.275): 0.333333333333
(9, 1.0, 1.0, 0.2): 0.466666666667
(9, 1.0, 1.0, 0.225): 0.466666666667
(9, 1.0, 1.0, 0.25): 0.333333333333
(9, 1.0, 1.0, 0.275): 0.466666666667
(9, 1.0, 1.0, 0.3): 0.333333333333
(9, 10.0, 10.0, 0.2): 0.733333333333
(9, 10.0, 10.0, 0.225): 0.6
(9, 10.0, 10.0, 0.25): 0.4
(9, 10.0, 10.0, 0.275): 0.4
(9, 10.0, 10.0, 0.3): 0.4
(9, 100.0, 100.0, 0.2): 0.6
(9, 100.0, 100.0, 0.225): 0.533333333333
(9, 100.0, 100.0, 0.25): 0.733333333333
(9, 100.0, 100.0, 0.275): 0.333333333333
(9, 100.0, 100.0, 0.3): 0.466666666667

Loss on all the set for every iteration, for every couple c,sigma (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 1.0, 0.2): 0.4
(0, 1.0, 1.0, 0.225): 0.333333333333
(0, 1.0, 1.0, 0.25): 0.4
(0, 1.0, 1.0, 0.275): 0.533333333333
(0, 1.0, 1.0, 0.3): 0.6
(0, 10.0, 10.0, 0.2): 0.533333333333
(0, 10.0, 10.0, 0.225): 0.6
(0, 10.0, 10.0, 0.25): 0.733333333333
(0, 10.0, 10.0, 0.275): 0.466666666667
(0, 10.0, 10.0, 0.3): 0.533333333333
(0, 100.0, 100.0, 0.2): 0.466666666667
(0, 100.0, 100.0, 0.225): 0.666666666667
(0, 100.0, 100.0, 0.25): 0.6
(0, 100.0, 100.0, 0.275): 0.533333333333
(0, 100.0, 100.0, 0.3): 0.666666666667
(1, 0.1, 0.1, 0.3): 0.6
(1, 1.0, 1.0, 0.2): 0.2
(1, 1.0, 1.0, 0.225): 0.133333333333
(1, 1.0, 1.0, 0.25): 0.733333333333
(1, 1.0, 1.0, 0.275): 0.6
(1, 1.0, 1.0, 0.3): 0.466666666667
(1, 10.0, 10.0, 0.2): 0.2
(1, 10.0, 10.0, 0.225): 0.666666666667
(1, 10.0, 10.0, 0.25): 0.666666666667
(1, 10.0, 10.0, 0.275): 0.666666666667
(1, 10.0, 10.0, 0.3): 0.533333333333
(1, 100.0, 100.0, 0.2): 0.4
(1, 100.0, 100.0, 0.225): 0.666666666667
(1, 100.0, 100.0, 0.25): 0.666666666667
(1, 100.0, 100.0, 0.275): 0.6
(1, 100.0, 100.0, 0.3): 0.533333333333
(2, 0.1, 0.1, 0.275): 0.4
(2, 0.1, 0.1, 0.3): 0.466666666667
(2, 1.0, 1.0, 0.2): 0.533333333333
(2, 1.0, 1.0, 0.225): 0.4
(2, 1.0, 1.0, 0.25): 0.2
(2, 1.0, 1.0, 0.275): 0.466666666667
(2, 1.0, 1.0, 0.3): 0.4
(2, 10.0, 10.0, 0.2): 0.533333333333
(2, 10.0, 10.0, 0.225): 0.333333333333
(2, 10.0, 10.0, 0.25): 0.466666666667
(2, 10.0, 10.0, 0.275): 0.4
(2, 10.0, 10.0, 0.3): 0.4
(2, 100.0, 100.0, 0.2): 0.4
(2, 100.0, 100.0, 0.225): 0.466666666667
(2, 100.0, 100.0, 0.25): 0.2
(2, 100.0, 100.0, 0.275): 0.466666666667
(2, 100.0, 100.0, 0.3): 0.4
(3, 0.1, 0.1, 0.25): 0.6
(3, 1.0, 1.0, 0.2): 0.533333333333
(3, 1.0, 1.0, 0.225): 0.266666666667
(3, 1.0, 1.0, 0.25): 0.666666666667
(3, 1.0, 1.0, 0.275): 0.533333333333
(3, 1.0, 1.0, 0.3): 0.8
(3, 10.0, 10.0, 0.2): 0.533333333333
(3, 10.0, 10.0, 0.225): 0.533333333333
(3, 10.0, 10.0, 0.25): 0.266666666667
(3, 10.0, 10.0, 0.275): 0.533333333333
(3, 10.0, 10.0, 0.3): 0.733333333333
(3, 100.0, 100.0, 0.2): 0.4
(3, 100.0, 100.0, 0.225): 0.533333333333
(3, 100.0, 100.0, 0.25): 0.533333333333
(3, 100.0, 100.0, 0.275): 0.533333333333
(3, 100.0, 100.0, 0.3): 0.4
(4, 1.0, 1.0, 0.2): 0.466666666667
(4, 1.0, 1.0, 0.225): 0.6
(4, 1.0, 1.0, 0.25): 0.666666666667
(4, 1.0, 1.0, 0.275): 0.533333333333
(4, 1.0, 1.0, 0.3): 0.266666666667
(4, 10.0, 10.0, 0.2): 0.466666666667
(4, 10.0, 10.0, 0.225): 0.466666666667
(4, 10.0, 10.0, 0.25): 0.533333333333
(4, 10.0, 10.0, 0.275): 0.333333333333
(4, 10.0, 10.0, 0.3): 0.333333333333
(4, 100.0, 100.0, 0.2): 0.533333333333
(4, 100.0, 100.0, 0.225): 0.6
(4, 100.0, 100.0, 0.25): 0.466666666667
(4, 100.0, 100.0, 0.275): 0.466666666667
(4, 100.0, 100.0, 0.3): 0.266666666667
(5, 1.0, 1.0, 0.2): 0.4
(5, 1.0, 1.0, 0.225): 0.4
(5, 1.0, 1.0, 0.25): 0.933333333333
(5, 1.0, 1.0, 0.275): 0.266666666667
(5, 1.0, 1.0, 0.3): 0.4
(5, 10.0, 10.0, 0.2): 0.466666666667
(5, 10.0, 10.0, 0.225): 0.466666666667
(5, 10.0, 10.0, 0.25): 0.866666666667
(5, 10.0, 10.0, 0.275): 0.4
(5, 10.0, 10.0, 0.3): 0.266666666667
(5, 100.0, 100.0, 0.2): 0.466666666667
(5, 100.0, 100.0, 0.225): 0.466666666667
(5, 100.0, 100.0, 0.25): 0.933333333333
(5, 100.0, 100.0, 0.275): 0.4
(5, 100.0, 100.0, 0.3): 0.466666666667
(6, 0.1, 0.1, 0.3): 0.733333333333
(6, 1.0, 1.0, 0.2): 0.666666666667
(6, 1.0, 1.0, 0.225): 0.533333333333
(6, 1.0, 1.0, 0.25): 0.6
(6, 1.0, 1.0, 0.275): 0.533333333333
(6, 1.0, 1.0, 0.3): 0.933333333333
(6, 10.0, 10.0, 0.2): 0.666666666667
(6, 10.0, 10.0, 0.225): 0.333333333333
(6, 10.0, 10.0, 0.25): 0.333333333333
(6, 10.0, 10.0, 0.275): 0.533333333333
(6, 10.0, 10.0, 0.3): 0.666666666667
(6, 100.0, 100.0, 0.2): 0.666666666667
(6, 100.0, 100.0, 0.225): 0.266666666667
(6, 100.0, 100.0, 0.25): 0.466666666667
(6, 100.0, 100.0, 0.275): 0.533333333333
(6, 100.0, 100.0, 0.3): 0.666666666667
(7, 1.0, 1.0, 0.2): 0.8
(7, 1.0, 1.0, 0.225): 0.6
(7, 1.0, 1.0, 0.25): 0.533333333333
(7, 1.0, 1.0, 0.275): 0.666666666667
(7, 1.0, 1.0, 0.3): 0.533333333333
(7, 10.0, 10.0, 0.2): 0.733333333333
(7, 10.0, 10.0, 0.225): 0.533333333333
(7, 10.0, 10.0, 0.25): 0.333333333333
(7, 10.0, 10.0, 0.275): 0.533333333333
(7, 10.0, 10.0, 0.3): 0.6
(7, 100.0, 100.0, 0.2): 0.666666666667
(7, 100.0, 100.0, 0.225): 0.533333333333
(7, 100.0, 100.0, 0.25): 0.733333333333
(7, 100.0, 100.0, 0.275): 0.6
(7, 100.0, 100.0, 0.3): 0.4
(8, 1.0, 1.0, 0.2): 0.6
(8, 1.0, 1.0, 0.225): 0.666666666667
(8, 1.0, 1.0, 0.25): 0.733333333333
(8, 1.0, 1.0, 0.275): 0.533333333333
(8, 1.0, 1.0, 0.3): 0.533333333333
(8, 10.0, 10.0, 0.2): 0.4
(8, 10.0, 10.0, 0.225): 0.6
(8, 10.0, 10.0, 0.25): 0.6
(8, 10.0, 10.0, 0.275): 0.666666666667
(8, 10.0, 10.0, 0.3): 0.6
(8, 100.0, 100.0, 0.2): 0.466666666667
(8, 100.0, 100.0, 0.225): 0.4
(8, 100.0, 100.0, 0.25): 0.8
(8, 100.0, 100.0, 0.275): 0.6
(8, 100.0, 100.0, 0.3): 0.666666666667
(9, 0.1, 0.1, 0.275): 0.666666666667
(9, 1.0, 1.0, 0.2): 0.533333333333
(9, 1.0, 1.0, 0.225): 0.533333333333
(9, 1.0, 1.0, 0.25): 0.666666666667
(9, 1.0, 1.0, 0.275): 0.533333333333
(9, 1.0, 1.0, 0.3): 0.666666666667
(9, 10.0, 10.0, 0.2): 0.266666666667
(9, 10.0, 10.0, 0.225): 0.4
(9, 10.0, 10.0, 0.25): 0.6
(9, 10.0, 10.0, 0.275): 0.6
(9, 10.0, 10.0, 0.3): 0.6
(9, 100.0, 100.0, 0.2): 0.4
(9, 100.0, 100.0, 0.225): 0.466666666667
(9, 100.0, 100.0, 0.25): 0.266666666667
(9, 100.0, 100.0, 0.275): 0.666666666667
(9, 100.0, 100.0, 0.3): 0.533333333333

Accuracy mean :0.4773504273504274
Std deviation :0.15144071548327145
Loss mean :0.5226495726495727
Std deviation :0.15144071548327145



Crisp_LinearMu_3dim, number of iterations: 10

Parameters info
Tradeoffs parameter: [  0.1   1.   10.  100. ]
Using same parameters for all the clusterization: True
Gaussian kernel sigmas: [0.2   0.225 0.25  0.275 0.3  ]
Number of principal dimension considerated: 3
Dataset length: 150

Clusterization info
Minimum size of a cluster: in perc: 0.01, in int: 2
Type of mu: Linear Muzzifier
Fuzzifier: Crisp
Cluster graph uses all connections: False
Cluster to label info
Force the number of cluster to be the number of different labels: False
Force the labels to be always represent by a cluster: False

Validation info
Conflict resolution: random
Number of cluster considerated for validation (top_k): None


RESULTS

On TEST set

Accuracy on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 100.0, 100.0, 0.225): 0.533333333333
(1, 100.0, 100.0, 0.225): 0.466666666667
(2, 100.0, 100.0, 0.3): 0.333333333333
(3, 100.0, 100.0, 0.275): 0.333333333333
(4, 100.0, 100.0, 0.3): 0.4
(5, 1.0, 1.0, 0.3): 0.533333333333
(6, 10.0, 10.0, 0.25): 0.6
(7, 100.0, 100.0, 0.275): 0.533333333333
(8, 1.0, 1.0, 0.25): 0.266666666667
(9, 1.0, 1.0, 0.3): 0.4

Loss on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 100.0, 100.0, 0.225): 0.466666666667
(1, 100.0, 100.0, 0.225): 0.533333333333
(2, 100.0, 100.0, 0.3): 0.666666666667
(3, 100.0, 100.0, 0.275): 0.666666666667
(4, 100.0, 100.0, 0.3): 0.6
(5, 1.0, 1.0, 0.3): 0.466666666667
(6, 10.0, 10.0, 0.25): 0.4
(7, 100.0, 100.0, 0.275): 0.466666666667
(8, 1.0, 1.0, 0.25): 0.733333333333
(9, 1.0, 1.0, 0.3): 0.6

Accuracy mean :0.44000000000000006
Std deviation :0.10413666234542206
Loss mean :0.56
Std deviation :0.10413666234542203

On TRAINING set

Accuracy on training set for every iteration (n_of_the_iter, c, sigma):
(0, 100.0, 100.0, 0.225): 0.711111111111
(1, 100.0, 100.0, 0.225): 0.525925925926
(2, 100.0, 100.0, 0.3): 0.481481481481
(3, 100.0, 100.0, 0.275): 0.659259259259
(4, 100.0, 100.0, 0.3): 0.585185185185
(5, 1.0, 1.0, 0.3): 0.488888888889
(6, 10.0, 10.0, 0.25): 0.488888888889
(7, 100.0, 100.0, 0.275): 0.474074074074
(8, 1.0, 1.0, 0.25): 0.740740740741
(9, 1.0, 1.0, 0.3): 0.6
Loss on training set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 100.0, 100.0, 0.225): 0.288888888889
(1, 100.0, 100.0, 0.225): 0.474074074074
(2, 100.0, 100.0, 0.3): 0.518518518519
(3, 100.0, 100.0, 0.275): 0.340740740741
(4, 100.0, 100.0, 0.3): 0.414814814815
(5, 1.0, 1.0, 0.3): 0.511111111111
(6, 10.0, 10.0, 0.25): 0.511111111111
(7, 100.0, 100.0, 0.275): 0.525925925926
(8, 1.0, 1.0, 0.25): 0.259259259259
(9, 1.0, 1.0, 0.3): 0.4

Accuracy mean :0.5755555555555556
Std deviation :0.09486399187643597
Loss mean :0.42444444444444435
Std deviation :0.09486399187643597

On ALL the sets

Accuracy on all the set for every iteration, for every couple c,sigma (n_of_the_iter, c, sigma):
(0, 1.0, 1.0, 0.2): 0.2
(0, 1.0, 1.0, 0.225): 0.2
(0, 1.0, 1.0, 0.25): 0.266666666667
(0, 1.0, 1.0, 0.275): 0.2
(0, 1.0, 1.0, 0.3): 0.133333333333
(0, 10.0, 10.0, 0.2): 0.133333333333
(0, 10.0, 10.0, 0.225): 0.333333333333
(0, 10.0, 10.0, 0.25): 0.2
(0, 10.0, 10.0, 0.275): 0.2
(0, 10.0, 10.0, 0.3): 0.333333333333
(0, 100.0, 100.0, 0.2): 0.133333333333
(0, 100.0, 100.0, 0.225): 0.466666666667
(0, 100.0, 100.0, 0.25): 0.133333333333
(0, 100.0, 100.0, 0.275): 0.2
(0, 100.0, 100.0, 0.3): 0.333333333333
(1, 0.1, 0.1, 0.225): 0.733333333333
(1, 1.0, 1.0, 0.2): 0.466666666667
(1, 1.0, 1.0, 0.225): 0.533333333333
(1, 1.0, 1.0, 0.25): 0.4
(1, 1.0, 1.0, 0.275): 0.666666666667
(1, 1.0, 1.0, 0.3): 0.6
(1, 10.0, 10.0, 0.225): 0.4
(1, 10.0, 10.0, 0.25): 0.0666666666667
(1, 10.0, 10.0, 0.275): 0.466666666667
(1, 10.0, 10.0, 0.3): 0.666666666667
(1, 100.0, 100.0, 0.225): 0.8
(1, 100.0, 100.0, 0.25): 0.4
(1, 100.0, 100.0, 0.275): 0.533333333333
(1, 100.0, 100.0, 0.3): 0.4
(2, 0.1, 0.1, 0.3): 0.266666666667
(2, 1.0, 1.0, 0.2): 0.4
(2, 1.0, 1.0, 0.225): 0.533333333333
(2, 1.0, 1.0, 0.25): 0.533333333333
(2, 1.0, 1.0, 0.275): 0.2
(2, 1.0, 1.0, 0.3): 0.266666666667
(2, 10.0, 10.0, 0.2): 0.333333333333
(2, 10.0, 10.0, 0.225): 0.4
(2, 10.0, 10.0, 0.25): 0.4
(2, 10.0, 10.0, 0.275): 0.2
(2, 10.0, 10.0, 0.3): 0.533333333333
(2, 100.0, 100.0, 0.2): 0.333333333333
(2, 100.0, 100.0, 0.225): 0.4
(2, 100.0, 100.0, 0.25): 0.333333333333
(2, 100.0, 100.0, 0.275): 0.333333333333
(2, 100.0, 100.0, 0.3): 0.6
(3, 1.0, 1.0, 0.2): 0.0666666666667
(3, 1.0, 1.0, 0.225): 0.133333333333
(3, 1.0, 1.0, 0.25): 0.2
(3, 1.0, 1.0, 0.275): 0.533333333333
(3, 1.0, 1.0, 0.3): 0.533333333333
(3, 10.0, 10.0, 0.2): 0.266666666667
(3, 10.0, 10.0, 0.225): 0.333333333333
(3, 10.0, 10.0, 0.25): 0.2
(3, 10.0, 10.0, 0.275): 0.4
(3, 10.0, 10.0, 0.3): 0.2
(3, 100.0, 100.0, 0.2): 0.333333333333
(3, 100.0, 100.0, 0.225): 0.2
(3, 100.0, 100.0, 0.25): 0.333333333333
(3, 100.0, 100.0, 0.275): 0.6
(3, 100.0, 100.0, 0.3): 0.2
(4, 0.1, 0.1, 0.275): 0.4
(4, 1.0, 1.0, 0.2): 0.466666666667
(4, 1.0, 1.0, 0.225): 0.466666666667
(4, 1.0, 1.0, 0.25): 0.6
(4, 1.0, 1.0, 0.275): 0.533333333333
(4, 1.0, 1.0, 0.3): 0.533333333333
(4, 10.0, 10.0, 0.2): 0.466666666667
(4, 10.0, 10.0, 0.225): 0.266666666667
(4, 10.0, 10.0, 0.25): 0.666666666667
(4, 10.0, 10.0, 0.275): 0.6
(4, 10.0, 10.0, 0.3): 0.6
(4, 100.0, 100.0, 0.2): 0.533333333333
(4, 100.0, 100.0, 0.225): 0.266666666667
(4, 100.0, 100.0, 0.25): 0.666666666667
(4, 100.0, 100.0, 0.275): 0.6
(4, 100.0, 100.0, 0.3): 0.8
(5, 0.1, 0.1, 0.3): 0.4
(5, 1.0, 1.0, 0.225): 0.133333333333
(5, 1.0, 1.0, 0.25): 0.4
(5, 1.0, 1.0, 0.275): 0.4
(5, 1.0, 1.0, 0.3): 0.6
(5, 10.0, 10.0, 0.2): 0.333333333333
(5, 10.0, 10.0, 0.225): 0.333333333333
(5, 10.0, 10.0, 0.25): 0.333333333333
(5, 10.0, 10.0, 0.275): 0.266666666667
(5, 10.0, 10.0, 0.3): 0.4
(5, 100.0, 100.0, 0.225): 0.2
(5, 100.0, 100.0, 0.25): 0.333333333333
(5, 100.0, 100.0, 0.275): 0.333333333333
(5, 100.0, 100.0, 0.3): 0.466666666667
(6, 1.0, 1.0, 0.225): 0.533333333333
(6, 1.0, 1.0, 0.25): 0.466666666667
(6, 1.0, 1.0, 0.275): 0.466666666667
(6, 1.0, 1.0, 0.3): 0.466666666667
(6, 10.0, 10.0, 0.225): 0.266666666667
(6, 10.0, 10.0, 0.25): 0.666666666667
(6, 10.0, 10.0, 0.275): 0.466666666667
(6, 10.0, 10.0, 0.3): 0.666666666667
(6, 100.0, 100.0, 0.225): 0.533333333333
(6, 100.0, 100.0, 0.25): 0.4
(6, 100.0, 100.0, 0.275): 0.466666666667
(6, 100.0, 100.0, 0.3): 0.533333333333
(7, 1.0, 1.0, 0.2): 0.4
(7, 1.0, 1.0, 0.225): 0.4
(7, 1.0, 1.0, 0.25): 0.2
(7, 1.0, 1.0, 0.275): 0.333333333333
(7, 1.0, 1.0, 0.3): 0.333333333333
(7, 10.0, 10.0, 0.2): 0.2
(7, 10.0, 10.0, 0.225): 0.533333333333
(7, 10.0, 10.0, 0.25): 0.2
(7, 10.0, 10.0, 0.275): 0.4
(7, 10.0, 10.0, 0.3): 0.466666666667
(7, 100.0, 100.0, 0.225): 0.4
(7, 100.0, 100.0, 0.25): 0.2
(7, 100.0, 100.0, 0.275): 0.733333333333
(7, 100.0, 100.0, 0.3): 0.6
(8, 1.0, 1.0, 0.2): 0.333333333333
(8, 1.0, 1.0, 0.225): 0.466666666667
(8, 1.0, 1.0, 0.25): 0.666666666667
(8, 1.0, 1.0, 0.275): 0.466666666667
(8, 1.0, 1.0, 0.3): 0.266666666667
(8, 10.0, 10.0, 0.2): 0.333333333333
(8, 10.0, 10.0, 0.225): 0.4
(8, 10.0, 10.0, 0.25): 0.466666666667
(8, 10.0, 10.0, 0.275): 0.6
(8, 10.0, 10.0, 0.3): 0.6
(8, 100.0, 100.0, 0.2): 0.333333333333
(8, 100.0, 100.0, 0.225): 0.466666666667
(8, 100.0, 100.0, 0.25): 0.333333333333
(8, 100.0, 100.0, 0.275): 0.533333333333
(8, 100.0, 100.0, 0.3): 0.333333333333
(9, 1.0, 1.0, 0.2): 0.266666666667
(9, 1.0, 1.0, 0.225): 0.333333333333
(9, 1.0, 1.0, 0.25): 0.4
(9, 1.0, 1.0, 0.275): 0.533333333333
(9, 1.0, 1.0, 0.3): 0.666666666667
(9, 10.0, 10.0, 0.2): 0.4
(9, 10.0, 10.0, 0.225): 0.533333333333
(9, 10.0, 10.0, 0.25): 0.466666666667
(9, 10.0, 10.0, 0.275): 0.466666666667
(9, 10.0, 10.0, 0.3): 0.666666666667
(9, 100.0, 100.0, 0.2): 0.333333333333
(9, 100.0, 100.0, 0.225): 0.466666666667
(9, 100.0, 100.0, 0.25): 0.533333333333
(9, 100.0, 100.0, 0.275): 0.666666666667
(9, 100.0, 100.0, 0.3): 0.466666666667

Loss on all the set for every iteration, for every couple c,sigma (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 1.0, 0.2): 0.8
(0, 1.0, 1.0, 0.225): 0.8
(0, 1.0, 1.0, 0.25): 0.733333333333
(0, 1.0, 1.0, 0.275): 0.8
(0, 1.0, 1.0, 0.3): 0.866666666667
(0, 10.0, 10.0, 0.2): 0.866666666667
(0, 10.0, 10.0, 0.225): 0.666666666667
(0, 10.0, 10.0, 0.25): 0.8
(0, 10.0, 10.0, 0.275): 0.8
(0, 10.0, 10.0, 0.3): 0.666666666667
(0, 100.0, 100.0, 0.2): 0.866666666667
(0, 100.0, 100.0, 0.225): 0.533333333333
(0, 100.0, 100.0, 0.25): 0.866666666667
(0, 100.0, 100.0, 0.275): 0.8
(0, 100.0, 100.0, 0.3): 0.666666666667
(1, 0.1, 0.1, 0.225): 0.266666666667
(1, 1.0, 1.0, 0.2): 0.533333333333
(1, 1.0, 1.0, 0.225): 0.466666666667
(1, 1.0, 1.0, 0.25): 0.6
(1, 1.0, 1.0, 0.275): 0.333333333333
(1, 1.0, 1.0, 0.3): 0.4
(1, 10.0, 10.0, 0.225): 0.6
(1, 10.0, 10.0, 0.25): 0.933333333333
(1, 10.0, 10.0, 0.275): 0.533333333333
(1, 10.0, 10.0, 0.3): 0.333333333333
(1, 100.0, 100.0, 0.225): 0.2
(1, 100.0, 100.0, 0.25): 0.6
(1, 100.0, 100.0, 0.275): 0.466666666667
(1, 100.0, 100.0, 0.3): 0.6
(2, 0.1, 0.1, 0.3): 0.733333333333
(2, 1.0, 1.0, 0.2): 0.6
(2, 1.0, 1.0, 0.225): 0.466666666667
(2, 1.0, 1.0, 0.25): 0.466666666667
(2, 1.0, 1.0, 0.275): 0.8
(2, 1.0, 1.0, 0.3): 0.733333333333
(2, 10.0, 10.0, 0.2): 0.666666666667
(2, 10.0, 10.0, 0.225): 0.6
(2, 10.0, 10.0, 0.25): 0.6
(2, 10.0, 10.0, 0.275): 0.8
(2, 10.0, 10.0, 0.3): 0.466666666667
(2, 100.0, 100.0, 0.2): 0.666666666667
(2, 100.0, 100.0, 0.225): 0.6
(2, 100.0, 100.0, 0.25): 0.666666666667
(2, 100.0, 100.0, 0.275): 0.666666666667
(2, 100.0, 100.0, 0.3): 0.4
(3, 1.0, 1.0, 0.2): 0.933333333333
(3, 1.0, 1.0, 0.225): 0.866666666667
(3, 1.0, 1.0, 0.25): 0.8
(3, 1.0, 1.0, 0.275): 0.466666666667
(3, 1.0, 1.0, 0.3): 0.466666666667
(3, 10.0, 10.0, 0.2): 0.733333333333
(3, 10.0, 10.0, 0.225): 0.666666666667
(3, 10.0, 10.0, 0.25): 0.8
(3, 10.0, 10.0, 0.275): 0.6
(3, 10.0, 10.0, 0.3): 0.8
(3, 100.0, 100.0, 0.2): 0.666666666667
(3, 100.0, 100.0, 0.225): 0.8
(3, 100.0, 100.0, 0.25): 0.666666666667
(3, 100.0, 100.0, 0.275): 0.4
(3, 100.0, 100.0, 0.3): 0.8
(4, 0.1, 0.1, 0.275): 0.6
(4, 1.0, 1.0, 0.2): 0.533333333333
(4, 1.0, 1.0, 0.225): 0.533333333333
(4, 1.0, 1.0, 0.25): 0.4
(4, 1.0, 1.0, 0.275): 0.466666666667
(4, 1.0, 1.0, 0.3): 0.466666666667
(4, 10.0, 10.0, 0.2): 0.533333333333
(4, 10.0, 10.0, 0.225): 0.733333333333
(4, 10.0, 10.0, 0.25): 0.333333333333
(4, 10.0, 10.0, 0.275): 0.4
(4, 10.0, 10.0, 0.3): 0.4
(4, 100.0, 100.0, 0.2): 0.466666666667
(4, 100.0, 100.0, 0.225): 0.733333333333
(4, 100.0, 100.0, 0.25): 0.333333333333
(4, 100.0, 100.0, 0.275): 0.4
(4, 100.0, 100.0, 0.3): 0.2
(5, 0.1, 0.1, 0.3): 0.6
(5, 1.0, 1.0, 0.225): 0.866666666667
(5, 1.0, 1.0, 0.25): 0.6
(5, 1.0, 1.0, 0.275): 0.6
(5, 1.0, 1.0, 0.3): 0.4
(5, 10.0, 10.0, 0.2): 0.666666666667
(5, 10.0, 10.0, 0.225): 0.666666666667
(5, 10.0, 10.0, 0.25): 0.666666666667
(5, 10.0, 10.0, 0.275): 0.733333333333
(5, 10.0, 10.0, 0.3): 0.6
(5, 100.0, 100.0, 0.225): 0.8
(5, 100.0, 100.0, 0.25): 0.666666666667
(5, 100.0, 100.0, 0.275): 0.666666666667
(5, 100.0, 100.0, 0.3): 0.533333333333
(6, 1.0, 1.0, 0.225): 0.466666666667
(6, 1.0, 1.0, 0.25): 0.533333333333
(6, 1.0, 1.0, 0.275): 0.533333333333
(6, 1.0, 1.0, 0.3): 0.533333333333
(6, 10.0, 10.0, 0.225): 0.733333333333
(6, 10.0, 10.0, 0.25): 0.333333333333
(6, 10.0, 10.0, 0.275): 0.533333333333
(6, 10.0, 10.0, 0.3): 0.333333333333
(6, 100.0, 100.0, 0.225): 0.466666666667
(6, 100.0, 100.0, 0.25): 0.6
(6, 100.0, 100.0, 0.275): 0.533333333333
(6, 100.0, 100.0, 0.3): 0.466666666667
(7, 1.0, 1.0, 0.2): 0.6
(7, 1.0, 1.0, 0.225): 0.6
(7, 1.0, 1.0, 0.25): 0.8
(7, 1.0, 1.0, 0.275): 0.666666666667
(7, 1.0, 1.0, 0.3): 0.666666666667
(7, 10.0, 10.0, 0.2): 0.8
(7, 10.0, 10.0, 0.225): 0.466666666667
(7, 10.0, 10.0, 0.25): 0.8
(7, 10.0, 10.0, 0.275): 0.6
(7, 10.0, 10.0, 0.3): 0.533333333333
(7, 100.0, 100.0, 0.225): 0.6
(7, 100.0, 100.0, 0.25): 0.8
(7, 100.0, 100.0, 0.275): 0.266666666667
(7, 100.0, 100.0, 0.3): 0.4
(8, 1.0, 1.0, 0.2): 0.666666666667
(8, 1.0, 1.0, 0.225): 0.533333333333
(8, 1.0, 1.0, 0.25): 0.333333333333
(8, 1.0, 1.0, 0.275): 0.533333333333
(8, 1.0, 1.0, 0.3): 0.733333333333
(8, 10.0, 10.0, 0.2): 0.666666666667
(8, 10.0, 10.0, 0.225): 0.6
(8, 10.0, 10.0, 0.25): 0.533333333333
(8, 10.0, 10.0, 0.275): 0.4
(8, 10.0, 10.0, 0.3): 0.4
(8, 100.0, 100.0, 0.2): 0.666666666667
(8, 100.0, 100.0, 0.225): 0.533333333333
(8, 100.0, 100.0, 0.25): 0.666666666667
(8, 100.0, 100.0, 0.275): 0.466666666667
(8, 100.0, 100.0, 0.3): 0.666666666667
(9, 1.0, 1.0, 0.2): 0.733333333333
(9, 1.0, 1.0, 0.225): 0.666666666667
(9, 1.0, 1.0, 0.25): 0.6
(9, 1.0, 1.0, 0.275): 0.466666666667
(9, 1.0, 1.0, 0.3): 0.333333333333
(9, 10.0, 10.0, 0.2): 0.6
(9, 10.0, 10.0, 0.225): 0.466666666667
(9, 10.0, 10.0, 0.25): 0.533333333333
(9, 10.0, 10.0, 0.275): 0.533333333333
(9, 10.0, 10.0, 0.3): 0.333333333333
(9, 100.0, 100.0, 0.2): 0.666666666667
(9, 100.0, 100.0, 0.225): 0.533333333333
(9, 100.0, 100.0, 0.25): 0.466666666667
(9, 100.0, 100.0, 0.275): 0.333333333333
(9, 100.0, 100.0, 0.3): 0.533333333333

Accuracy mean :0.4091324200913242
Std deviation :0.16021867419501953
Loss mean :0.5908675799086757
Std deviation :0.16021867419501956



Crisp_LinearMu_4dim, number of iterations: 10

Parameters info
Tradeoffs parameter: [  0.1   1.   10.  100. ]
Using same parameters for all the clusterization: True
Gaussian kernel sigmas: [0.2   0.225 0.25  0.275 0.3  ]
Number of principal dimension considerated: 4
Dataset length: 150

Clusterization info
Minimum size of a cluster: in perc: 0.01, in int: 2
Type of mu: Linear Muzzifier
Fuzzifier: Crisp
Cluster graph uses all connections: False
Cluster to label info
Force the number of cluster to be the number of different labels: False
Force the labels to be always represent by a cluster: False

Validation info
Conflict resolution: random
Number of cluster considerated for validation (top_k): None


RESULTS

On TEST set

Accuracy on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 0.1, 0.1, 0.25): 0.466666666667
(1, 0.1, 0.1, 0.275): 0.4
(2, 100.0, 100.0, 0.225): 0.6
(3, 100.0, 100.0, 0.225): 0.266666666667
(4, 0.1, 0.1, 0.275): 0.466666666667
(5, 1.0, 1.0, 0.25): 0.4
(6, 100.0, 100.0, 0.225): 0.466666666667
(7, 0.1, 0.1, 0.3): 0.266666666667
(8, 100.0, 100.0, 0.275): 0.333333333333
(9, 100.0, 100.0, 0.275): 0.4

Loss on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 0.1, 0.1, 0.25): 0.533333333333
(1, 0.1, 0.1, 0.275): 0.6
(2, 100.0, 100.0, 0.225): 0.4
(3, 100.0, 100.0, 0.225): 0.733333333333
(4, 0.1, 0.1, 0.275): 0.533333333333
(5, 1.0, 1.0, 0.25): 0.6
(6, 100.0, 100.0, 0.225): 0.533333333333
(7, 0.1, 0.1, 0.3): 0.733333333333
(8, 100.0, 100.0, 0.275): 0.666666666667
(9, 100.0, 100.0, 0.275): 0.6

Accuracy mean :0.4066666666666666
Std deviation :0.09637888196533974
Loss mean :0.5933333333333334
Std deviation :0.09637888196533971

On TRAINING set

Accuracy on training set for every iteration (n_of_the_iter, c, sigma):
(0, 0.1, 0.1, 0.25): 0.474074074074
(1, 0.1, 0.1, 0.275): 0.577777777778
(2, 100.0, 100.0, 0.225): 0.651851851852
(3, 100.0, 100.0, 0.225): 0.562962962963
(4, 0.1, 0.1, 0.275): 0.451851851852
(5, 1.0, 1.0, 0.25): 0.525925925926
(6, 100.0, 100.0, 0.225): 0.651851851852
(7, 0.1, 0.1, 0.3): 0.340740740741
(8, 100.0, 100.0, 0.275): 0.488888888889
(9, 100.0, 100.0, 0.275): 0.666666666667
Loss on training set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 0.1, 0.1, 0.25): 0.525925925926
(1, 0.1, 0.1, 0.275): 0.422222222222
(2, 100.0, 100.0, 0.225): 0.348148148148
(3, 100.0, 100.0, 0.225): 0.437037037037
(4, 0.1, 0.1, 0.275): 0.548148148148
(5, 1.0, 1.0, 0.25): 0.474074074074
(6, 100.0, 100.0, 0.225): 0.348148148148
(7, 0.1, 0.1, 0.3): 0.659259259259
(8, 100.0, 100.0, 0.275): 0.511111111111
(9, 100.0, 100.0, 0.275): 0.333333333333

Accuracy mean :0.5392592592592592
Std deviation :0.09881603641868027
Loss mean :0.4607407407407408
Std deviation :0.09881603641868024

On ALL the sets

Accuracy on all the set for every iteration, for every couple c,sigma (n_of_the_iter, c, sigma):
(0, 0.1, 0.1, 0.25): 0.533333333333
(0, 0.1, 0.1, 0.3): 0.4
(0, 1.0, 1.0, 0.2): 0.266666666667
(0, 1.0, 1.0, 0.225): 0.466666666667
(0, 1.0, 1.0, 0.25): 0.4
(0, 1.0, 1.0, 0.275): 0.4
(0, 1.0, 1.0, 0.3): 0.466666666667
(0, 10.0, 10.0, 0.2): 0.266666666667
(0, 10.0, 10.0, 0.225): 0.4
(0, 10.0, 10.0, 0.25): 0.466666666667
(0, 10.0, 10.0, 0.275): 0.533333333333
(0, 10.0, 10.0, 0.3): 0.466666666667
(0, 100.0, 100.0, 0.2): 0.266666666667
(0, 100.0, 100.0, 0.225): 0.533333333333
(0, 100.0, 100.0, 0.25): 0.466666666667
(0, 100.0, 100.0, 0.275): 0.533333333333
(0, 100.0, 100.0, 0.3): 0.533333333333
(1, 0.1, 0.1, 0.2): 0.333333333333
(1, 0.1, 0.1, 0.25): 0.333333333333
(1, 0.1, 0.1, 0.275): 0.533333333333
(1, 0.1, 0.1, 0.3): 0.533333333333
(1, 1.0, 1.0, 0.2): 0.333333333333
(1, 1.0, 1.0, 0.225): 0.266666666667
(1, 1.0, 1.0, 0.25): 0.133333333333
(1, 1.0, 1.0, 0.275): 0.333333333333
(1, 1.0, 1.0, 0.3): 0.266666666667
(1, 10.0, 10.0, 0.2): 0.333333333333
(1, 10.0, 10.0, 0.225): 0.4
(1, 10.0, 10.0, 0.25): 0.466666666667
(1, 10.0, 10.0, 0.275): 0.4
(1, 10.0, 10.0, 0.3): 0.4
(1, 100.0, 100.0, 0.2): 0.333333333333
(1, 100.0, 100.0, 0.225): 0.333333333333
(1, 100.0, 100.0, 0.25): 0.333333333333
(1, 100.0, 100.0, 0.275): 0.466666666667
(1, 100.0, 100.0, 0.3): 0.4
(2, 0.1, 0.1, 0.3): 0.4
(2, 1.0, 1.0, 0.2): 0.2
(2, 1.0, 1.0, 0.225): 0.2
(2, 1.0, 1.0, 0.25): 0.4
(2, 1.0, 1.0, 0.275): 0.4
(2, 1.0, 1.0, 0.3): 0.333333333333
(2, 10.0, 10.0, 0.2): 0.266666666667
(2, 10.0, 10.0, 0.225): 0.533333333333
(2, 10.0, 10.0, 0.25): 0.466666666667
(2, 10.0, 10.0, 0.275): 0.266666666667
(2, 10.0, 10.0, 0.3): 0.4
(2, 100.0, 100.0, 0.2): 0.4
(2, 100.0, 100.0, 0.225): 0.6
(2, 100.0, 100.0, 0.25): 0.466666666667
(2, 100.0, 100.0, 0.275): 0.533333333333
(2, 100.0, 100.0, 0.3): 0.533333333333
(3, 0.1, 0.1, 0.2): 0.4
(3, 0.1, 0.1, 0.25): 0.266666666667
(3, 0.1, 0.1, 0.275): 0.333333333333
(3, 0.1, 0.1, 0.3): 0.333333333333
(3, 1.0, 1.0, 0.2): 0.266666666667
(3, 1.0, 1.0, 0.225): 0.266666666667
(3, 1.0, 1.0, 0.25): 0.333333333333
(3, 1.0, 1.0, 0.275): 0.4
(3, 1.0, 1.0, 0.3): 0.2
(3, 10.0, 10.0, 0.2): 0.2
(3, 10.0, 10.0, 0.225): 0.333333333333
(3, 10.0, 10.0, 0.25): 0.333333333333
(3, 10.0, 10.0, 0.275): 0.133333333333
(3, 10.0, 10.0, 0.3): 0.266666666667
(3, 100.0, 100.0, 0.2): 0.2
(3, 100.0, 100.0, 0.225): 0.4
(3, 100.0, 100.0, 0.25): 0.133333333333
(3, 100.0, 100.0, 0.275): 0.333333333333
(3, 100.0, 100.0, 0.3): 0.333333333333
(4, 0.1, 0.1, 0.25): 0.466666666667
(4, 0.1, 0.1, 0.275): 0.6
(4, 0.1, 0.1, 0.3): 0.4
(4, 1.0, 1.0, 0.2): 0.2
(4, 1.0, 1.0, 0.225): 0.2
(4, 1.0, 1.0, 0.25): 0.466666666667
(4, 1.0, 1.0, 0.275): 0.6
(4, 1.0, 1.0, 0.3): 0.533333333333
(4, 10.0, 10.0, 0.2): 0.2
(4, 10.0, 10.0, 0.225): 0.4
(4, 10.0, 10.0, 0.25): 0.266666666667
(4, 10.0, 10.0, 0.275): 0.333333333333
(4, 10.0, 10.0, 0.3): 0.6
(4, 100.0, 100.0, 0.225): 0.333333333333
(4, 100.0, 100.0, 0.25): 0.466666666667
(4, 100.0, 100.0, 0.275): 0.266666666667
(4, 100.0, 100.0, 0.3): 0.466666666667
(5, 0.1, 0.1, 0.25): 0.2
(5, 0.1, 0.1, 0.3): 0.2
(5, 1.0, 1.0, 0.2): 0.333333333333
(5, 1.0, 1.0, 0.225): 0.266666666667
(5, 1.0, 1.0, 0.25): 0.4
(5, 1.0, 1.0, 0.275): 0.2
(5, 1.0, 1.0, 0.3): 0.2
(5, 10.0, 10.0, 0.2): 0.266666666667
(5, 10.0, 10.0, 0.225): 0.2
(5, 10.0, 10.0, 0.25): 0.4
(5, 10.0, 10.0, 0.275): 0.2
(5, 10.0, 10.0, 0.3): 0.2
(5, 100.0, 100.0, 0.2): 0.333333333333
(5, 100.0, 100.0, 0.225): 0.2
(5, 100.0, 100.0, 0.25): 0.2
(5, 100.0, 100.0, 0.275): 0.333333333333
(5, 100.0, 100.0, 0.3): 0.2
(6, 0.1, 0.1, 0.25): 0.266666666667
(6, 0.1, 0.1, 0.3): 0.333333333333
(6, 1.0, 1.0, 0.2): 0.266666666667
(6, 1.0, 1.0, 0.225): 0.4
(6, 1.0, 1.0, 0.25): 0.466666666667
(6, 1.0, 1.0, 0.275): 0.533333333333
(6, 1.0, 1.0, 0.3): 0.4
(6, 10.0, 10.0, 0.2): 0.333333333333
(6, 10.0, 10.0, 0.225): 0.4
(6, 10.0, 10.0, 0.25): 0.4
(6, 10.0, 10.0, 0.275): 0.466666666667
(6, 10.0, 10.0, 0.3): 0.466666666667
(6, 100.0, 100.0, 0.2): 0.466666666667
(6, 100.0, 100.0, 0.225): 0.533333333333
(6, 100.0, 100.0, 0.25): 0.333333333333
(6, 100.0, 100.0, 0.275): 0.4
(6, 100.0, 100.0, 0.3): 0.466666666667
(7, 0.1, 0.1, 0.25): 0.266666666667
(7, 0.1, 0.1, 0.275): 0.333333333333
(7, 0.1, 0.1, 0.3): 0.4
(7, 1.0, 1.0, 0.2): 0.133333333333
(7, 1.0, 1.0, 0.25): 0.4
(7, 1.0, 1.0, 0.275): 0.4
(7, 1.0, 1.0, 0.3): 0.4
(7, 10.0, 10.0, 0.2): 0.133333333333
(7, 10.0, 10.0, 0.25): 0.333333333333
(7, 10.0, 10.0, 0.275): 0.4
(7, 10.0, 10.0, 0.3): 0.333333333333
(7, 100.0, 100.0, 0.2): 0.133333333333
(7, 100.0, 100.0, 0.25): 0.266666666667
(7, 100.0, 100.0, 0.275): 0.4
(7, 100.0, 100.0, 0.3): 0.4
(8, 0.1, 0.1, 0.225): 0.266666666667
(8, 0.1, 0.1, 0.25): 0.333333333333
(8, 0.1, 0.1, 0.3): 0.2
(8, 1.0, 1.0, 0.2): 0.333333333333
(8, 1.0, 1.0, 0.225): 0.0666666666667
(8, 1.0, 1.0, 0.25): 0.266666666667
(8, 1.0, 1.0, 0.275): 0.533333333333
(8, 1.0, 1.0, 0.3): 0.133333333333
(8, 10.0, 10.0, 0.2): 0.266666666667
(8, 10.0, 10.0, 0.225): 0.4
(8, 10.0, 10.0, 0.25): 0.266666666667
(8, 10.0, 10.0, 0.275): 0.2
(8, 10.0, 10.0, 0.3): 0.333333333333
(8, 100.0, 100.0, 0.2): 0.333333333333
(8, 100.0, 100.0, 0.225): 0.266666666667
(8, 100.0, 100.0, 0.25): 0.333333333333
(8, 100.0, 100.0, 0.275): 0.6
(8, 100.0, 100.0, 0.3): 0.4
(9, 0.1, 0.1, 0.25): 0.533333333333
(9, 0.1, 0.1, 0.275): 0.333333333333
(9, 1.0, 1.0, 0.2): 0.466666666667
(9, 1.0, 1.0, 0.225): 0.2
(9, 1.0, 1.0, 0.25): 0.4
(9, 1.0, 1.0, 0.275): 0.4
(9, 1.0, 1.0, 0.3): 0.333333333333
(9, 10.0, 10.0, 0.2): 0.4
(9, 10.0, 10.0, 0.225): 0.333333333333
(9, 10.0, 10.0, 0.25): 0.466666666667
(9, 10.0, 10.0, 0.275): 0.333333333333
(9, 10.0, 10.0, 0.3): 0.4
(9, 100.0, 100.0, 0.2): 0.4
(9, 100.0, 100.0, 0.225): 0.333333333333
(9, 100.0, 100.0, 0.25): 0.4
(9, 100.0, 100.0, 0.275): 0.533333333333
(9, 100.0, 100.0, 0.3): 0.4

Loss on all the set for every iteration, for every couple c,sigma (n_of_the_iter, best_c, best_sigma):
(0, 0.1, 0.1, 0.25): 0.466666666667
(0, 0.1, 0.1, 0.3): 0.6
(0, 1.0, 1.0, 0.2): 0.733333333333
(0, 1.0, 1.0, 0.225): 0.533333333333
(0, 1.0, 1.0, 0.25): 0.6
(0, 1.0, 1.0, 0.275): 0.6
(0, 1.0, 1.0, 0.3): 0.533333333333
(0, 10.0, 10.0, 0.2): 0.733333333333
(0, 10.0, 10.0, 0.225): 0.6
(0, 10.0, 10.0, 0.25): 0.533333333333
(0, 10.0, 10.0, 0.275): 0.466666666667
(0, 10.0, 10.0, 0.3): 0.533333333333
(0, 100.0, 100.0, 0.2): 0.733333333333
(0, 100.0, 100.0, 0.225): 0.466666666667
(0, 100.0, 100.0, 0.25): 0.533333333333
(0, 100.0, 100.0, 0.275): 0.466666666667
(0, 100.0, 100.0, 0.3): 0.466666666667
(1, 0.1, 0.1, 0.2): 0.666666666667
(1, 0.1, 0.1, 0.25): 0.666666666667
(1, 0.1, 0.1, 0.275): 0.466666666667
(1, 0.1, 0.1, 0.3): 0.466666666667
(1, 1.0, 1.0, 0.2): 0.666666666667
(1, 1.0, 1.0, 0.225): 0.733333333333
(1, 1.0, 1.0, 0.25): 0.866666666667
(1, 1.0, 1.0, 0.275): 0.666666666667
(1, 1.0, 1.0, 0.3): 0.733333333333
(1, 10.0, 10.0, 0.2): 0.666666666667
(1, 10.0, 10.0, 0.225): 0.6
(1, 10.0, 10.0, 0.25): 0.533333333333
(1, 10.0, 10.0, 0.275): 0.6
(1, 10.0, 10.0, 0.3): 0.6
(1, 100.0, 100.0, 0.2): 0.666666666667
(1, 100.0, 100.0, 0.225): 0.666666666667
(1, 100.0, 100.0, 0.25): 0.666666666667
(1, 100.0, 100.0, 0.275): 0.533333333333
(1, 100.0, 100.0, 0.3): 0.6
(2, 0.1, 0.1, 0.3): 0.6
(2, 1.0, 1.0, 0.2): 0.8
(2, 1.0, 1.0, 0.225): 0.8
(2, 1.0, 1.0, 0.25): 0.6
(2, 1.0, 1.0, 0.275): 0.6
(2, 1.0, 1.0, 0.3): 0.666666666667
(2, 10.0, 10.0, 0.2): 0.733333333333
(2, 10.0, 10.0, 0.225): 0.466666666667
(2, 10.0, 10.0, 0.25): 0.533333333333
(2, 10.0, 10.0, 0.275): 0.733333333333
(2, 10.0, 10.0, 0.3): 0.6
(2, 100.0, 100.0, 0.2): 0.6
(2, 100.0, 100.0, 0.225): 0.4
(2, 100.0, 100.0, 0.25): 0.533333333333
(2, 100.0, 100.0, 0.275): 0.466666666667
(2, 100.0, 100.0, 0.3): 0.466666666667
(3, 0.1, 0.1, 0.2): 0.6
(3, 0.1, 0.1, 0.25): 0.733333333333
(3, 0.1, 0.1, 0.275): 0.666666666667
(3, 0.1, 0.1, 0.3): 0.666666666667
(3, 1.0, 1.0, 0.2): 0.733333333333
(3, 1.0, 1.0, 0.225): 0.733333333333
(3, 1.0, 1.0, 0.25): 0.666666666667
(3, 1.0, 1.0, 0.275): 0.6
(3, 1.0, 1.0, 0.3): 0.8
(3, 10.0, 10.0, 0.2): 0.8
(3, 10.0, 10.0, 0.225): 0.666666666667
(3, 10.0, 10.0, 0.25): 0.666666666667
(3, 10.0, 10.0, 0.275): 0.866666666667
(3, 10.0, 10.0, 0.3): 0.733333333333
(3, 100.0, 100.0, 0.2): 0.8
(3, 100.0, 100.0, 0.225): 0.6
(3, 100.0, 100.0, 0.25): 0.866666666667
(3, 100.0, 100.0, 0.275): 0.666666666667
(3, 100.0, 100.0, 0.3): 0.666666666667
(4, 0.1, 0.1, 0.25): 0.533333333333
(4, 0.1, 0.1, 0.275): 0.4
(4, 0.1, 0.1, 0.3): 0.6
(4, 1.0, 1.0, 0.2): 0.8
(4, 1.0, 1.0, 0.225): 0.8
(4, 1.0, 1.0, 0.25): 0.533333333333
(4, 1.0, 1.0, 0.275): 0.4
(4, 1.0, 1.0, 0.3): 0.466666666667
(4, 10.0, 10.0, 0.2): 0.8
(4, 10.0, 10.0, 0.225): 0.6
(4, 10.0, 10.0, 0.25): 0.733333333333
(4, 10.0, 10.0, 0.275): 0.666666666667
(4, 10.0, 10.0, 0.3): 0.4
(4, 100.0, 100.0, 0.225): 0.666666666667
(4, 100.0, 100.0, 0.25): 0.533333333333
(4, 100.0, 100.0, 0.275): 0.733333333333
(4, 100.0, 100.0, 0.3): 0.533333333333
(5, 0.1, 0.1, 0.25): 0.8
(5, 0.1, 0.1, 0.3): 0.8
(5, 1.0, 1.0, 0.2): 0.666666666667
(5, 1.0, 1.0, 0.225): 0.733333333333
(5, 1.0, 1.0, 0.25): 0.6
(5, 1.0, 1.0, 0.275): 0.8
(5, 1.0, 1.0, 0.3): 0.8
(5, 10.0, 10.0, 0.2): 0.733333333333
(5, 10.0, 10.0, 0.225): 0.8
(5, 10.0, 10.0, 0.25): 0.6
(5, 10.0, 10.0, 0.275): 0.8
(5, 10.0, 10.0, 0.3): 0.8
(5, 100.0, 100.0, 0.2): 0.666666666667
(5, 100.0, 100.0, 0.225): 0.8
(5, 100.0, 100.0, 0.25): 0.8
(5, 100.0, 100.0, 0.275): 0.666666666667
(5, 100.0, 100.0, 0.3): 0.8
(6, 0.1, 0.1, 0.25): 0.733333333333
(6, 0.1, 0.1, 0.3): 0.666666666667
(6, 1.0, 1.0, 0.2): 0.733333333333
(6, 1.0, 1.0, 0.225): 0.6
(6, 1.0, 1.0, 0.25): 0.533333333333
(6, 1.0, 1.0, 0.275): 0.466666666667
(6, 1.0, 1.0, 0.3): 0.6
(6, 10.0, 10.0, 0.2): 0.666666666667
(6, 10.0, 10.0, 0.225): 0.6
(6, 10.0, 10.0, 0.25): 0.6
(6, 10.0, 10.0, 0.275): 0.533333333333
(6, 10.0, 10.0, 0.3): 0.533333333333
(6, 100.0, 100.0, 0.2): 0.533333333333
(6, 100.0, 100.0, 0.225): 0.466666666667
(6, 100.0, 100.0, 0.25): 0.666666666667
(6, 100.0, 100.0, 0.275): 0.6
(6, 100.0, 100.0, 0.3): 0.533333333333
(7, 0.1, 0.1, 0.25): 0.733333333333
(7, 0.1, 0.1, 0.275): 0.666666666667
(7, 0.1, 0.1, 0.3): 0.6
(7, 1.0, 1.0, 0.2): 0.866666666667
(7, 1.0, 1.0, 0.25): 0.6
(7, 1.0, 1.0, 0.275): 0.6
(7, 1.0, 1.0, 0.3): 0.6
(7, 10.0, 10.0, 0.2): 0.866666666667
(7, 10.0, 10.0, 0.25): 0.666666666667
(7, 10.0, 10.0, 0.275): 0.6
(7, 10.0, 10.0, 0.3): 0.666666666667
(7, 100.0, 100.0, 0.2): 0.866666666667
(7, 100.0, 100.0, 0.25): 0.733333333333
(7, 100.0, 100.0, 0.275): 0.6
(7, 100.0, 100.0, 0.3): 0.6
(8, 0.1, 0.1, 0.225): 0.733333333333
(8, 0.1, 0.1, 0.25): 0.666666666667
(8, 0.1, 0.1, 0.3): 0.8
(8, 1.0, 1.0, 0.2): 0.666666666667
(8, 1.0, 1.0, 0.225): 0.933333333333
(8, 1.0, 1.0, 0.25): 0.733333333333
(8, 1.0, 1.0, 0.275): 0.466666666667
(8, 1.0, 1.0, 0.3): 0.866666666667
(8, 10.0, 10.0, 0.2): 0.733333333333
(8, 10.0, 10.0, 0.225): 0.6
(8, 10.0, 10.0, 0.25): 0.733333333333
(8, 10.0, 10.0, 0.275): 0.8
(8, 10.0, 10.0, 0.3): 0.666666666667
(8, 100.0, 100.0, 0.2): 0.666666666667
(8, 100.0, 100.0, 0.225): 0.733333333333
(8, 100.0, 100.0, 0.25): 0.666666666667
(8, 100.0, 100.0, 0.275): 0.4
(8, 100.0, 100.0, 0.3): 0.6
(9, 0.1, 0.1, 0.25): 0.466666666667
(9, 0.1, 0.1, 0.275): 0.666666666667
(9, 1.0, 1.0, 0.2): 0.533333333333
(9, 1.0, 1.0, 0.225): 0.8
(9, 1.0, 1.0, 0.25): 0.6
(9, 1.0, 1.0, 0.275): 0.6
(9, 1.0, 1.0, 0.3): 0.666666666667
(9, 10.0, 10.0, 0.2): 0.6
(9, 10.0, 10.0, 0.225): 0.666666666667
(9, 10.0, 10.0, 0.25): 0.533333333333
(9, 10.0, 10.0, 0.275): 0.666666666667
(9, 10.0, 10.0, 0.3): 0.6
(9, 100.0, 100.0, 0.2): 0.6
(9, 100.0, 100.0, 0.225): 0.666666666667
(9, 100.0, 100.0, 0.25): 0.6
(9, 100.0, 100.0, 0.275): 0.466666666667
(9, 100.0, 100.0, 0.3): 0.6

Accuracy mean :0.3558139534883721
Std deviation :0.11507124637929607
Loss mean :0.6441860465116278
Std deviation :0.11507124637929607



Linear_LinearMu_2dim, number of iterations: 10

Parameters info
Tradeoffs parameter: [  0.1   1.   10.  100. ]
Using same parameters for all the clusterization: True
Gaussian kernel sigmas: [0.2   0.225 0.25  0.275 0.3  ]
Number of principal dimension considerated: 2
Dataset length: 150

Clusterization info
Minimum size of a cluster: in perc: 0.01, in int: 2
Type of mu: Linear Muzzifier
Fuzzifier: Linear
Cluster graph uses all connections: False
Cluster to label info
Force the number of cluster to be the number of different labels: False
Force the labels to be always represent by a cluster: False

Validation info
Conflict resolution: random
Number of cluster considerated for validation (top_k): None


RESULTS

On TEST set

Accuracy on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 1.0, 0.2): 0.866666666667
(1, 1.0, 1.0, 0.25): 0.8
(2, 100.0, 100.0, 0.2): 0.933333333333
(3, 10.0, 10.0, 0.25): 0.533333333333
(4, 1.0, 1.0, 0.2): 0.866666666667
(5, 1.0, 1.0, 0.25): 0.6
(6, 1.0, 1.0, 0.225): 0.8
(7, 1.0, 1.0, 0.2): 0.6
(8, 1.0, 1.0, 0.2): 0.8
(9, 1.0, 1.0, 0.25): 0.6

Loss on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 1.0, 0.2): 0.133333333333
(1, 1.0, 1.0, 0.25): 0.2
(2, 100.0, 100.0, 0.2): 0.0666666666667
(3, 10.0, 10.0, 0.25): 0.466666666667
(4, 1.0, 1.0, 0.2): 0.133333333333
(5, 1.0, 1.0, 0.25): 0.4
(6, 1.0, 1.0, 0.225): 0.2
(7, 1.0, 1.0, 0.2): 0.4
(8, 1.0, 1.0, 0.2): 0.2
(9, 1.0, 1.0, 0.25): 0.4

Accuracy mean :0.74
Std deviation :0.1348249894410446
Loss mean :0.26
Std deviation :0.13482498944104457

On TRAINING set

Accuracy on training set for every iteration (n_of_the_iter, c, sigma):
(0, 1.0, 1.0, 0.2): 0.918518518519
(1, 1.0, 1.0, 0.25): 0.866666666667
(2, 100.0, 100.0, 0.2): 0.814814814815
(3, 10.0, 10.0, 0.25): 0.377777777778
(4, 1.0, 1.0, 0.2): 0.851851851852
(5, 1.0, 1.0, 0.25): 0.62962962963
(6, 1.0, 1.0, 0.225): 0.777777777778
(7, 1.0, 1.0, 0.2): 0.696296296296
(8, 1.0, 1.0, 0.2): 0.866666666667
(9, 1.0, 1.0, 0.25): 0.674074074074
Loss on training set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 1.0, 0.2): 0.0814814814815
(1, 1.0, 1.0, 0.25): 0.133333333333
(2, 100.0, 100.0, 0.2): 0.185185185185
(3, 10.0, 10.0, 0.25): 0.622222222222
(4, 1.0, 1.0, 0.2): 0.148148148148
(5, 1.0, 1.0, 0.25): 0.37037037037
(6, 1.0, 1.0, 0.225): 0.222222222222
(7, 1.0, 1.0, 0.2): 0.303703703704
(8, 1.0, 1.0, 0.2): 0.133333333333
(9, 1.0, 1.0, 0.25): 0.325925925926

Accuracy mean :0.7474074074074074
Std deviation :0.15250806720437535
Loss mean :0.2525925925925926
Std deviation :0.15250806720437535

On ALL the sets

Accuracy on all the set for every iteration, for every couple c,sigma (n_of_the_iter, c, sigma):
(0, 1.0, 1.0, 0.2): 1.0
(0, 1.0, 1.0, 0.225): 0.933333333333
(0, 1.0, 1.0, 0.25): 0.866666666667
(0, 1.0, 1.0, 0.275): 0.333333333333
(0, 1.0, 1.0, 0.3): 0.933333333333
(0, 10.0, 10.0, 0.2): 0.8
(0, 10.0, 10.0, 0.225): 0.733333333333
(0, 10.0, 10.0, 0.25): 0.533333333333
(0, 10.0, 10.0, 0.275): 0.333333333333
(0, 10.0, 10.0, 0.3): 0.8
(0, 100.0, 100.0, 0.2): 0.8
(0, 100.0, 100.0, 0.225): 0.8
(0, 100.0, 100.0, 0.25): 0.333333333333
(0, 100.0, 100.0, 0.275): 0.333333333333
(0, 100.0, 100.0, 0.3): 0.666666666667
(1, 1.0, 1.0, 0.2): 0.933333333333
(1, 1.0, 1.0, 0.225): 0.933333333333
(1, 1.0, 1.0, 0.25): 0.933333333333
(1, 1.0, 1.0, 0.275): 0.933333333333
(1, 1.0, 1.0, 0.3): 0.866666666667
(1, 10.0, 10.0, 0.2): 0.933333333333
(1, 10.0, 10.0, 0.225): 0.866666666667
(1, 10.0, 10.0, 0.25): 0.866666666667
(1, 10.0, 10.0, 0.275): 0.933333333333
(1, 10.0, 10.0, 0.3): 0.866666666667
(1, 100.0, 100.0, 0.2): 0.866666666667
(1, 100.0, 100.0, 0.225): 0.933333333333
(1, 100.0, 100.0, 0.25): 0.466666666667
(1, 100.0, 100.0, 0.275): 0.866666666667
(1, 100.0, 100.0, 0.3): 0.866666666667
(2, 1.0, 1.0, 0.2): 1.0
(2, 1.0, 1.0, 0.225): 0.6
(2, 1.0, 1.0, 0.25): 0.666666666667
(2, 1.0, 1.0, 0.275): 0.666666666667
(2, 1.0, 1.0, 0.3): 0.6
(2, 10.0, 10.0, 0.2): 0.333333333333
(2, 10.0, 10.0, 0.225): 0.533333333333
(2, 10.0, 10.0, 0.25): 0.666666666667
(2, 10.0, 10.0, 0.275): 0.733333333333
(2, 10.0, 10.0, 0.3): 0.6
(2, 100.0, 100.0, 0.2): 1.0
(2, 100.0, 100.0, 0.225): 0.6
(2, 100.0, 100.0, 0.25): 0.8
(2, 100.0, 100.0, 0.275): 0.666666666667
(2, 100.0, 100.0, 0.3): 0.6
(3, 0.1, 0.1, 0.3): 1.0
(3, 1.0, 1.0, 0.2): 0.933333333333
(3, 1.0, 1.0, 0.225): 0.933333333333
(3, 1.0, 1.0, 0.25): 1.0
(3, 1.0, 1.0, 0.275): 0.2
(3, 1.0, 1.0, 0.3): 0.533333333333
(3, 10.0, 10.0, 0.2): 0.866666666667
(3, 10.0, 10.0, 0.225): 1.0
(3, 10.0, 10.0, 0.25): 1.0
(3, 10.0, 10.0, 0.275): 0.2
(3, 10.0, 10.0, 0.3): 0.533333333333
(3, 100.0, 100.0, 0.2): 0.933333333333
(3, 100.0, 100.0, 0.225): 0.933333333333
(3, 100.0, 100.0, 0.25): 1.0
(3, 100.0, 100.0, 0.275): 0.4
(3, 100.0, 100.0, 0.3): 0.533333333333
(4, 1.0, 1.0, 0.2): 0.8
(4, 1.0, 1.0, 0.225): 0.733333333333
(4, 1.0, 1.0, 0.25): 0.8
(4, 1.0, 1.0, 0.275): 0.733333333333
(4, 1.0, 1.0, 0.3): 0.733333333333
(4, 10.0, 10.0, 0.2): 0.733333333333
(4, 10.0, 10.0, 0.225): 0.733333333333
(4, 10.0, 10.0, 0.25): 0.733333333333
(4, 10.0, 10.0, 0.275): 0.733333333333
(4, 10.0, 10.0, 0.3): 0.666666666667
(4, 100.0, 100.0, 0.2): 0.733333333333
(4, 100.0, 100.0, 0.225): 0.733333333333
(4, 100.0, 100.0, 0.25): 0.733333333333
(4, 100.0, 100.0, 0.275): 0.666666666667
(4, 100.0, 100.0, 0.3): 0.666666666667
(5, 1.0, 1.0, 0.2): 0.8
(5, 1.0, 1.0, 0.225): 0.733333333333
(5, 1.0, 1.0, 0.25): 0.866666666667
(5, 1.0, 1.0, 0.275): 0.8
(5, 1.0, 1.0, 0.3): 0.733333333333
(5, 10.0, 10.0, 0.2): 0.8
(5, 10.0, 10.0, 0.225): 0.733333333333
(5, 10.0, 10.0, 0.25): 0.8
(5, 10.0, 10.0, 0.275): 0.8
(5, 10.0, 10.0, 0.3): 0.8
(5, 100.0, 100.0, 0.2): 0.666666666667
(5, 100.0, 100.0, 0.225): 0.666666666667
(5, 100.0, 100.0, 0.25): 0.8
(5, 100.0, 100.0, 0.275): 0.733333333333
(5, 100.0, 100.0, 0.3): 0.533333333333
(6, 0.1, 0.1, 0.275): 0.8
(6, 0.1, 0.1, 0.3): 0.8
(6, 1.0, 1.0, 0.2): 1.0
(6, 1.0, 1.0, 0.225): 1.0
(6, 1.0, 1.0, 0.25): 0.0666666666667
(6, 1.0, 1.0, 0.275): 0.8
(6, 1.0, 1.0, 0.3): 0.8
(6, 10.0, 10.0, 0.2): 0.933333333333
(6, 10.0, 10.0, 0.225): 0.0666666666667
(6, 10.0, 10.0, 0.25): 0.933333333333
(6, 10.0, 10.0, 0.275): 0.8
(6, 10.0, 10.0, 0.3): 0.8
(6, 100.0, 100.0, 0.2): 0.933333333333
(6, 100.0, 100.0, 0.225): 0.933333333333
(6, 100.0, 100.0, 0.25): 0.866666666667
(6, 100.0, 100.0, 0.275): 0.8
(6, 100.0, 100.0, 0.3): 0.8
(7, 0.1, 0.1, 0.25): 0.6
(7, 0.1, 0.1, 0.3): 0.6
(7, 1.0, 1.0, 0.2): 0.933333333333
(7, 1.0, 1.0, 0.225): 0.866666666667
(7, 1.0, 1.0, 0.25): 0.6
(7, 1.0, 1.0, 0.275): 0.4
(7, 1.0, 1.0, 0.3): 0.666666666667
(7, 10.0, 10.0, 0.2): 0.866666666667
(7, 10.0, 10.0, 0.225): 0.866666666667
(7, 10.0, 10.0, 0.25): 0.8
(7, 10.0, 10.0, 0.275): 0.333333333333
(7, 10.0, 10.0, 0.3): 0.733333333333
(7, 100.0, 100.0, 0.2): 0.866666666667
(7, 100.0, 100.0, 0.225): 0.8
(7, 100.0, 100.0, 0.25): 0.733333333333
(7, 100.0, 100.0, 0.275): 0.266666666667
(7, 100.0, 100.0, 0.3): 0.666666666667
(8, 1.0, 1.0, 0.2): 1.0
(8, 1.0, 1.0, 0.225): 0.8
(8, 1.0, 1.0, 0.25): 0.666666666667
(8, 1.0, 1.0, 0.275): 0.866666666667
(8, 1.0, 1.0, 0.3): 0.2
(8, 10.0, 10.0, 0.2): 0.933333333333
(8, 10.0, 10.0, 0.225): 0.933333333333
(8, 10.0, 10.0, 0.25): 0.733333333333
(8, 10.0, 10.0, 0.275): 0.866666666667
(8, 10.0, 10.0, 0.3): 0.666666666667
(8, 100.0, 100.0, 0.2): 0.866666666667
(8, 100.0, 100.0, 0.225): 1.0
(8, 100.0, 100.0, 0.25): 0.866666666667
(8, 100.0, 100.0, 0.275): 0.8
(8, 100.0, 100.0, 0.3): 0.2
(9, 0.1, 0.1, 0.225): 0.933333333333
(9, 1.0, 1.0, 0.2): 0.533333333333
(9, 1.0, 1.0, 0.225): 0.866666666667
(9, 1.0, 1.0, 0.25): 1.0
(9, 1.0, 1.0, 0.275): 0.933333333333
(9, 1.0, 1.0, 0.3): 0.6
(9, 10.0, 10.0, 0.2): 0.2
(9, 10.0, 10.0, 0.225): 0.933333333333
(9, 10.0, 10.0, 0.25): 0.933333333333
(9, 10.0, 10.0, 0.275): 0.733333333333
(9, 10.0, 10.0, 0.3): 0.4
(9, 100.0, 100.0, 0.2): 0.333333333333
(9, 100.0, 100.0, 0.225): 0.866666666667
(9, 100.0, 100.0, 0.25): 0.866666666667
(9, 100.0, 100.0, 0.275): 0.733333333333
(9, 100.0, 100.0, 0.3): 0.533333333333

Loss on all the set for every iteration, for every couple c,sigma (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 1.0, 0.2): 0.0
(0, 1.0, 1.0, 0.225): 0.0666666666667
(0, 1.0, 1.0, 0.25): 0.133333333333
(0, 1.0, 1.0, 0.275): 0.666666666667
(0, 1.0, 1.0, 0.3): 0.0666666666667
(0, 10.0, 10.0, 0.2): 0.2
(0, 10.0, 10.0, 0.225): 0.266666666667
(0, 10.0, 10.0, 0.25): 0.466666666667
(0, 10.0, 10.0, 0.275): 0.666666666667
(0, 10.0, 10.0, 0.3): 0.2
(0, 100.0, 100.0, 0.2): 0.2
(0, 100.0, 100.0, 0.225): 0.2
(0, 100.0, 100.0, 0.25): 0.666666666667
(0, 100.0, 100.0, 0.275): 0.666666666667
(0, 100.0, 100.0, 0.3): 0.333333333333
(1, 1.0, 1.0, 0.2): 0.0666666666667
(1, 1.0, 1.0, 0.225): 0.0666666666667
(1, 1.0, 1.0, 0.25): 0.0666666666667
(1, 1.0, 1.0, 0.275): 0.0666666666667
(1, 1.0, 1.0, 0.3): 0.133333333333
(1, 10.0, 10.0, 0.2): 0.0666666666667
(1, 10.0, 10.0, 0.225): 0.133333333333
(1, 10.0, 10.0, 0.25): 0.133333333333
(1, 10.0, 10.0, 0.275): 0.0666666666667
(1, 10.0, 10.0, 0.3): 0.133333333333
(1, 100.0, 100.0, 0.2): 0.133333333333
(1, 100.0, 100.0, 0.225): 0.0666666666667
(1, 100.0, 100.0, 0.25): 0.533333333333
(1, 100.0, 100.0, 0.275): 0.133333333333
(1, 100.0, 100.0, 0.3): 0.133333333333
(2, 1.0, 1.0, 0.2): 0.0
(2, 1.0, 1.0, 0.225): 0.4
(2, 1.0, 1.0, 0.25): 0.333333333333
(2, 1.0, 1.0, 0.275): 0.333333333333
(2, 1.0, 1.0, 0.3): 0.4
(2, 10.0, 10.0, 0.2): 0.666666666667
(2, 10.0, 10.0, 0.225): 0.466666666667
(2, 10.0, 10.0, 0.25): 0.333333333333
(2, 10.0, 10.0, 0.275): 0.266666666667
(2, 10.0, 10.0, 0.3): 0.4
(2, 100.0, 100.0, 0.2): 0.0
(2, 100.0, 100.0, 0.225): 0.4
(2, 100.0, 100.0, 0.25): 0.2
(2, 100.0, 100.0, 0.275): 0.333333333333
(2, 100.0, 100.0, 0.3): 0.4
(3, 0.1, 0.1, 0.3): 0.0
(3, 1.0, 1.0, 0.2): 0.0666666666667
(3, 1.0, 1.0, 0.225): 0.0666666666667
(3, 1.0, 1.0, 0.25): 0.0
(3, 1.0, 1.0, 0.275): 0.8
(3, 1.0, 1.0, 0.3): 0.466666666667
(3, 10.0, 10.0, 0.2): 0.133333333333
(3, 10.0, 10.0, 0.225): 0.0
(3, 10.0, 10.0, 0.25): 0.0
(3, 10.0, 10.0, 0.275): 0.8
(3, 10.0, 10.0, 0.3): 0.466666666667
(3, 100.0, 100.0, 0.2): 0.0666666666667
(3, 100.0, 100.0, 0.225): 0.0666666666667
(3, 100.0, 100.0, 0.25): 0.0
(3, 100.0, 100.0, 0.275): 0.6
(3, 100.0, 100.0, 0.3): 0.466666666667
(4, 1.0, 1.0, 0.2): 0.2
(4, 1.0, 1.0, 0.225): 0.266666666667
(4, 1.0, 1.0, 0.25): 0.2
(4, 1.0, 1.0, 0.275): 0.266666666667
(4, 1.0, 1.0, 0.3): 0.266666666667
(4, 10.0, 10.0, 0.2): 0.266666666667
(4, 10.0, 10.0, 0.225): 0.266666666667
(4, 10.0, 10.0, 0.25): 0.266666666667
(4, 10.0, 10.0, 0.275): 0.266666666667
(4, 10.0, 10.0, 0.3): 0.333333333333
(4, 100.0, 100.0, 0.2): 0.266666666667
(4, 100.0, 100.0, 0.225): 0.266666666667
(4, 100.0, 100.0, 0.25): 0.266666666667
(4, 100.0, 100.0, 0.275): 0.333333333333
(4, 100.0, 100.0, 0.3): 0.333333333333
(5, 1.0, 1.0, 0.2): 0.2
(5, 1.0, 1.0, 0.225): 0.266666666667
(5, 1.0, 1.0, 0.25): 0.133333333333
(5, 1.0, 1.0, 0.275): 0.2
(5, 1.0, 1.0, 0.3): 0.266666666667
(5, 10.0, 10.0, 0.2): 0.2
(5, 10.0, 10.0, 0.225): 0.266666666667
(5, 10.0, 10.0, 0.25): 0.2
(5, 10.0, 10.0, 0.275): 0.2
(5, 10.0, 10.0, 0.3): 0.2
(5, 100.0, 100.0, 0.2): 0.333333333333
(5, 100.0, 100.0, 0.225): 0.333333333333
(5, 100.0, 100.0, 0.25): 0.2
(5, 100.0, 100.0, 0.275): 0.266666666667
(5, 100.0, 100.0, 0.3): 0.466666666667
(6, 0.1, 0.1, 0.275): 0.2
(6, 0.1, 0.1, 0.3): 0.2
(6, 1.0, 1.0, 0.2): 0.0
(6, 1.0, 1.0, 0.225): 0.0
(6, 1.0, 1.0, 0.25): 0.933333333333
(6, 1.0, 1.0, 0.275): 0.2
(6, 1.0, 1.0, 0.3): 0.2
(6, 10.0, 10.0, 0.2): 0.0666666666667
(6, 10.0, 10.0, 0.225): 0.933333333333
(6, 10.0, 10.0, 0.25): 0.0666666666667
(6, 10.0, 10.0, 0.275): 0.2
(6, 10.0, 10.0, 0.3): 0.2
(6, 100.0, 100.0, 0.2): 0.0666666666667
(6, 100.0, 100.0, 0.225): 0.0666666666667
(6, 100.0, 100.0, 0.25): 0.133333333333
(6, 100.0, 100.0, 0.275): 0.2
(6, 100.0, 100.0, 0.3): 0.2
(7, 0.1, 0.1, 0.25): 0.4
(7, 0.1, 0.1, 0.3): 0.4
(7, 1.0, 1.0, 0.2): 0.0666666666667
(7, 1.0, 1.0, 0.225): 0.133333333333
(7, 1.0, 1.0, 0.25): 0.4
(7, 1.0, 1.0, 0.275): 0.6
(7, 1.0, 1.0, 0.3): 0.333333333333
(7, 10.0, 10.0, 0.2): 0.133333333333
(7, 10.0, 10.0, 0.225): 0.133333333333
(7, 10.0, 10.0, 0.25): 0.2
(7, 10.0, 10.0, 0.275): 0.666666666667
(7, 10.0, 10.0, 0.3): 0.266666666667
(7, 100.0, 100.0, 0.2): 0.133333333333
(7, 100.0, 100.0, 0.225): 0.2
(7, 100.0, 100.0, 0.25): 0.266666666667
(7, 100.0, 100.0, 0.275): 0.733333333333
(7, 100.0, 100.0, 0.3): 0.333333333333
(8, 1.0, 1.0, 0.2): 0.0
(8, 1.0, 1.0, 0.225): 0.2
(8, 1.0, 1.0, 0.25): 0.333333333333
(8, 1.0, 1.0, 0.275): 0.133333333333
(8, 1.0, 1.0, 0.3): 0.8
(8, 10.0, 10.0, 0.2): 0.0666666666667
(8, 10.0, 10.0, 0.225): 0.0666666666667
(8, 10.0, 10.0, 0.25): 0.266666666667
(8, 10.0, 10.0, 0.275): 0.133333333333
(8, 10.0, 10.0, 0.3): 0.333333333333
(8, 100.0, 100.0, 0.2): 0.133333333333
(8, 100.0, 100.0, 0.225): 0.0
(8, 100.0, 100.0, 0.25): 0.133333333333
(8, 100.0, 100.0, 0.275): 0.2
(8, 100.0, 100.0, 0.3): 0.8
(9, 0.1, 0.1, 0.225): 0.0666666666667
(9, 1.0, 1.0, 0.2): 0.466666666667
(9, 1.0, 1.0, 0.225): 0.133333333333
(9, 1.0, 1.0, 0.25): 0.0
(9, 1.0, 1.0, 0.275): 0.0666666666667
(9, 1.0, 1.0, 0.3): 0.4
(9, 10.0, 10.0, 0.2): 0.8
(9, 10.0, 10.0, 0.225): 0.0666666666667
(9, 10.0, 10.0, 0.25): 0.0666666666667
(9, 10.0, 10.0, 0.275): 0.266666666667
(9, 10.0, 10.0, 0.3): 0.6
(9, 100.0, 100.0, 0.2): 0.666666666667
(9, 100.0, 100.0, 0.225): 0.133333333333
(9, 100.0, 100.0, 0.25): 0.133333333333
(9, 100.0, 100.0, 0.275): 0.266666666667
(9, 100.0, 100.0, 0.3): 0.466666666667

Accuracy mean :0.7376068376068375
Std deviation :0.21158464487658155
Loss mean :0.2623931623931624
Std deviation :0.21158464487658155



Linear_LinearMu_3dim, number of iterations: 10

Parameters info
Tradeoffs parameter: [  0.1   1.   10.  100. ]
Using same parameters for all the clusterization: True
Gaussian kernel sigmas: [0.2   0.225 0.25  0.275 0.3  ]
Number of principal dimension considerated: 3
Dataset length: 150

Clusterization info
Minimum size of a cluster: in perc: 0.01, in int: 2
Type of mu: Linear Muzzifier
Fuzzifier: Linear
Cluster graph uses all connections: False
Cluster to label info
Force the number of cluster to be the number of different labels: False
Force the labels to be always represent by a cluster: False

Validation info
Conflict resolution: random
Number of cluster considerated for validation (top_k): None


RESULTS

On TEST set

Accuracy on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 100.0, 100.0, 0.2): 0.933333333333
(1, 1.0, 1.0, 0.225): 0.866666666667
(2, 100.0, 100.0, 0.2): 0.266666666667
(3, 10.0, 10.0, 0.225): 0.933333333333
(4, 1.0, 1.0, 0.225): 0.6
(5, 10.0, 10.0, 0.2): 0.866666666667
(6, 10.0, 10.0, 0.2): 0.933333333333
(7, 1.0, 1.0, 0.2): 0.8
(8, 1.0, 1.0, 0.225): 0.733333333333
(9, 10.0, 10.0, 0.25): 0.866666666667

Loss on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 100.0, 100.0, 0.2): 0.0666666666667
(1, 1.0, 1.0, 0.225): 0.133333333333
(2, 100.0, 100.0, 0.2): 0.733333333333
(3, 10.0, 10.0, 0.225): 0.0666666666667
(4, 1.0, 1.0, 0.225): 0.4
(5, 10.0, 10.0, 0.2): 0.133333333333
(6, 10.0, 10.0, 0.2): 0.0666666666667
(7, 1.0, 1.0, 0.2): 0.2
(8, 1.0, 1.0, 0.225): 0.266666666667
(9, 10.0, 10.0, 0.25): 0.133333333333

Accuracy mean :0.78
Std deviation :0.19787762772874443
Loss mean :0.22000000000000003
Std deviation :0.1978776277287444

On TRAINING set

Accuracy on training set for every iteration (n_of_the_iter, c, sigma):
(0, 100.0, 100.0, 0.2): 0.859259259259
(1, 1.0, 1.0, 0.225): 0.866666666667
(2, 100.0, 100.0, 0.2): 0.340740740741
(3, 10.0, 10.0, 0.225): 0.859259259259
(4, 1.0, 1.0, 0.225): 0.674074074074
(5, 10.0, 10.0, 0.2): 0.881481481481
(6, 10.0, 10.0, 0.2): 0.8
(7, 1.0, 1.0, 0.2): 0.837037037037
(8, 1.0, 1.0, 0.225): 0.888888888889
(9, 10.0, 10.0, 0.25): 0.844444444444
Loss on training set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 100.0, 100.0, 0.2): 0.140740740741
(1, 1.0, 1.0, 0.225): 0.133333333333
(2, 100.0, 100.0, 0.2): 0.659259259259
(3, 10.0, 10.0, 0.225): 0.140740740741
(4, 1.0, 1.0, 0.225): 0.325925925926
(5, 10.0, 10.0, 0.2): 0.118518518519
(6, 10.0, 10.0, 0.2): 0.2
(7, 1.0, 1.0, 0.2): 0.162962962963
(8, 1.0, 1.0, 0.225): 0.111111111111
(9, 10.0, 10.0, 0.25): 0.155555555556

Accuracy mean :0.7851851851851852
Std deviation :0.1593539770017761
Loss mean :0.21481481481481485
Std deviation :0.1593539770017761

On ALL the sets

Accuracy on all the set for every iteration, for every couple c,sigma (n_of_the_iter, c, sigma):
(0, 0.1, 0.1, 0.275): 0.533333333333
(0, 1.0, 1.0, 0.2): 0.933333333333
(0, 1.0, 1.0, 0.225): 0.933333333333
(0, 1.0, 1.0, 0.25): 0.533333333333
(0, 1.0, 1.0, 0.275): 0.533333333333
(0, 1.0, 1.0, 0.3): 0.533333333333
(0, 10.0, 10.0, 0.2): 0.933333333333
(0, 10.0, 10.0, 0.225): 0.933333333333
(0, 10.0, 10.0, 0.25): 0.533333333333
(0, 10.0, 10.0, 0.275): 0.533333333333
(0, 10.0, 10.0, 0.3): 0.533333333333
(0, 100.0, 100.0, 0.2): 1.0
(0, 100.0, 100.0, 0.225): 0.933333333333
(0, 100.0, 100.0, 0.25): 0.533333333333
(0, 100.0, 100.0, 0.275): 0.533333333333
(0, 100.0, 100.0, 0.3): 0.533333333333
(1, 1.0, 1.0, 0.225): 1.0
(1, 1.0, 1.0, 0.25): 0.866666666667
(1, 1.0, 1.0, 0.275): 0.933333333333
(1, 1.0, 1.0, 0.3): 0.8
(1, 10.0, 10.0, 0.225): 1.0
(1, 10.0, 10.0, 0.25): 0.866666666667
(1, 10.0, 10.0, 0.275): 0.933333333333
(1, 10.0, 10.0, 0.3): 0.6
(1, 100.0, 100.0, 0.225): 1.0
(1, 100.0, 100.0, 0.25): 0.933333333333
(1, 100.0, 100.0, 0.275): 0.933333333333
(1, 100.0, 100.0, 0.3): 0.6
(2, 0.1, 0.1, 0.275): 0.866666666667
(2, 0.1, 0.1, 0.3): 0.6
(2, 1.0, 1.0, 0.2): 0.933333333333
(2, 1.0, 1.0, 0.225): 0.866666666667
(2, 1.0, 1.0, 0.25): 0.866666666667
(2, 1.0, 1.0, 0.275): 0.6
(2, 1.0, 1.0, 0.3): 0.6
(2, 10.0, 10.0, 0.2): 0.933333333333
(2, 10.0, 10.0, 0.225): 0.933333333333
(2, 10.0, 10.0, 0.25): 0.8
(2, 10.0, 10.0, 0.275): 0.6
(2, 10.0, 10.0, 0.3): 0.6
(2, 100.0, 100.0, 0.2): 0.933333333333
(2, 100.0, 100.0, 0.225): 0.733333333333
(2, 100.0, 100.0, 0.25): 0.8
(2, 100.0, 100.0, 0.275): 0.6
(2, 100.0, 100.0, 0.3): 0.6
(3, 0.1, 0.1, 0.225): 1.0
(3, 0.1, 0.1, 0.275): 0.533333333333
(3, 0.1, 0.1, 0.3): 0.866666666667
(3, 1.0, 1.0, 0.2): 0.933333333333
(3, 1.0, 1.0, 0.225): 0.866666666667
(3, 1.0, 1.0, 0.25): 0.666666666667
(3, 1.0, 1.0, 0.275): 0.533333333333
(3, 1.0, 1.0, 0.3): 0.866666666667
(3, 10.0, 10.0, 0.2): 0.933333333333
(3, 10.0, 10.0, 0.225): 0.933333333333
(3, 10.0, 10.0, 0.25): 0.733333333333
(3, 10.0, 10.0, 0.275): 0.533333333333
(3, 10.0, 10.0, 0.3): 0.533333333333
(3, 100.0, 100.0, 0.2): 0.866666666667
(3, 100.0, 100.0, 0.225): 0.8
(3, 100.0, 100.0, 0.25): 0.666666666667
(3, 100.0, 100.0, 0.275): 0.533333333333
(3, 100.0, 100.0, 0.3): 0.8
(4, 0.1, 0.1, 0.275): 0.8
(4, 1.0, 1.0, 0.2): 0.666666666667
(4, 1.0, 1.0, 0.225): 0.933333333333
(4, 1.0, 1.0, 0.25): 0.8
(4, 1.0, 1.0, 0.275): 0.8
(4, 1.0, 1.0, 0.3): 0.333333333333
(4, 10.0, 10.0, 0.2): 0.666666666667
(4, 10.0, 10.0, 0.225): 0.866666666667
(4, 10.0, 10.0, 0.25): 0.866666666667
(4, 10.0, 10.0, 0.275): 0.866666666667
(4, 10.0, 10.0, 0.3): 0.666666666667
(4, 100.0, 100.0, 0.2): 0.666666666667
(4, 100.0, 100.0, 0.225): 0.933333333333
(4, 100.0, 100.0, 0.25): 0.8
(4, 100.0, 100.0, 0.275): 0.866666666667
(4, 100.0, 100.0, 0.3): 0.666666666667
(5, 0.1, 0.1, 0.3): 0.6
(5, 1.0, 1.0, 0.2): 0.866666666667
(5, 1.0, 1.0, 0.225): 0.266666666667
(5, 1.0, 1.0, 0.25): 0.866666666667
(5, 1.0, 1.0, 0.275): 0.733333333333
(5, 1.0, 1.0, 0.3): 0.6
(5, 10.0, 10.0, 0.2): 0.866666666667
(5, 10.0, 10.0, 0.225): 0.266666666667
(5, 10.0, 10.0, 0.25): 0.866666666667
(5, 10.0, 10.0, 0.275): 0.666666666667
(5, 10.0, 10.0, 0.3): 0.6
(5, 100.0, 100.0, 0.2): 0.866666666667
(5, 100.0, 100.0, 0.225): 0.266666666667
(5, 100.0, 100.0, 0.25): 0.666666666667
(5, 100.0, 100.0, 0.275): 0.733333333333
(5, 100.0, 100.0, 0.3): 0.6
(6, 1.0, 1.0, 0.2): 1.0
(6, 1.0, 1.0, 0.225): 1.0
(6, 1.0, 1.0, 0.25): 0.866666666667
(6, 1.0, 1.0, 0.275): 0.466666666667
(6, 1.0, 1.0, 0.3): 0.866666666667
(6, 10.0, 10.0, 0.2): 1.0
(6, 10.0, 10.0, 0.225): 0.933333333333
(6, 10.0, 10.0, 0.25): 0.866666666667
(6, 10.0, 10.0, 0.275): 0.466666666667
(6, 10.0, 10.0, 0.3): 0.6
(6, 100.0, 100.0, 0.2): 0.933333333333
(6, 100.0, 100.0, 0.225): 0.933333333333
(6, 100.0, 100.0, 0.25): 0.866666666667
(6, 100.0, 100.0, 0.275): 0.466666666667
(6, 100.0, 100.0, 0.3): 0.866666666667
(7, 1.0, 1.0, 0.2): 0.933333333333
(7, 1.0, 1.0, 0.225): 0.933333333333
(7, 1.0, 1.0, 0.25): 0.866666666667
(7, 1.0, 1.0, 0.275): 0.8
(7, 1.0, 1.0, 0.3): 0.533333333333
(7, 10.0, 10.0, 0.2): 0.866666666667
(7, 10.0, 10.0, 0.225): 0.866666666667
(7, 10.0, 10.0, 0.25): 0.733333333333
(7, 10.0, 10.0, 0.275): 0.866666666667
(7, 10.0, 10.0, 0.3): 0.6
(7, 100.0, 100.0, 0.2): 0.933333333333
(7, 100.0, 100.0, 0.225): 0.8
(7, 100.0, 100.0, 0.25): 0.8
(7, 100.0, 100.0, 0.275): 0.733333333333
(7, 100.0, 100.0, 0.3): 0.466666666667
(8, 0.1, 0.1, 0.275): 0.666666666667
(8, 0.1, 0.1, 0.3): 0.2
(8, 1.0, 1.0, 0.2): 0.933333333333
(8, 1.0, 1.0, 0.225): 1.0
(8, 1.0, 1.0, 0.25): 0.666666666667
(8, 1.0, 1.0, 0.275): 0.666666666667
(8, 1.0, 1.0, 0.3): 0.666666666667
(8, 10.0, 10.0, 0.2): 0.933333333333
(8, 10.0, 10.0, 0.225): 0.8
(8, 10.0, 10.0, 0.25): 0.8
(8, 10.0, 10.0, 0.275): 0.666666666667
(8, 10.0, 10.0, 0.3): 0.666666666667
(8, 100.0, 100.0, 0.2): 0.933333333333
(8, 100.0, 100.0, 0.225): 0.666666666667
(8, 100.0, 100.0, 0.25): 0.733333333333
(8, 100.0, 100.0, 0.275): 0.666666666667
(8, 100.0, 100.0, 0.3): 0.666666666667
(9, 0.1, 0.1, 0.225): 0.266666666667
(9, 0.1, 0.1, 0.3): 0.8
(9, 1.0, 1.0, 0.225): 0.266666666667
(9, 1.0, 1.0, 0.25): 0.8
(9, 1.0, 1.0, 0.275): 0.8
(9, 1.0, 1.0, 0.3): 0.8
(9, 10.0, 10.0, 0.225): 0.266666666667
(9, 10.0, 10.0, 0.25): 0.866666666667
(9, 10.0, 10.0, 0.275): 0.666666666667
(9, 10.0, 10.0, 0.3): 0.8
(9, 100.0, 100.0, 0.225): 0.8
(9, 100.0, 100.0, 0.25): 0.733333333333
(9, 100.0, 100.0, 0.275): 0.666666666667
(9, 100.0, 100.0, 0.3): 0.733333333333

Loss on all the set for every iteration, for every couple c,sigma (n_of_the_iter, best_c, best_sigma):
(0, 0.1, 0.1, 0.275): 0.466666666667
(0, 1.0, 1.0, 0.2): 0.0666666666667
(0, 1.0, 1.0, 0.225): 0.0666666666667
(0, 1.0, 1.0, 0.25): 0.466666666667
(0, 1.0, 1.0, 0.275): 0.466666666667
(0, 1.0, 1.0, 0.3): 0.466666666667
(0, 10.0, 10.0, 0.2): 0.0666666666667
(0, 10.0, 10.0, 0.225): 0.0666666666667
(0, 10.0, 10.0, 0.25): 0.466666666667
(0, 10.0, 10.0, 0.275): 0.466666666667
(0, 10.0, 10.0, 0.3): 0.466666666667
(0, 100.0, 100.0, 0.2): 0.0
(0, 100.0, 100.0, 0.225): 0.0666666666667
(0, 100.0, 100.0, 0.25): 0.466666666667
(0, 100.0, 100.0, 0.275): 0.466666666667
(0, 100.0, 100.0, 0.3): 0.466666666667
(1, 1.0, 1.0, 0.225): 0.0
(1, 1.0, 1.0, 0.25): 0.133333333333
(1, 1.0, 1.0, 0.275): 0.0666666666667
(1, 1.0, 1.0, 0.3): 0.2
(1, 10.0, 10.0, 0.225): 0.0
(1, 10.0, 10.0, 0.25): 0.133333333333
(1, 10.0, 10.0, 0.275): 0.0666666666667
(1, 10.0, 10.0, 0.3): 0.4
(1, 100.0, 100.0, 0.225): 0.0
(1, 100.0, 100.0, 0.25): 0.0666666666667
(1, 100.0, 100.0, 0.275): 0.0666666666667
(1, 100.0, 100.0, 0.3): 0.4
(2, 0.1, 0.1, 0.275): 0.133333333333
(2, 0.1, 0.1, 0.3): 0.4
(2, 1.0, 1.0, 0.2): 0.0666666666667
(2, 1.0, 1.0, 0.225): 0.133333333333
(2, 1.0, 1.0, 0.25): 0.133333333333
(2, 1.0, 1.0, 0.275): 0.4
(2, 1.0, 1.0, 0.3): 0.4
(2, 10.0, 10.0, 0.2): 0.0666666666667
(2, 10.0, 10.0, 0.225): 0.0666666666667
(2, 10.0, 10.0, 0.25): 0.2
(2, 10.0, 10.0, 0.275): 0.4
(2, 10.0, 10.0, 0.3): 0.4
(2, 100.0, 100.0, 0.2): 0.0666666666667
(2, 100.0, 100.0, 0.225): 0.266666666667
(2, 100.0, 100.0, 0.25): 0.2
(2, 100.0, 100.0, 0.275): 0.4
(2, 100.0, 100.0, 0.3): 0.4
(3, 0.1, 0.1, 0.225): 0.0
(3, 0.1, 0.1, 0.275): 0.466666666667
(3, 0.1, 0.1, 0.3): 0.133333333333
(3, 1.0, 1.0, 0.2): 0.0666666666667
(3, 1.0, 1.0, 0.225): 0.133333333333
(3, 1.0, 1.0, 0.25): 0.333333333333
(3, 1.0, 1.0, 0.275): 0.466666666667
(3, 1.0, 1.0, 0.3): 0.133333333333
(3, 10.0, 10.0, 0.2): 0.0666666666667
(3, 10.0, 10.0, 0.225): 0.0666666666667
(3, 10.0, 10.0, 0.25): 0.266666666667
(3, 10.0, 10.0, 0.275): 0.466666666667
(3, 10.0, 10.0, 0.3): 0.466666666667
(3, 100.0, 100.0, 0.2): 0.133333333333
(3, 100.0, 100.0, 0.225): 0.2
(3, 100.0, 100.0, 0.25): 0.333333333333
(3, 100.0, 100.0, 0.275): 0.466666666667
(3, 100.0, 100.0, 0.3): 0.2
(4, 0.1, 0.1, 0.275): 0.2
(4, 1.0, 1.0, 0.2): 0.333333333333
(4, 1.0, 1.0, 0.225): 0.0666666666667
(4, 1.0, 1.0, 0.25): 0.2
(4, 1.0, 1.0, 0.275): 0.2
(4, 1.0, 1.0, 0.3): 0.666666666667
(4, 10.0, 10.0, 0.2): 0.333333333333
(4, 10.0, 10.0, 0.225): 0.133333333333
(4, 10.0, 10.0, 0.25): 0.133333333333
(4, 10.0, 10.0, 0.275): 0.133333333333
(4, 10.0, 10.0, 0.3): 0.333333333333
(4, 100.0, 100.0, 0.2): 0.333333333333
(4, 100.0, 100.0, 0.225): 0.0666666666667
(4, 100.0, 100.0, 0.25): 0.2
(4, 100.0, 100.0, 0.275): 0.133333333333
(4, 100.0, 100.0, 0.3): 0.333333333333
(5, 0.1, 0.1, 0.3): 0.4
(5, 1.0, 1.0, 0.2): 0.133333333333
(5, 1.0, 1.0, 0.225): 0.733333333333
(5, 1.0, 1.0, 0.25): 0.133333333333
(5, 1.0, 1.0, 0.275): 0.266666666667
(5, 1.0, 1.0, 0.3): 0.4
(5, 10.0, 10.0, 0.2): 0.133333333333
(5, 10.0, 10.0, 0.225): 0.733333333333
(5, 10.0, 10.0, 0.25): 0.133333333333
(5, 10.0, 10.0, 0.275): 0.333333333333
(5, 10.0, 10.0, 0.3): 0.4
(5, 100.0, 100.0, 0.2): 0.133333333333
(5, 100.0, 100.0, 0.225): 0.733333333333
(5, 100.0, 100.0, 0.25): 0.333333333333
(5, 100.0, 100.0, 0.275): 0.266666666667
(5, 100.0, 100.0, 0.3): 0.4
(6, 1.0, 1.0, 0.2): 0.0
(6, 1.0, 1.0, 0.225): 0.0
(6, 1.0, 1.0, 0.25): 0.133333333333
(6, 1.0, 1.0, 0.275): 0.533333333333
(6, 1.0, 1.0, 0.3): 0.133333333333
(6, 10.0, 10.0, 0.2): 0.0
(6, 10.0, 10.0, 0.225): 0.0666666666667
(6, 10.0, 10.0, 0.25): 0.133333333333
(6, 10.0, 10.0, 0.275): 0.533333333333
(6, 10.0, 10.0, 0.3): 0.4
(6, 100.0, 100.0, 0.2): 0.0666666666667
(6, 100.0, 100.0, 0.225): 0.0666666666667
(6, 100.0, 100.0, 0.25): 0.133333333333
(6, 100.0, 100.0, 0.275): 0.533333333333
(6, 100.0, 100.0, 0.3): 0.133333333333
(7, 1.0, 1.0, 0.2): 0.0666666666667
(7, 1.0, 1.0, 0.225): 0.0666666666667
(7, 1.0, 1.0, 0.25): 0.133333333333
(7, 1.0, 1.0, 0.275): 0.2
(7, 1.0, 1.0, 0.3): 0.466666666667
(7, 10.0, 10.0, 0.2): 0.133333333333
(7, 10.0, 10.0, 0.225): 0.133333333333
(7, 10.0, 10.0, 0.25): 0.266666666667
(7, 10.0, 10.0, 0.275): 0.133333333333
(7, 10.0, 10.0, 0.3): 0.4
(7, 100.0, 100.0, 0.2): 0.0666666666667
(7, 100.0, 100.0, 0.225): 0.2
(7, 100.0, 100.0, 0.25): 0.2
(7, 100.0, 100.0, 0.275): 0.266666666667
(7, 100.0, 100.0, 0.3): 0.533333333333
(8, 0.1, 0.1, 0.275): 0.333333333333
(8, 0.1, 0.1, 0.3): 0.8
(8, 1.0, 1.0, 0.2): 0.0666666666667
(8, 1.0, 1.0, 0.225): 0.0
(8, 1.0, 1.0, 0.25): 0.333333333333
(8, 1.0, 1.0, 0.275): 0.333333333333
(8, 1.0, 1.0, 0.3): 0.333333333333
(8, 10.0, 10.0, 0.2): 0.0666666666667
(8, 10.0, 10.0, 0.225): 0.2
(8, 10.0, 10.0, 0.25): 0.2
(8, 10.0, 10.0, 0.275): 0.333333333333
(8, 10.0, 10.0, 0.3): 0.333333333333
(8, 100.0, 100.0, 0.2): 0.0666666666667
(8, 100.0, 100.0, 0.225): 0.333333333333
(8, 100.0, 100.0, 0.25): 0.266666666667
(8, 100.0, 100.0, 0.275): 0.333333333333
(8, 100.0, 100.0, 0.3): 0.333333333333
(9, 0.1, 0.1, 0.225): 0.733333333333
(9, 0.1, 0.1, 0.3): 0.2
(9, 1.0, 1.0, 0.225): 0.733333333333
(9, 1.0, 1.0, 0.25): 0.2
(9, 1.0, 1.0, 0.275): 0.2
(9, 1.0, 1.0, 0.3): 0.2
(9, 10.0, 10.0, 0.225): 0.733333333333
(9, 10.0, 10.0, 0.25): 0.133333333333
(9, 10.0, 10.0, 0.275): 0.333333333333
(9, 10.0, 10.0, 0.3): 0.2
(9, 100.0, 100.0, 0.225): 0.2
(9, 100.0, 100.0, 0.25): 0.266666666667
(9, 100.0, 100.0, 0.275): 0.333333333333
(9, 100.0, 100.0, 0.3): 0.266666666667

Accuracy mean :0.7431623931623931
Std deviation :0.18471576489136216
Loss mean :0.2568376068376068
Std deviation :0.18471576489136213



Linear_LinearMu_4dim, number of iterations: 10

Parameters info
Tradeoffs parameter: [  0.1   1.   10.  100. ]
Using same parameters for all the clusterization: True
Gaussian kernel sigmas: [0.2   0.225 0.25  0.275 0.3  ]
Number of principal dimension considerated: 4
Dataset length: 150

Clusterization info
Minimum size of a cluster: in perc: 0.01, in int: 2
Type of mu: Linear Muzzifier
Fuzzifier: Linear
Cluster graph uses all connections: False
Cluster to label info
Force the number of cluster to be the number of different labels: False
Force the labels to be always represent by a cluster: False

Validation info
Conflict resolution: random
Number of cluster considerated for validation (top_k): None


RESULTS

On TEST set

Accuracy on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 1.0, 0.225): 0.933333333333
(1, 100.0, 100.0, 0.225): 0.2
(2, 100.0, 100.0, 0.225): 0.933333333333
(3, 100.0, 100.0, 0.2): 0.533333333333
(4, 100.0, 100.0, 0.225): 0.666666666667
(5, 100.0, 100.0, 0.225): 0.933333333333
(6, 100.0, 100.0, 0.225): 0.266666666667
(7, 10.0, 10.0, 0.225): 0.666666666667
(8, 10.0, 10.0, 0.3): 0.666666666667
(9, 100.0, 100.0, 0.225): 0.4

Loss on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 1.0, 0.225): 0.0666666666667
(1, 100.0, 100.0, 0.225): 0.8
(2, 100.0, 100.0, 0.225): 0.0666666666667
(3, 100.0, 100.0, 0.2): 0.466666666667
(4, 100.0, 100.0, 0.225): 0.333333333333
(5, 100.0, 100.0, 0.225): 0.0666666666667
(6, 100.0, 100.0, 0.225): 0.733333333333
(7, 10.0, 10.0, 0.225): 0.333333333333
(8, 10.0, 10.0, 0.3): 0.333333333333
(9, 100.0, 100.0, 0.225): 0.6

Accuracy mean :0.62
Std deviation :0.2565584187319181
Loss mean :0.38
Std deviation :0.25655841873191815

On TRAINING set

Accuracy on training set for every iteration (n_of_the_iter, c, sigma):
(0, 1.0, 1.0, 0.225): 0.748148148148
(1, 100.0, 100.0, 0.225): 0.348148148148
(2, 100.0, 100.0, 0.225): 0.77037037037
(3, 100.0, 100.0, 0.2): 0.681481481481
(4, 100.0, 100.0, 0.225): 0.666666666667
(5, 100.0, 100.0, 0.225): 0.822222222222
(6, 100.0, 100.0, 0.225): 0.340740740741
(7, 10.0, 10.0, 0.225): 0.762962962963
(8, 10.0, 10.0, 0.3): 0.666666666667
(9, 100.0, 100.0, 0.225): 0.681481481481
Loss on training set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 1.0, 0.225): 0.251851851852
(1, 100.0, 100.0, 0.225): 0.651851851852
(2, 100.0, 100.0, 0.225): 0.22962962963
(3, 100.0, 100.0, 0.2): 0.318518518519
(4, 100.0, 100.0, 0.225): 0.333333333333
(5, 100.0, 100.0, 0.225): 0.177777777778
(6, 100.0, 100.0, 0.225): 0.659259259259
(7, 10.0, 10.0, 0.225): 0.237037037037
(8, 10.0, 10.0, 0.3): 0.333333333333
(9, 100.0, 100.0, 0.225): 0.318518518519

Accuracy mean :0.6488888888888888
Std deviation :0.1599451209038907
Loss mean :0.3511111111111111
Std deviation :0.1599451209038907

On ALL the sets

Accuracy on all the set for every iteration, for every couple c,sigma (n_of_the_iter, c, sigma):
(0, 1.0, 1.0, 0.2): 0.333333333333
(0, 1.0, 1.0, 0.225): 0.866666666667
(0, 1.0, 1.0, 0.25): 0.6
(0, 1.0, 1.0, 0.275): 0.6
(0, 1.0, 1.0, 0.3): 0.666666666667
(0, 10.0, 10.0, 0.2): 0.333333333333
(0, 10.0, 10.0, 0.225): 0.8
(0, 10.0, 10.0, 0.25): 0.6
(0, 10.0, 10.0, 0.275): 0.6
(0, 10.0, 10.0, 0.3): 0.6
(0, 100.0, 100.0, 0.2): 0.333333333333
(0, 100.0, 100.0, 0.225): 0.866666666667
(0, 100.0, 100.0, 0.25): 0.6
(0, 100.0, 100.0, 0.275): 0.666666666667
(0, 100.0, 100.0, 0.3): 0.733333333333
(1, 0.1, 0.1, 0.275): 0.533333333333
(1, 0.1, 0.1, 0.3): 0.2
(1, 1.0, 1.0, 0.2): 0.133333333333
(1, 1.0, 1.0, 0.225): 0.733333333333
(1, 1.0, 1.0, 0.25): 0.2
(1, 1.0, 1.0, 0.275): 0.533333333333
(1, 1.0, 1.0, 0.3): 0.2
(1, 10.0, 10.0, 0.2): 0.0
(1, 10.0, 10.0, 0.225): 0.733333333333
(1, 10.0, 10.0, 0.25): 0.2
(1, 10.0, 10.0, 0.275): 0.533333333333
(1, 10.0, 10.0, 0.3): 0.2
(1, 100.0, 100.0, 0.2): 0.266666666667
(1, 100.0, 100.0, 0.225): 0.8
(1, 100.0, 100.0, 0.25): 0.2
(1, 100.0, 100.0, 0.275): 0.533333333333
(1, 100.0, 100.0, 0.3): 0.2
(2, 0.1, 0.1, 0.25): 0.533333333333
(2, 0.1, 0.1, 0.275): 0.666666666667
(2, 0.1, 0.1, 0.3): 0.533333333333
(2, 1.0, 1.0, 0.2): 0.533333333333
(2, 1.0, 1.0, 0.225): 0.533333333333
(2, 1.0, 1.0, 0.25): 0.533333333333
(2, 1.0, 1.0, 0.275): 0.666666666667
(2, 1.0, 1.0, 0.3): 0.533333333333
(2, 10.0, 10.0, 0.2): 0.2
(2, 10.0, 10.0, 0.225): 0.533333333333
(2, 10.0, 10.0, 0.25): 0.533333333333
(2, 10.0, 10.0, 0.275): 0.6
(2, 10.0, 10.0, 0.3): 0.533333333333
(2, 100.0, 100.0, 0.2): 0.2
(2, 100.0, 100.0, 0.225): 0.933333333333
(2, 100.0, 100.0, 0.25): 0.533333333333
(2, 100.0, 100.0, 0.275): 0.8
(2, 100.0, 100.0, 0.3): 0.533333333333
(3, 0.1, 0.1, 0.275): 0.866666666667
(3, 0.1, 0.1, 0.3): 0.6
(3, 1.0, 1.0, 0.2): 0.8
(3, 1.0, 1.0, 0.225): 0.6
(3, 1.0, 1.0, 0.25): 0.6
(3, 1.0, 1.0, 0.275): 0.866666666667
(3, 1.0, 1.0, 0.3): 0.6
(3, 10.0, 10.0, 0.2): 0.933333333333
(3, 10.0, 10.0, 0.225): 0.6
(3, 10.0, 10.0, 0.25): 0.6
(3, 10.0, 10.0, 0.275): 0.666666666667
(3, 10.0, 10.0, 0.3): 0.6
(3, 100.0, 100.0, 0.2): 0.933333333333
(3, 100.0, 100.0, 0.225): 0.6
(3, 100.0, 100.0, 0.25): 0.6
(3, 100.0, 100.0, 0.275): 0.8
(3, 100.0, 100.0, 0.3): 0.6
(4, 0.1, 0.1, 0.25): 0.666666666667
(4, 0.1, 0.1, 0.275): 0.666666666667
(4, 0.1, 0.1, 0.3): 0.666666666667
(4, 1.0, 1.0, 0.2): 0.666666666667
(4, 1.0, 1.0, 0.25): 0.666666666667
(4, 1.0, 1.0, 0.275): 0.666666666667
(4, 1.0, 1.0, 0.3): 0.666666666667
(4, 10.0, 10.0, 0.2): 0.666666666667
(4, 10.0, 10.0, 0.225): 0.866666666667
(4, 10.0, 10.0, 0.25): 0.666666666667
(4, 10.0, 10.0, 0.275): 0.666666666667
(4, 10.0, 10.0, 0.3): 0.666666666667
(4, 100.0, 100.0, 0.2): 0.666666666667
(4, 100.0, 100.0, 0.225): 0.866666666667
(4, 100.0, 100.0, 0.25): 0.666666666667
(4, 100.0, 100.0, 0.275): 0.666666666667
(4, 100.0, 100.0, 0.3): 0.666666666667
(5, 0.1, 0.1, 0.275): 0.733333333333
(5, 0.1, 0.1, 0.3): 0.733333333333
(5, 1.0, 1.0, 0.2): 0.733333333333
(5, 1.0, 1.0, 0.225): 0.466666666667
(5, 1.0, 1.0, 0.25): 0.733333333333
(5, 1.0, 1.0, 0.275): 0.733333333333
(5, 1.0, 1.0, 0.3): 0.733333333333
(5, 10.0, 10.0, 0.2): 0.733333333333
(5, 10.0, 10.0, 0.225): 0.466666666667
(5, 10.0, 10.0, 0.25): 0.466666666667
(5, 10.0, 10.0, 0.275): 0.733333333333
(5, 10.0, 10.0, 0.3): 0.733333333333
(5, 100.0, 100.0, 0.2): 0.733333333333
(5, 100.0, 100.0, 0.225): 0.866666666667
(5, 100.0, 100.0, 0.25): 0.8
(5, 100.0, 100.0, 0.275): 0.733333333333
(5, 100.0, 100.0, 0.3): 0.733333333333
(6, 0.1, 0.1, 0.25): 0.733333333333
(6, 0.1, 0.1, 0.3): 0.733333333333
(6, 1.0, 1.0, 0.2): 0.133333333333
(6, 1.0, 1.0, 0.225): 0.733333333333
(6, 1.0, 1.0, 0.25): 0.733333333333
(6, 1.0, 1.0, 0.275): 0.733333333333
(6, 1.0, 1.0, 0.3): 0.733333333333
(6, 10.0, 10.0, 0.2): 0.133333333333
(6, 10.0, 10.0, 0.225): 0.866666666667
(6, 10.0, 10.0, 0.25): 0.733333333333
(6, 10.0, 10.0, 0.275): 0.733333333333
(6, 10.0, 10.0, 0.3): 0.733333333333
(6, 100.0, 100.0, 0.225): 0.866666666667
(6, 100.0, 100.0, 0.25): 0.8
(6, 100.0, 100.0, 0.275): 0.733333333333
(6, 100.0, 100.0, 0.3): 0.733333333333
(7, 0.1, 0.1, 0.275): 0.6
(7, 0.1, 0.1, 0.3): 0.6
(7, 1.0, 1.0, 0.225): 0.8
(7, 1.0, 1.0, 0.25): 0.266666666667
(7, 1.0, 1.0, 0.275): 0.6
(7, 1.0, 1.0, 0.3): 0.6
(7, 10.0, 10.0, 0.225): 0.866666666667
(7, 10.0, 10.0, 0.25): 0.6
(7, 10.0, 10.0, 0.275): 0.6
(7, 10.0, 10.0, 0.3): 0.6
(7, 100.0, 100.0, 0.225): 0.8
(7, 100.0, 100.0, 0.25): 0.266666666667
(7, 100.0, 100.0, 0.275): 0.6
(7, 100.0, 100.0, 0.3): 0.6
(8, 0.1, 0.1, 0.25): 0.666666666667
(8, 0.1, 0.1, 0.3): 0.666666666667
(8, 1.0, 1.0, 0.2): 0.333333333333
(8, 1.0, 1.0, 0.225): 0.333333333333
(8, 1.0, 1.0, 0.25): 0.666666666667
(8, 1.0, 1.0, 0.275): 0.666666666667
(8, 1.0, 1.0, 0.3): 0.666666666667
(8, 10.0, 10.0, 0.2): 0.333333333333
(8, 10.0, 10.0, 0.25): 0.666666666667
(8, 10.0, 10.0, 0.275): 0.666666666667
(8, 10.0, 10.0, 0.3): 0.8
(8, 100.0, 100.0, 0.2): 0.466666666667
(8, 100.0, 100.0, 0.225): 0.666666666667
(8, 100.0, 100.0, 0.25): 0.666666666667
(8, 100.0, 100.0, 0.275): 0.333333333333
(8, 100.0, 100.0, 0.3): 0.666666666667
(9, 0.1, 0.1, 0.25): 0.666666666667
(9, 0.1, 0.1, 0.3): 0.666666666667
(9, 1.0, 1.0, 0.225): 0.733333333333
(9, 1.0, 1.0, 0.25): 0.666666666667
(9, 1.0, 1.0, 0.275): 0.6
(9, 1.0, 1.0, 0.3): 0.666666666667
(9, 10.0, 10.0, 0.225): 0.733333333333
(9, 10.0, 10.0, 0.25): 0.666666666667
(9, 10.0, 10.0, 0.275): 0.333333333333
(9, 10.0, 10.0, 0.3): 0.666666666667
(9, 100.0, 100.0, 0.225): 0.8
(9, 100.0, 100.0, 0.25): 0.666666666667
(9, 100.0, 100.0, 0.275): 0.6
(9, 100.0, 100.0, 0.3): 0.666666666667

Loss on all the set for every iteration, for every couple c,sigma (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 1.0, 0.2): 0.666666666667
(0, 1.0, 1.0, 0.225): 0.133333333333
(0, 1.0, 1.0, 0.25): 0.4
(0, 1.0, 1.0, 0.275): 0.4
(0, 1.0, 1.0, 0.3): 0.333333333333
(0, 10.0, 10.0, 0.2): 0.666666666667
(0, 10.0, 10.0, 0.225): 0.2
(0, 10.0, 10.0, 0.25): 0.4
(0, 10.0, 10.0, 0.275): 0.4
(0, 10.0, 10.0, 0.3): 0.4
(0, 100.0, 100.0, 0.2): 0.666666666667
(0, 100.0, 100.0, 0.225): 0.133333333333
(0, 100.0, 100.0, 0.25): 0.4
(0, 100.0, 100.0, 0.275): 0.333333333333
(0, 100.0, 100.0, 0.3): 0.266666666667
(1, 0.1, 0.1, 0.275): 0.466666666667
(1, 0.1, 0.1, 0.3): 0.8
(1, 1.0, 1.0, 0.2): 0.866666666667
(1, 1.0, 1.0, 0.225): 0.266666666667
(1, 1.0, 1.0, 0.25): 0.8
(1, 1.0, 1.0, 0.275): 0.466666666667
(1, 1.0, 1.0, 0.3): 0.8
(1, 10.0, 10.0, 0.2): 1.0
(1, 10.0, 10.0, 0.225): 0.266666666667
(1, 10.0, 10.0, 0.25): 0.8
(1, 10.0, 10.0, 0.275): 0.466666666667
(1, 10.0, 10.0, 0.3): 0.8
(1, 100.0, 100.0, 0.2): 0.733333333333
(1, 100.0, 100.0, 0.225): 0.2
(1, 100.0, 100.0, 0.25): 0.8
(1, 100.0, 100.0, 0.275): 0.466666666667
(1, 100.0, 100.0, 0.3): 0.8
(2, 0.1, 0.1, 0.25): 0.466666666667
(2, 0.1, 0.1, 0.275): 0.333333333333
(2, 0.1, 0.1, 0.3): 0.466666666667
(2, 1.0, 1.0, 0.2): 0.466666666667
(2, 1.0, 1.0, 0.225): 0.466666666667
(2, 1.0, 1.0, 0.25): 0.466666666667
(2, 1.0, 1.0, 0.275): 0.333333333333
(2, 1.0, 1.0, 0.3): 0.466666666667
(2, 10.0, 10.0, 0.2): 0.8
(2, 10.0, 10.0, 0.225): 0.466666666667
(2, 10.0, 10.0, 0.25): 0.466666666667
(2, 10.0, 10.0, 0.275): 0.4
(2, 10.0, 10.0, 0.3): 0.466666666667
(2, 100.0, 100.0, 0.2): 0.8
(2, 100.0, 100.0, 0.225): 0.0666666666667
(2, 100.0, 100.0, 0.25): 0.466666666667
(2, 100.0, 100.0, 0.275): 0.2
(2, 100.0, 100.0, 0.3): 0.466666666667
(3, 0.1, 0.1, 0.275): 0.133333333333
(3, 0.1, 0.1, 0.3): 0.4
(3, 1.0, 1.0, 0.2): 0.2
(3, 1.0, 1.0, 0.225): 0.4
(3, 1.0, 1.0, 0.25): 0.4
(3, 1.0, 1.0, 0.275): 0.133333333333
(3, 1.0, 1.0, 0.3): 0.4
(3, 10.0, 10.0, 0.2): 0.0666666666667
(3, 10.0, 10.0, 0.225): 0.4
(3, 10.0, 10.0, 0.25): 0.4
(3, 10.0, 10.0, 0.275): 0.333333333333
(3, 10.0, 10.0, 0.3): 0.4
(3, 100.0, 100.0, 0.2): 0.0666666666667
(3, 100.0, 100.0, 0.225): 0.4
(3, 100.0, 100.0, 0.25): 0.4
(3, 100.0, 100.0, 0.275): 0.2
(3, 100.0, 100.0, 0.3): 0.4
(4, 0.1, 0.1, 0.25): 0.333333333333
(4, 0.1, 0.1, 0.275): 0.333333333333
(4, 0.1, 0.1, 0.3): 0.333333333333
(4, 1.0, 1.0, 0.2): 0.333333333333
(4, 1.0, 1.0, 0.25): 0.333333333333
(4, 1.0, 1.0, 0.275): 0.333333333333
(4, 1.0, 1.0, 0.3): 0.333333333333
(4, 10.0, 10.0, 0.2): 0.333333333333
(4, 10.0, 10.0, 0.225): 0.133333333333
(4, 10.0, 10.0, 0.25): 0.333333333333
(4, 10.0, 10.0, 0.275): 0.333333333333
(4, 10.0, 10.0, 0.3): 0.333333333333
(4, 100.0, 100.0, 0.2): 0.333333333333
(4, 100.0, 100.0, 0.225): 0.133333333333
(4, 100.0, 100.0, 0.25): 0.333333333333
(4, 100.0, 100.0, 0.275): 0.333333333333
(4, 100.0, 100.0, 0.3): 0.333333333333
(5, 0.1, 0.1, 0.275): 0.266666666667
(5, 0.1, 0.1, 0.3): 0.266666666667
(5, 1.0, 1.0, 0.2): 0.266666666667
(5, 1.0, 1.0, 0.225): 0.533333333333
(5, 1.0, 1.0, 0.25): 0.266666666667
(5, 1.0, 1.0, 0.275): 0.266666666667
(5, 1.0, 1.0, 0.3): 0.266666666667
(5, 10.0, 10.0, 0.2): 0.266666666667
(5, 10.0, 10.0, 0.225): 0.533333333333
(5, 10.0, 10.0, 0.25): 0.533333333333
(5, 10.0, 10.0, 0.275): 0.266666666667
(5, 10.0, 10.0, 0.3): 0.266666666667
(5, 100.0, 100.0, 0.2): 0.266666666667
(5, 100.0, 100.0, 0.225): 0.133333333333
(5, 100.0, 100.0, 0.25): 0.2
(5, 100.0, 100.0, 0.275): 0.266666666667
(5, 100.0, 100.0, 0.3): 0.266666666667
(6, 0.1, 0.1, 0.25): 0.266666666667
(6, 0.1, 0.1, 0.3): 0.266666666667
(6, 1.0, 1.0, 0.2): 0.866666666667
(6, 1.0, 1.0, 0.225): 0.266666666667
(6, 1.0, 1.0, 0.25): 0.266666666667
(6, 1.0, 1.0, 0.275): 0.266666666667
(6, 1.0, 1.0, 0.3): 0.266666666667
(6, 10.0, 10.0, 0.2): 0.866666666667
(6, 10.0, 10.0, 0.225): 0.133333333333
(6, 10.0, 10.0, 0.25): 0.266666666667
(6, 10.0, 10.0, 0.275): 0.266666666667
(6, 10.0, 10.0, 0.3): 0.266666666667
(6, 100.0, 100.0, 0.225): 0.133333333333
(6, 100.0, 100.0, 0.25): 0.2
(6, 100.0, 100.0, 0.275): 0.266666666667
(6, 100.0, 100.0, 0.3): 0.266666666667
(7, 0.1, 0.1, 0.275): 0.4
(7, 0.1, 0.1, 0.3): 0.4
(7, 1.0, 1.0, 0.225): 0.2
(7, 1.0, 1.0, 0.25): 0.733333333333
(7, 1.0, 1.0, 0.275): 0.4
(7, 1.0, 1.0, 0.3): 0.4
(7, 10.0, 10.0, 0.225): 0.133333333333
(7, 10.0, 10.0, 0.25): 0.4
(7, 10.0, 10.0, 0.275): 0.4
(7, 10.0, 10.0, 0.3): 0.4
(7, 100.0, 100.0, 0.225): 0.2
(7, 100.0, 100.0, 0.25): 0.733333333333
(7, 100.0, 100.0, 0.275): 0.4
(7, 100.0, 100.0, 0.3): 0.4
(8, 0.1, 0.1, 0.25): 0.333333333333
(8, 0.1, 0.1, 0.3): 0.333333333333
(8, 1.0, 1.0, 0.2): 0.666666666667
(8, 1.0, 1.0, 0.225): 0.666666666667
(8, 1.0, 1.0, 0.25): 0.333333333333
(8, 1.0, 1.0, 0.275): 0.333333333333
(8, 1.0, 1.0, 0.3): 0.333333333333
(8, 10.0, 10.0, 0.2): 0.666666666667
(8, 10.0, 10.0, 0.25): 0.333333333333
(8, 10.0, 10.0, 0.275): 0.333333333333
(8, 10.0, 10.0, 0.3): 0.2
(8, 100.0, 100.0, 0.2): 0.533333333333
(8, 100.0, 100.0, 0.225): 0.333333333333
(8, 100.0, 100.0, 0.25): 0.333333333333
(8, 100.0, 100.0, 0.275): 0.666666666667
(8, 100.0, 100.0, 0.3): 0.333333333333
(9, 0.1, 0.1, 0.25): 0.333333333333
(9, 0.1, 0.1, 0.3): 0.333333333333
(9, 1.0, 1.0, 0.225): 0.266666666667
(9, 1.0, 1.0, 0.25): 0.333333333333
(9, 1.0, 1.0, 0.275): 0.4
(9, 1.0, 1.0, 0.3): 0.333333333333
(9, 10.0, 10.0, 0.225): 0.266666666667
(9, 10.0, 10.0, 0.25): 0.333333333333
(9, 10.0, 10.0, 0.275): 0.666666666667
(9, 10.0, 10.0, 0.3): 0.333333333333
(9, 100.0, 100.0, 0.225): 0.2
(9, 100.0, 100.0, 0.25): 0.333333333333
(9, 100.0, 100.0, 0.275): 0.4
(9, 100.0, 100.0, 0.3): 0.333333333333

Accuracy mean :0.6115942028985507
Std deviation :0.1868802707745387
Loss mean :0.3884057971014493
Std deviation :0.1868802707745387



QuantileConstPiecewise_LinearMu_2dim, number of iterations: 10

Parameters info
Tradeoffs parameter: [  0.1   1.   10.  100. ]
Using same parameters for all the clusterization: True
Gaussian kernel sigmas: [0.2   0.225 0.25  0.275 0.3  ]
Number of principal dimension considerated: 2
Dataset length: 150

Clusterization info
Minimum size of a cluster: in perc: 0.01, in int: 2
Type of mu: Linear Muzzifier
Fuzzifier: QuantileConstPiecewise
Cluster graph uses all connections: False
Cluster to label info
Force the number of cluster to be the number of different labels: False
Force the labels to be always represent by a cluster: False

Validation info
Conflict resolution: random
Number of cluster considerated for validation (top_k): None


RESULTS

On TEST set

Accuracy on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 1.0, 0.25): 0.533333333333
(1, 1.0, 1.0, 0.225): 0.666666666667
(2, 100.0, 100.0, 0.3): 0.6
(3, 10.0, 10.0, 0.225): 0.6
(4, 10.0, 10.0, 0.275): 0.6
(5, 10.0, 10.0, 0.2): 0.933333333333
(6, 100.0, 100.0, 0.25): 0.466666666667
(7, 1.0, 1.0, 0.3): 0.733333333333
(8, 1.0, 1.0, 0.275): 0.733333333333
(9, 1.0, 1.0, 0.2): 0.866666666667

Loss on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 1.0, 0.25): 0.466666666667
(1, 1.0, 1.0, 0.225): 0.333333333333
(2, 100.0, 100.0, 0.3): 0.4
(3, 10.0, 10.0, 0.225): 0.4
(4, 10.0, 10.0, 0.275): 0.4
(5, 10.0, 10.0, 0.2): 0.0666666666667
(6, 100.0, 100.0, 0.25): 0.533333333333
(7, 1.0, 1.0, 0.3): 0.266666666667
(8, 1.0, 1.0, 0.275): 0.266666666667
(9, 1.0, 1.0, 0.2): 0.133333333333

Accuracy mean :0.6733333333333333
Std deviation :0.13808210118138653
Loss mean :0.32666666666666666
Std deviation :0.13808210118138653

On TRAINING set

Accuracy on training set for every iteration (n_of_the_iter, c, sigma):
(0, 1.0, 1.0, 0.25): 0.681481481481
(1, 1.0, 1.0, 0.225): 0.822222222222
(2, 100.0, 100.0, 0.3): 0.562962962963
(3, 10.0, 10.0, 0.225): 0.837037037037
(4, 10.0, 10.0, 0.275): 0.651851851852
(5, 10.0, 10.0, 0.2): 0.837037037037
(6, 100.0, 100.0, 0.25): 0.681481481481
(7, 1.0, 1.0, 0.3): 0.711111111111
(8, 1.0, 1.0, 0.275): 0.681481481481
(9, 1.0, 1.0, 0.2): 0.762962962963
Loss on training set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 1.0, 0.25): 0.318518518519
(1, 1.0, 1.0, 0.225): 0.177777777778
(2, 100.0, 100.0, 0.3): 0.437037037037
(3, 10.0, 10.0, 0.225): 0.162962962963
(4, 10.0, 10.0, 0.275): 0.348148148148
(5, 10.0, 10.0, 0.2): 0.162962962963
(6, 100.0, 100.0, 0.25): 0.318518518519
(7, 1.0, 1.0, 0.3): 0.288888888889
(8, 1.0, 1.0, 0.275): 0.318518518519
(9, 1.0, 1.0, 0.2): 0.237037037037

Accuracy mean :0.7229629629629629
Std deviation :0.08569573252728632
Loss mean :0.277037037037037
Std deviation :0.08569573252728635

On ALL the sets

Accuracy on all the set for every iteration, for every couple c,sigma (n_of_the_iter, c, sigma):
(0, 1.0, 1.0, 0.2): 0.466666666667
(0, 1.0, 1.0, 0.225): 0.666666666667
(0, 1.0, 1.0, 0.25): 0.933333333333
(0, 1.0, 1.0, 0.275): 0.666666666667
(0, 1.0, 1.0, 0.3): 0.733333333333
(0, 10.0, 10.0, 0.2): 0.733333333333
(0, 10.0, 10.0, 0.225): 0.733333333333
(0, 10.0, 10.0, 0.25): 0.533333333333
(0, 10.0, 10.0, 0.275): 0.533333333333
(0, 10.0, 10.0, 0.3): 0.666666666667
(0, 100.0, 100.0, 0.2): 0.733333333333
(0, 100.0, 100.0, 0.225): 0.733333333333
(0, 100.0, 100.0, 0.25): 0.8
(0, 100.0, 100.0, 0.275): 0.6
(0, 100.0, 100.0, 0.3): 0.733333333333
(1, 1.0, 1.0, 0.2): 0.666666666667
(1, 1.0, 1.0, 0.225): 0.933333333333
(1, 1.0, 1.0, 0.25): 0.933333333333
(1, 1.0, 1.0, 0.275): 0.8
(1, 1.0, 1.0, 0.3): 0.733333333333
(1, 10.0, 10.0, 0.2): 0.733333333333
(1, 10.0, 10.0, 0.225): 0.933333333333
(1, 10.0, 10.0, 0.25): 0.666666666667
(1, 10.0, 10.0, 0.275): 0.866666666667
(1, 10.0, 10.0, 0.3): 0.2
(1, 100.0, 100.0, 0.2): 0.666666666667
(1, 100.0, 100.0, 0.225): 0.8
(1, 100.0, 100.0, 0.25): 0.933333333333
(1, 100.0, 100.0, 0.275): 0.866666666667
(1, 100.0, 100.0, 0.3): 0.733333333333
(2, 1.0, 1.0, 0.2): 0.466666666667
(2, 1.0, 1.0, 0.225): 0.666666666667
(2, 1.0, 1.0, 0.25): 0.866666666667
(2, 1.0, 1.0, 0.275): 0.733333333333
(2, 1.0, 1.0, 0.3): 0.8
(2, 10.0, 10.0, 0.2): 0.733333333333
(2, 10.0, 10.0, 0.225): 0.733333333333
(2, 10.0, 10.0, 0.25): 0.866666666667
(2, 10.0, 10.0, 0.275): 0.666666666667
(2, 10.0, 10.0, 0.3): 0.6
(2, 100.0, 100.0, 0.2): 0.6
(2, 100.0, 100.0, 0.225): 0.733333333333
(2, 100.0, 100.0, 0.25): 0.666666666667
(2, 100.0, 100.0, 0.275): 0.733333333333
(2, 100.0, 100.0, 0.3): 0.866666666667
(3, 0.1, 0.1, 0.25): 0.733333333333
(3, 0.1, 0.1, 0.275): 0.6
(3, 1.0, 1.0, 0.2): 0.466666666667
(3, 1.0, 1.0, 0.225): 0.6
(3, 1.0, 1.0, 0.25): 0.666666666667
(3, 1.0, 1.0, 0.275): 0.733333333333
(3, 1.0, 1.0, 0.3): 0.666666666667
(3, 10.0, 10.0, 0.2): 0.6
(3, 10.0, 10.0, 0.225): 0.8
(3, 10.0, 10.0, 0.25): 0.2
(3, 10.0, 10.0, 0.275): 0.666666666667
(3, 10.0, 10.0, 0.3): 0.4
(3, 100.0, 100.0, 0.2): 0.533333333333
(3, 100.0, 100.0, 0.225): 0.733333333333
(3, 100.0, 100.0, 0.25): 0.333333333333
(3, 100.0, 100.0, 0.275): 0.666666666667
(3, 100.0, 100.0, 0.3): 0.466666666667
(4, 1.0, 1.0, 0.2): 0.6
(4, 1.0, 1.0, 0.225): 0.733333333333
(4, 1.0, 1.0, 0.25): 0.8
(4, 1.0, 1.0, 0.275): 0.733333333333
(4, 1.0, 1.0, 0.3): 0.8
(4, 10.0, 10.0, 0.2): 0.866666666667
(4, 10.0, 10.0, 0.225): 0.733333333333
(4, 10.0, 10.0, 0.25): 0.666666666667
(4, 10.0, 10.0, 0.275): 0.866666666667
(4, 10.0, 10.0, 0.3): 0.6
(4, 100.0, 100.0, 0.2): 0.6
(4, 100.0, 100.0, 0.225): 0.8
(4, 100.0, 100.0, 0.25): 0.8
(4, 100.0, 100.0, 0.275): 0.666666666667
(4, 100.0, 100.0, 0.3): 0.866666666667
(5, 1.0, 1.0, 0.2): 0.8
(5, 1.0, 1.0, 0.225): 0.666666666667
(5, 1.0, 1.0, 0.25): 0.733333333333
(5, 1.0, 1.0, 0.3): 0.866666666667
(5, 10.0, 10.0, 0.2): 0.933333333333
(5, 10.0, 10.0, 0.225): 0.666666666667
(5, 10.0, 10.0, 0.25): 0.733333333333
(5, 10.0, 10.0, 0.275): 0.333333333333
(5, 10.0, 10.0, 0.3): 0.866666666667
(5, 100.0, 100.0, 0.2): 0.8
(5, 100.0, 100.0, 0.225): 0.866666666667
(5, 100.0, 100.0, 0.25): 0.666666666667
(5, 100.0, 100.0, 0.275): 0.333333333333
(5, 100.0, 100.0, 0.3): 0.8
(6, 1.0, 1.0, 0.2): 0.733333333333
(6, 1.0, 1.0, 0.225): 0.6
(6, 1.0, 1.0, 0.25): 0.733333333333
(6, 1.0, 1.0, 0.275): 0.666666666667
(6, 1.0, 1.0, 0.3): 0.333333333333
(6, 10.0, 10.0, 0.2): 0.6
(6, 10.0, 10.0, 0.225): 0.6
(6, 10.0, 10.0, 0.25): 0.666666666667
(6, 10.0, 10.0, 0.275): 0.533333333333
(6, 10.0, 10.0, 0.3): 0.6
(6, 100.0, 100.0, 0.2): 0.666666666667
(6, 100.0, 100.0, 0.225): 0.666666666667
(6, 100.0, 100.0, 0.25): 0.8
(6, 100.0, 100.0, 0.275): 0.666666666667
(6, 100.0, 100.0, 0.3): 0.6
(7, 0.1, 0.1, 0.225): 0.133333333333
(7, 1.0, 1.0, 0.2): 0.8
(7, 1.0, 1.0, 0.225): 0.133333333333
(7, 1.0, 1.0, 0.25): 0.466666666667
(7, 1.0, 1.0, 0.275): 0.733333333333
(7, 1.0, 1.0, 0.3): 0.933333333333
(7, 10.0, 10.0, 0.2): 0.666666666667
(7, 10.0, 10.0, 0.225): 0.133333333333
(7, 10.0, 10.0, 0.25): 0.6
(7, 10.0, 10.0, 0.275): 0.866666666667
(7, 10.0, 10.0, 0.3): 0.666666666667
(7, 100.0, 100.0, 0.2): 0.733333333333
(7, 100.0, 100.0, 0.225): 0.133333333333
(7, 100.0, 100.0, 0.25): 0.266666666667
(7, 100.0, 100.0, 0.275): 0.866666666667
(7, 100.0, 100.0, 0.3): 0.666666666667
(8, 1.0, 1.0, 0.2): 0.6
(8, 1.0, 1.0, 0.225): 0.866666666667
(8, 1.0, 1.0, 0.25): 0.733333333333
(8, 1.0, 1.0, 0.275): 0.933333333333
(8, 1.0, 1.0, 0.3): 0.533333333333
(8, 10.0, 10.0, 0.2): 0.666666666667
(8, 10.0, 10.0, 0.225): 0.8
(8, 10.0, 10.0, 0.25): 0.733333333333
(8, 10.0, 10.0, 0.275): 0.666666666667
(8, 10.0, 10.0, 0.3): 0.533333333333
(8, 100.0, 100.0, 0.2): 0.466666666667
(8, 100.0, 100.0, 0.225): 0.6
(8, 100.0, 100.0, 0.25): 0.8
(8, 100.0, 100.0, 0.275): 0.6
(8, 100.0, 100.0, 0.3): 0.533333333333
(9, 0.1, 0.1, 0.25): 0.733333333333
(9, 1.0, 1.0, 0.2): 0.933333333333
(9, 1.0, 1.0, 0.225): 0.8
(9, 1.0, 1.0, 0.25): 0.6
(9, 1.0, 1.0, 0.275): 0.266666666667
(9, 1.0, 1.0, 0.3): 0.733333333333
(9, 10.0, 10.0, 0.2): 0.733333333333
(9, 10.0, 10.0, 0.225): 0.666666666667
(9, 10.0, 10.0, 0.25): 0.8
(9, 10.0, 10.0, 0.275): 0.866666666667
(9, 10.0, 10.0, 0.3): 0.666666666667
(9, 100.0, 100.0, 0.2): 0.733333333333
(9, 100.0, 100.0, 0.225): 0.733333333333
(9, 100.0, 100.0, 0.25): 0.8
(9, 100.0, 100.0, 0.275): 0.6
(9, 100.0, 100.0, 0.3): 0.8

Loss on all the set for every iteration, for every couple c,sigma (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 1.0, 0.2): 0.533333333333
(0, 1.0, 1.0, 0.225): 0.333333333333
(0, 1.0, 1.0, 0.25): 0.0666666666667
(0, 1.0, 1.0, 0.275): 0.333333333333
(0, 1.0, 1.0, 0.3): 0.266666666667
(0, 10.0, 10.0, 0.2): 0.266666666667
(0, 10.0, 10.0, 0.225): 0.266666666667
(0, 10.0, 10.0, 0.25): 0.466666666667
(0, 10.0, 10.0, 0.275): 0.466666666667
(0, 10.0, 10.0, 0.3): 0.333333333333
(0, 100.0, 100.0, 0.2): 0.266666666667
(0, 100.0, 100.0, 0.225): 0.266666666667
(0, 100.0, 100.0, 0.25): 0.2
(0, 100.0, 100.0, 0.275): 0.4
(0, 100.0, 100.0, 0.3): 0.266666666667
(1, 1.0, 1.0, 0.2): 0.333333333333
(1, 1.0, 1.0, 0.225): 0.0666666666667
(1, 1.0, 1.0, 0.25): 0.0666666666667
(1, 1.0, 1.0, 0.275): 0.2
(1, 1.0, 1.0, 0.3): 0.266666666667
(1, 10.0, 10.0, 0.2): 0.266666666667
(1, 10.0, 10.0, 0.225): 0.0666666666667
(1, 10.0, 10.0, 0.25): 0.333333333333
(1, 10.0, 10.0, 0.275): 0.133333333333
(1, 10.0, 10.0, 0.3): 0.8
(1, 100.0, 100.0, 0.2): 0.333333333333
(1, 100.0, 100.0, 0.225): 0.2
(1, 100.0, 100.0, 0.25): 0.0666666666667
(1, 100.0, 100.0, 0.275): 0.133333333333
(1, 100.0, 100.0, 0.3): 0.266666666667
(2, 1.0, 1.0, 0.2): 0.533333333333
(2, 1.0, 1.0, 0.225): 0.333333333333
(2, 1.0, 1.0, 0.25): 0.133333333333
(2, 1.0, 1.0, 0.275): 0.266666666667
(2, 1.0, 1.0, 0.3): 0.2
(2, 10.0, 10.0, 0.2): 0.266666666667
(2, 10.0, 10.0, 0.225): 0.266666666667
(2, 10.0, 10.0, 0.25): 0.133333333333
(2, 10.0, 10.0, 0.275): 0.333333333333
(2, 10.0, 10.0, 0.3): 0.4
(2, 100.0, 100.0, 0.2): 0.4
(2, 100.0, 100.0, 0.225): 0.266666666667
(2, 100.0, 100.0, 0.25): 0.333333333333
(2, 100.0, 100.0, 0.275): 0.266666666667
(2, 100.0, 100.0, 0.3): 0.133333333333
(3, 0.1, 0.1, 0.25): 0.266666666667
(3, 0.1, 0.1, 0.275): 0.4
(3, 1.0, 1.0, 0.2): 0.533333333333
(3, 1.0, 1.0, 0.225): 0.4
(3, 1.0, 1.0, 0.25): 0.333333333333
(3, 1.0, 1.0, 0.275): 0.266666666667
(3, 1.0, 1.0, 0.3): 0.333333333333
(3, 10.0, 10.0, 0.2): 0.4
(3, 10.0, 10.0, 0.225): 0.2
(3, 10.0, 10.0, 0.25): 0.8
(3, 10.0, 10.0, 0.275): 0.333333333333
(3, 10.0, 10.0, 0.3): 0.6
(3, 100.0, 100.0, 0.2): 0.466666666667
(3, 100.0, 100.0, 0.225): 0.266666666667
(3, 100.0, 100.0, 0.25): 0.666666666667
(3, 100.0, 100.0, 0.275): 0.333333333333
(3, 100.0, 100.0, 0.3): 0.533333333333
(4, 1.0, 1.0, 0.2): 0.4
(4, 1.0, 1.0, 0.225): 0.266666666667
(4, 1.0, 1.0, 0.25): 0.2
(4, 1.0, 1.0, 0.275): 0.266666666667
(4, 1.0, 1.0, 0.3): 0.2
(4, 10.0, 10.0, 0.2): 0.133333333333
(4, 10.0, 10.0, 0.225): 0.266666666667
(4, 10.0, 10.0, 0.25): 0.333333333333
(4, 10.0, 10.0, 0.275): 0.133333333333
(4, 10.0, 10.0, 0.3): 0.4
(4, 100.0, 100.0, 0.2): 0.4
(4, 100.0, 100.0, 0.225): 0.2
(4, 100.0, 100.0, 0.25): 0.2
(4, 100.0, 100.0, 0.275): 0.333333333333
(4, 100.0, 100.0, 0.3): 0.133333333333
(5, 1.0, 1.0, 0.2): 0.2
(5, 1.0, 1.0, 0.225): 0.333333333333
(5, 1.0, 1.0, 0.25): 0.266666666667
(5, 1.0, 1.0, 0.3): 0.133333333333
(5, 10.0, 10.0, 0.2): 0.0666666666667
(5, 10.0, 10.0, 0.225): 0.333333333333
(5, 10.0, 10.0, 0.25): 0.266666666667
(5, 10.0, 10.0, 0.275): 0.666666666667
(5, 10.0, 10.0, 0.3): 0.133333333333
(5, 100.0, 100.0, 0.2): 0.2
(5, 100.0, 100.0, 0.225): 0.133333333333
(5, 100.0, 100.0, 0.25): 0.333333333333
(5, 100.0, 100.0, 0.275): 0.666666666667
(5, 100.0, 100.0, 0.3): 0.2
(6, 1.0, 1.0, 0.2): 0.266666666667
(6, 1.0, 1.0, 0.225): 0.4
(6, 1.0, 1.0, 0.25): 0.266666666667
(6, 1.0, 1.0, 0.275): 0.333333333333
(6, 1.0, 1.0, 0.3): 0.666666666667
(6, 10.0, 10.0, 0.2): 0.4
(6, 10.0, 10.0, 0.225): 0.4
(6, 10.0, 10.0, 0.25): 0.333333333333
(6, 10.0, 10.0, 0.275): 0.466666666667
(6, 10.0, 10.0, 0.3): 0.4
(6, 100.0, 100.0, 0.2): 0.333333333333
(6, 100.0, 100.0, 0.225): 0.333333333333
(6, 100.0, 100.0, 0.25): 0.2
(6, 100.0, 100.0, 0.275): 0.333333333333
(6, 100.0, 100.0, 0.3): 0.4
(7, 0.1, 0.1, 0.225): 0.866666666667
(7, 1.0, 1.0, 0.2): 0.2
(7, 1.0, 1.0, 0.225): 0.866666666667
(7, 1.0, 1.0, 0.25): 0.533333333333
(7, 1.0, 1.0, 0.275): 0.266666666667
(7, 1.0, 1.0, 0.3): 0.0666666666667
(7, 10.0, 10.0, 0.2): 0.333333333333
(7, 10.0, 10.0, 0.225): 0.866666666667
(7, 10.0, 10.0, 0.25): 0.4
(7, 10.0, 10.0, 0.275): 0.133333333333
(7, 10.0, 10.0, 0.3): 0.333333333333
(7, 100.0, 100.0, 0.2): 0.266666666667
(7, 100.0, 100.0, 0.225): 0.866666666667
(7, 100.0, 100.0, 0.25): 0.733333333333
(7, 100.0, 100.0, 0.275): 0.133333333333
(7, 100.0, 100.0, 0.3): 0.333333333333
(8, 1.0, 1.0, 0.2): 0.4
(8, 1.0, 1.0, 0.225): 0.133333333333
(8, 1.0, 1.0, 0.25): 0.266666666667
(8, 1.0, 1.0, 0.275): 0.0666666666667
(8, 1.0, 1.0, 0.3): 0.466666666667
(8, 10.0, 10.0, 0.2): 0.333333333333
(8, 10.0, 10.0, 0.225): 0.2
(8, 10.0, 10.0, 0.25): 0.266666666667
(8, 10.0, 10.0, 0.275): 0.333333333333
(8, 10.0, 10.0, 0.3): 0.466666666667
(8, 100.0, 100.0, 0.2): 0.533333333333
(8, 100.0, 100.0, 0.225): 0.4
(8, 100.0, 100.0, 0.25): 0.2
(8, 100.0, 100.0, 0.275): 0.4
(8, 100.0, 100.0, 0.3): 0.466666666667
(9, 0.1, 0.1, 0.25): 0.266666666667
(9, 1.0, 1.0, 0.2): 0.0666666666667
(9, 1.0, 1.0, 0.225): 0.2
(9, 1.0, 1.0, 0.25): 0.4
(9, 1.0, 1.0, 0.275): 0.733333333333
(9, 1.0, 1.0, 0.3): 0.266666666667
(9, 10.0, 10.0, 0.2): 0.266666666667
(9, 10.0, 10.0, 0.225): 0.333333333333
(9, 10.0, 10.0, 0.25): 0.2
(9, 10.0, 10.0, 0.275): 0.133333333333
(9, 10.0, 10.0, 0.3): 0.333333333333
(9, 100.0, 100.0, 0.2): 0.266666666667
(9, 100.0, 100.0, 0.225): 0.266666666667
(9, 100.0, 100.0, 0.25): 0.2
(9, 100.0, 100.0, 0.275): 0.4
(9, 100.0, 100.0, 0.3): 0.2

Accuracy mean :0.6753812636165577
Std deviation :0.17325837334346866
Loss mean :0.32461873638344224
Std deviation :0.17325837334346864



QuantileConstPiecewise_LinearMu_3dim, number of iterations: 10

Parameters info
Tradeoffs parameter: [  0.1   1.   10.  100. ]
Using same parameters for all the clusterization: True
Gaussian kernel sigmas: [0.2   0.225 0.25  0.275 0.3  ]
Number of principal dimension considerated: 3
Dataset length: 150

Clusterization info
Minimum size of a cluster: in perc: 0.01, in int: 2
Type of mu: Linear Muzzifier
Fuzzifier: QuantileConstPiecewise
Cluster graph uses all connections: False
Cluster to label info
Force the number of cluster to be the number of different labels: False
Force the labels to be always represent by a cluster: False

Validation info
Conflict resolution: random
Number of cluster considerated for validation (top_k): None


RESULTS

On TEST set

Accuracy on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 10.0, 10.0, 0.3): 0.533333333333
(1, 1.0, 1.0, 0.25): 0.0666666666667
(2, 10.0, 10.0, 0.3): 0.6
(3, 100.0, 100.0, 0.3): 0.333333333333
(4, 10.0, 10.0, 0.3): 0.933333333333
(5, 100.0, 100.0, 0.275): 0.533333333333
(6, 1.0, 1.0, 0.25): 0.533333333333
(7, 100.0, 100.0, 0.25): 0.466666666667
(8, 10.0, 10.0, 0.3): 0.533333333333
(9, 100.0, 100.0, 0.3): 0.8

Loss on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 10.0, 10.0, 0.3): 0.466666666667
(1, 1.0, 1.0, 0.25): 0.933333333333
(2, 10.0, 10.0, 0.3): 0.4
(3, 100.0, 100.0, 0.3): 0.666666666667
(4, 10.0, 10.0, 0.3): 0.0666666666667
(5, 100.0, 100.0, 0.275): 0.466666666667
(6, 1.0, 1.0, 0.25): 0.466666666667
(7, 100.0, 100.0, 0.25): 0.533333333333
(8, 10.0, 10.0, 0.3): 0.466666666667
(9, 100.0, 100.0, 0.3): 0.2

Accuracy mean :0.5333333333333334
Std deviation :0.2231093404090868
Loss mean :0.4666666666666667
Std deviation :0.2231093404090868

On TRAINING set

Accuracy on training set for every iteration (n_of_the_iter, c, sigma):
(0, 10.0, 10.0, 0.3): 0.637037037037
(1, 1.0, 1.0, 0.25): 0.362962962963
(2, 10.0, 10.0, 0.3): 0.57037037037
(3, 100.0, 100.0, 0.3): 0.644444444444
(4, 10.0, 10.0, 0.3): 0.703703703704
(5, 100.0, 100.0, 0.275): 0.644444444444
(6, 1.0, 1.0, 0.25): 0.62962962963
(7, 100.0, 100.0, 0.25): 0.644444444444
(8, 10.0, 10.0, 0.3): 0.711111111111
(9, 100.0, 100.0, 0.3): 0.644444444444
Loss on training set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 10.0, 10.0, 0.3): 0.362962962963
(1, 1.0, 1.0, 0.25): 0.637037037037
(2, 10.0, 10.0, 0.3): 0.42962962963
(3, 100.0, 100.0, 0.3): 0.355555555556
(4, 10.0, 10.0, 0.3): 0.296296296296
(5, 100.0, 100.0, 0.275): 0.355555555556
(6, 1.0, 1.0, 0.25): 0.37037037037
(7, 100.0, 100.0, 0.25): 0.355555555556
(8, 10.0, 10.0, 0.3): 0.288888888889
(9, 100.0, 100.0, 0.3): 0.355555555556

Accuracy mean :0.6192592592592593
Std deviation :0.09306251125652983
Loss mean :0.3807407407407407
Std deviation :0.09306251125652984

On ALL the sets

Accuracy on all the set for every iteration, for every couple c,sigma (n_of_the_iter, c, sigma):
(0, 1.0, 1.0, 0.2): 0.2
(0, 1.0, 1.0, 0.225): 0.4
(0, 1.0, 1.0, 0.25): 0.266666666667
(0, 1.0, 1.0, 0.275): 0.6
(0, 1.0, 1.0, 0.3): 0.6
(0, 10.0, 10.0, 0.2): 0.2
(0, 10.0, 10.0, 0.225): 0.4
(0, 10.0, 10.0, 0.25): 0.533333333333
(0, 10.0, 10.0, 0.275): 0.6
(0, 10.0, 10.0, 0.3): 0.666666666667
(0, 100.0, 100.0, 0.2): 0.266666666667
(0, 100.0, 100.0, 0.225): 0.4
(0, 100.0, 100.0, 0.25): 0.333333333333
(0, 100.0, 100.0, 0.275): 0.6
(0, 100.0, 100.0, 0.3): 0.4
(1, 0.1, 0.1, 0.3): 0.666666666667
(1, 1.0, 1.0, 0.2): 0.266666666667
(1, 1.0, 1.0, 0.225): 0.0666666666667
(1, 1.0, 1.0, 0.25): 0.733333333333
(1, 1.0, 1.0, 0.275): 0.0666666666667
(1, 1.0, 1.0, 0.3): 0.6
(1, 10.0, 10.0, 0.2): 0.266666666667
(1, 10.0, 10.0, 0.225): 0.2
(1, 10.0, 10.0, 0.25): 0.333333333333
(1, 10.0, 10.0, 0.275): 0.0666666666667
(1, 10.0, 10.0, 0.3): 0.466666666667
(1, 100.0, 100.0, 0.2): 0.266666666667
(1, 100.0, 100.0, 0.225): 0.4
(1, 100.0, 100.0, 0.25): 0.266666666667
(1, 100.0, 100.0, 0.275): 0.0666666666667
(1, 100.0, 100.0, 0.3): 0.466666666667
(2, 0.1, 0.1, 0.3): 0.8
(2, 1.0, 1.0, 0.2): 0.4
(2, 1.0, 1.0, 0.225): 0.266666666667
(2, 1.0, 1.0, 0.25): 0.533333333333
(2, 1.0, 1.0, 0.275): 0.266666666667
(2, 1.0, 1.0, 0.3): 0.733333333333
(2, 10.0, 10.0, 0.2): 0.2
(2, 10.0, 10.0, 0.225): 0.466666666667
(2, 10.0, 10.0, 0.25): 0.466666666667
(2, 10.0, 10.0, 0.275): 0.666666666667
(2, 10.0, 10.0, 0.3): 0.8
(2, 100.0, 100.0, 0.2): 0.2
(2, 100.0, 100.0, 0.225): 0.333333333333
(2, 100.0, 100.0, 0.25): 0.666666666667
(2, 100.0, 100.0, 0.275): 0.666666666667
(2, 100.0, 100.0, 0.3): 0.666666666667
(3, 1.0, 1.0, 0.2): 0.533333333333
(3, 1.0, 1.0, 0.225): 0.466666666667
(3, 1.0, 1.0, 0.25): 0.333333333333
(3, 1.0, 1.0, 0.275): 0.333333333333
(3, 1.0, 1.0, 0.3): 0.466666666667
(3, 10.0, 10.0, 0.2): 0.4
(3, 10.0, 10.0, 0.225): 0.333333333333
(3, 10.0, 10.0, 0.25): 0.333333333333
(3, 10.0, 10.0, 0.275): 0.533333333333
(3, 10.0, 10.0, 0.3): 0.6
(3, 100.0, 100.0, 0.2): 0.2
(3, 100.0, 100.0, 0.225): 0.4
(3, 100.0, 100.0, 0.25): 0.4
(3, 100.0, 100.0, 0.275): 0.6
(3, 100.0, 100.0, 0.3): 0.666666666667
(4, 0.1, 0.1, 0.275): 0.6
(4, 1.0, 1.0, 0.2): 0.266666666667
(4, 1.0, 1.0, 0.225): 0.4
(4, 1.0, 1.0, 0.25): 0.466666666667
(4, 1.0, 1.0, 0.275): 0.4
(4, 1.0, 1.0, 0.3): 0.466666666667
(4, 10.0, 10.0, 0.2): 0.466666666667
(4, 10.0, 10.0, 0.225): 0.466666666667
(4, 10.0, 10.0, 0.25): 0.266666666667
(4, 10.0, 10.0, 0.275): 0.6
(4, 10.0, 10.0, 0.3): 0.666666666667
(4, 100.0, 100.0, 0.2): 0.2
(4, 100.0, 100.0, 0.225): 0.333333333333
(4, 100.0, 100.0, 0.25): 0.333333333333
(4, 100.0, 100.0, 0.275): 0.6
(4, 100.0, 100.0, 0.3): 0.466666666667
(5, 1.0, 1.0, 0.2): 0.266666666667
(5, 1.0, 1.0, 0.225): 0.266666666667
(5, 1.0, 1.0, 0.25): 0.333333333333
(5, 1.0, 1.0, 0.275): 0.533333333333
(5, 1.0, 1.0, 0.3): 0.6
(5, 10.0, 10.0, 0.2): 0.333333333333
(5, 10.0, 10.0, 0.225): 0.333333333333
(5, 10.0, 10.0, 0.25): 0.466666666667
(5, 10.0, 10.0, 0.275): 0.533333333333
(5, 10.0, 10.0, 0.3): 0.133333333333
(5, 100.0, 100.0, 0.2): 0.2
(5, 100.0, 100.0, 0.225): 0.133333333333
(5, 100.0, 100.0, 0.25): 0.6
(5, 100.0, 100.0, 0.275): 0.666666666667
(5, 100.0, 100.0, 0.3): 0.333333333333
(6, 0.1, 0.1, 0.225): 0.2
(6, 0.1, 0.1, 0.275): 0.533333333333
(6, 1.0, 1.0, 0.2): 0.2
(6, 1.0, 1.0, 0.225): 0.4
(6, 1.0, 1.0, 0.25): 0.733333333333
(6, 1.0, 1.0, 0.275): 0.4
(6, 1.0, 1.0, 0.3): 0.333333333333
(6, 10.0, 10.0, 0.2): 0.266666666667
(6, 10.0, 10.0, 0.225): 0.266666666667
(6, 10.0, 10.0, 0.25): 0.466666666667
(6, 10.0, 10.0, 0.275): 0.6
(6, 10.0, 10.0, 0.3): 0.6
(6, 100.0, 100.0, 0.2): 0.2
(6, 100.0, 100.0, 0.225): 0.2
(6, 100.0, 100.0, 0.25): 0.333333333333
(6, 100.0, 100.0, 0.275): 0.6
(6, 100.0, 100.0, 0.3): 0.6
(7, 0.1, 0.1, 0.275): 0.6
(7, 1.0, 1.0, 0.2): 0.466666666667
(7, 1.0, 1.0, 0.225): 0.466666666667
(7, 1.0, 1.0, 0.25): 0.6
(7, 1.0, 1.0, 0.275): 0.6
(7, 1.0, 1.0, 0.3): 0.333333333333
(7, 10.0, 10.0, 0.2): 0.466666666667
(7, 10.0, 10.0, 0.225): 0.4
(7, 10.0, 10.0, 0.25): 0.533333333333
(7, 10.0, 10.0, 0.275): 0.533333333333
(7, 10.0, 10.0, 0.3): 0.533333333333
(7, 100.0, 100.0, 0.2): 0.533333333333
(7, 100.0, 100.0, 0.225): 0.333333333333
(7, 100.0, 100.0, 0.25): 0.666666666667
(7, 100.0, 100.0, 0.275): 0.466666666667
(7, 100.0, 100.0, 0.3): 0.666666666667
(8, 1.0, 1.0, 0.2): 0.266666666667
(8, 1.0, 1.0, 0.225): 0.2
(8, 1.0, 1.0, 0.25): 0.266666666667
(8, 1.0, 1.0, 0.275): 0.266666666667
(8, 1.0, 1.0, 0.3): 0.533333333333
(8, 10.0, 10.0, 0.2): 0.266666666667
(8, 10.0, 10.0, 0.225): 0.2
(8, 10.0, 10.0, 0.25): 0.4
(8, 10.0, 10.0, 0.275): 0.4
(8, 10.0, 10.0, 0.3): 0.666666666667
(8, 100.0, 100.0, 0.2): 0.0666666666667
(8, 100.0, 100.0, 0.225): 0.2
(8, 100.0, 100.0, 0.25): 0.466666666667
(8, 100.0, 100.0, 0.275): 0.4
(8, 100.0, 100.0, 0.3): 0.533333333333
(9, 1.0, 1.0, 0.2): 0.466666666667
(9, 1.0, 1.0, 0.25): 0.2
(9, 1.0, 1.0, 0.275): 0.4
(9, 1.0, 1.0, 0.3): 0.666666666667
(9, 10.0, 10.0, 0.2): 0.466666666667
(9, 10.0, 10.0, 0.25): 0.2
(9, 10.0, 10.0, 0.275): 0.4
(9, 10.0, 10.0, 0.3): 0.533333333333
(9, 100.0, 100.0, 0.2): 0.666666666667
(9, 100.0, 100.0, 0.225): 0.333333333333
(9, 100.0, 100.0, 0.25): 0.2
(9, 100.0, 100.0, 0.275): 0.266666666667
(9, 100.0, 100.0, 0.3): 0.666666666667

Loss on all the set for every iteration, for every couple c,sigma (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 1.0, 0.2): 0.8
(0, 1.0, 1.0, 0.225): 0.6
(0, 1.0, 1.0, 0.25): 0.733333333333
(0, 1.0, 1.0, 0.275): 0.4
(0, 1.0, 1.0, 0.3): 0.4
(0, 10.0, 10.0, 0.2): 0.8
(0, 10.0, 10.0, 0.225): 0.6
(0, 10.0, 10.0, 0.25): 0.466666666667
(0, 10.0, 10.0, 0.275): 0.4
(0, 10.0, 10.0, 0.3): 0.333333333333
(0, 100.0, 100.0, 0.2): 0.733333333333
(0, 100.0, 100.0, 0.225): 0.6
(0, 100.0, 100.0, 0.25): 0.666666666667
(0, 100.0, 100.0, 0.275): 0.4
(0, 100.0, 100.0, 0.3): 0.6
(1, 0.1, 0.1, 0.3): 0.333333333333
(1, 1.0, 1.0, 0.2): 0.733333333333
(1, 1.0, 1.0, 0.225): 0.933333333333
(1, 1.0, 1.0, 0.25): 0.266666666667
(1, 1.0, 1.0, 0.275): 0.933333333333
(1, 1.0, 1.0, 0.3): 0.4
(1, 10.0, 10.0, 0.2): 0.733333333333
(1, 10.0, 10.0, 0.225): 0.8
(1, 10.0, 10.0, 0.25): 0.666666666667
(1, 10.0, 10.0, 0.275): 0.933333333333
(1, 10.0, 10.0, 0.3): 0.533333333333
(1, 100.0, 100.0, 0.2): 0.733333333333
(1, 100.0, 100.0, 0.225): 0.6
(1, 100.0, 100.0, 0.25): 0.733333333333
(1, 100.0, 100.0, 0.275): 0.933333333333
(1, 100.0, 100.0, 0.3): 0.533333333333
(2, 0.1, 0.1, 0.3): 0.2
(2, 1.0, 1.0, 0.2): 0.6
(2, 1.0, 1.0, 0.225): 0.733333333333
(2, 1.0, 1.0, 0.25): 0.466666666667
(2, 1.0, 1.0, 0.275): 0.733333333333
(2, 1.0, 1.0, 0.3): 0.266666666667
(2, 10.0, 10.0, 0.2): 0.8
(2, 10.0, 10.0, 0.225): 0.533333333333
(2, 10.0, 10.0, 0.25): 0.533333333333
(2, 10.0, 10.0, 0.275): 0.333333333333
(2, 10.0, 10.0, 0.3): 0.2
(2, 100.0, 100.0, 0.2): 0.8
(2, 100.0, 100.0, 0.225): 0.666666666667
(2, 100.0, 100.0, 0.25): 0.333333333333
(2, 100.0, 100.0, 0.275): 0.333333333333
(2, 100.0, 100.0, 0.3): 0.333333333333
(3, 1.0, 1.0, 0.2): 0.466666666667
(3, 1.0, 1.0, 0.225): 0.533333333333
(3, 1.0, 1.0, 0.25): 0.666666666667
(3, 1.0, 1.0, 0.275): 0.666666666667
(3, 1.0, 1.0, 0.3): 0.533333333333
(3, 10.0, 10.0, 0.2): 0.6
(3, 10.0, 10.0, 0.225): 0.666666666667
(3, 10.0, 10.0, 0.25): 0.666666666667
(3, 10.0, 10.0, 0.275): 0.466666666667
(3, 10.0, 10.0, 0.3): 0.4
(3, 100.0, 100.0, 0.2): 0.8
(3, 100.0, 100.0, 0.225): 0.6
(3, 100.0, 100.0, 0.25): 0.6
(3, 100.0, 100.0, 0.275): 0.4
(3, 100.0, 100.0, 0.3): 0.333333333333
(4, 0.1, 0.1, 0.275): 0.4
(4, 1.0, 1.0, 0.2): 0.733333333333
(4, 1.0, 1.0, 0.225): 0.6
(4, 1.0, 1.0, 0.25): 0.533333333333
(4, 1.0, 1.0, 0.275): 0.6
(4, 1.0, 1.0, 0.3): 0.533333333333
(4, 10.0, 10.0, 0.2): 0.533333333333
(4, 10.0, 10.0, 0.225): 0.533333333333
(4, 10.0, 10.0, 0.25): 0.733333333333
(4, 10.0, 10.0, 0.275): 0.4
(4, 10.0, 10.0, 0.3): 0.333333333333
(4, 100.0, 100.0, 0.2): 0.8
(4, 100.0, 100.0, 0.225): 0.666666666667
(4, 100.0, 100.0, 0.25): 0.666666666667
(4, 100.0, 100.0, 0.275): 0.4
(4, 100.0, 100.0, 0.3): 0.533333333333
(5, 1.0, 1.0, 0.2): 0.733333333333
(5, 1.0, 1.0, 0.225): 0.733333333333
(5, 1.0, 1.0, 0.25): 0.666666666667
(5, 1.0, 1.0, 0.275): 0.466666666667
(5, 1.0, 1.0, 0.3): 0.4
(5, 10.0, 10.0, 0.2): 0.666666666667
(5, 10.0, 10.0, 0.225): 0.666666666667
(5, 10.0, 10.0, 0.25): 0.533333333333
(5, 10.0, 10.0, 0.275): 0.466666666667
(5, 10.0, 10.0, 0.3): 0.866666666667
(5, 100.0, 100.0, 0.2): 0.8
(5, 100.0, 100.0, 0.225): 0.866666666667
(5, 100.0, 100.0, 0.25): 0.4
(5, 100.0, 100.0, 0.275): 0.333333333333
(5, 100.0, 100.0, 0.3): 0.666666666667
(6, 0.1, 0.1, 0.225): 0.8
(6, 0.1, 0.1, 0.275): 0.466666666667
(6, 1.0, 1.0, 0.2): 0.8
(6, 1.0, 1.0, 0.225): 0.6
(6, 1.0, 1.0, 0.25): 0.266666666667
(6, 1.0, 1.0, 0.275): 0.6
(6, 1.0, 1.0, 0.3): 0.666666666667
(6, 10.0, 10.0, 0.2): 0.733333333333
(6, 10.0, 10.0, 0.225): 0.733333333333
(6, 10.0, 10.0, 0.25): 0.533333333333
(6, 10.0, 10.0, 0.275): 0.4
(6, 10.0, 10.0, 0.3): 0.4
(6, 100.0, 100.0, 0.2): 0.8
(6, 100.0, 100.0, 0.225): 0.8
(6, 100.0, 100.0, 0.25): 0.666666666667
(6, 100.0, 100.0, 0.275): 0.4
(6, 100.0, 100.0, 0.3): 0.4
(7, 0.1, 0.1, 0.275): 0.4
(7, 1.0, 1.0, 0.2): 0.533333333333
(7, 1.0, 1.0, 0.225): 0.533333333333
(7, 1.0, 1.0, 0.25): 0.4
(7, 1.0, 1.0, 0.275): 0.4
(7, 1.0, 1.0, 0.3): 0.666666666667
(7, 10.0, 10.0, 0.2): 0.533333333333
(7, 10.0, 10.0, 0.225): 0.6
(7, 10.0, 10.0, 0.25): 0.466666666667
(7, 10.0, 10.0, 0.275): 0.466666666667
(7, 10.0, 10.0, 0.3): 0.466666666667
(7, 100.0, 100.0, 0.2): 0.466666666667
(7, 100.0, 100.0, 0.225): 0.666666666667
(7, 100.0, 100.0, 0.25): 0.333333333333
(7, 100.0, 100.0, 0.275): 0.533333333333
(7, 100.0, 100.0, 0.3): 0.333333333333
(8, 1.0, 1.0, 0.2): 0.733333333333
(8, 1.0, 1.0, 0.225): 0.8
(8, 1.0, 1.0, 0.25): 0.733333333333
(8, 1.0, 1.0, 0.275): 0.733333333333
(8, 1.0, 1.0, 0.3): 0.466666666667
(8, 10.0, 10.0, 0.2): 0.733333333333
(8, 10.0, 10.0, 0.225): 0.8
(8, 10.0, 10.0, 0.25): 0.6
(8, 10.0, 10.0, 0.275): 0.6
(8, 10.0, 10.0, 0.3): 0.333333333333
(8, 100.0, 100.0, 0.2): 0.933333333333
(8, 100.0, 100.0, 0.225): 0.8
(8, 100.0, 100.0, 0.25): 0.533333333333
(8, 100.0, 100.0, 0.275): 0.6
(8, 100.0, 100.0, 0.3): 0.466666666667
(9, 1.0, 1.0, 0.2): 0.533333333333
(9, 1.0, 1.0, 0.25): 0.8
(9, 1.0, 1.0, 0.275): 0.6
(9, 1.0, 1.0, 0.3): 0.333333333333
(9, 10.0, 10.0, 0.2): 0.533333333333
(9, 10.0, 10.0, 0.25): 0.8
(9, 10.0, 10.0, 0.275): 0.6
(9, 10.0, 10.0, 0.3): 0.466666666667
(9, 100.0, 100.0, 0.2): 0.333333333333
(9, 100.0, 100.0, 0.225): 0.666666666667
(9, 100.0, 100.0, 0.25): 0.8
(9, 100.0, 100.0, 0.275): 0.733333333333
(9, 100.0, 100.0, 0.3): 0.333333333333

Accuracy mean :0.42121212121212126
Std deviation :0.1725296963717623
Loss mean :0.5787878787878787
Std deviation :0.17252969637176233



QuantileConstPiecewise_LinearMu_4dim, number of iterations: 10

Parameters info
Tradeoffs parameter: [  0.1   1.   10.  100. ]
Using same parameters for all the clusterization: True
Gaussian kernel sigmas: [0.2   0.225 0.25  0.275 0.3  ]
Number of principal dimension considerated: 4
Dataset length: 150

Clusterization info
Minimum size of a cluster: in perc: 0.01, in int: 2
Type of mu: Linear Muzzifier
Fuzzifier: QuantileConstPiecewise
Cluster graph uses all connections: False
Cluster to label info
Force the number of cluster to be the number of different labels: False
Force the labels to be always represent by a cluster: False

Validation info
Conflict resolution: random
Number of cluster considerated for validation (top_k): None


RESULTS

On TEST set

Accuracy on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 1.0, 0.3): 0.2
(1, 100.0, 100.0, 0.275): 0.466666666667
(2, 1.0, 1.0, 0.25): 0.466666666667
(3, 0.1, 0.1, 0.3): 0.266666666667
(4, 100.0, 100.0, 0.225): 0.266666666667
(5, 10.0, 10.0, 0.225): 0.4
(6, 10.0, 10.0, 0.225): 0.4
(7, 100.0, 100.0, 0.25): 0.4
(8, 10.0, 10.0, 0.3): 0.533333333333
(9, 1.0, 1.0, 0.225): 0.4

Loss on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 1.0, 0.3): 0.8
(1, 100.0, 100.0, 0.275): 0.533333333333
(2, 1.0, 1.0, 0.25): 0.533333333333
(3, 0.1, 0.1, 0.3): 0.733333333333
(4, 100.0, 100.0, 0.225): 0.733333333333
(5, 10.0, 10.0, 0.225): 0.6
(6, 10.0, 10.0, 0.225): 0.6
(7, 100.0, 100.0, 0.25): 0.6
(8, 10.0, 10.0, 0.3): 0.466666666667
(9, 1.0, 1.0, 0.225): 0.6

Accuracy mean :0.38
Std deviation :0.09910712498212337
Loss mean :0.62
Std deviation :0.09910712498212337

On TRAINING set

Accuracy on training set for every iteration (n_of_the_iter, c, sigma):
(0, 1.0, 1.0, 0.3): 0.348148148148
(1, 100.0, 100.0, 0.275): 0.637037037037
(2, 1.0, 1.0, 0.25): 0.437037037037
(3, 0.1, 0.1, 0.3): 0.562962962963
(4, 100.0, 100.0, 0.225): 0.585185185185
(5, 10.0, 10.0, 0.225): 0.518518518519
(6, 10.0, 10.0, 0.225): 0.481481481481
(7, 100.0, 100.0, 0.25): 0.622222222222
(8, 10.0, 10.0, 0.3): 0.637037037037
(9, 1.0, 1.0, 0.225): 0.414814814815
Loss on training set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 1.0, 0.3): 0.651851851852
(1, 100.0, 100.0, 0.275): 0.362962962963
(2, 1.0, 1.0, 0.25): 0.562962962963
(3, 0.1, 0.1, 0.3): 0.437037037037
(4, 100.0, 100.0, 0.225): 0.414814814815
(5, 10.0, 10.0, 0.225): 0.481481481481
(6, 10.0, 10.0, 0.225): 0.518518518519
(7, 100.0, 100.0, 0.25): 0.377777777778
(8, 10.0, 10.0, 0.3): 0.362962962963
(9, 1.0, 1.0, 0.225): 0.585185185185

Accuracy mean :0.5244444444444445
Std deviation :0.09628489961050028
Loss mean :0.4755555555555556
Std deviation :0.09628489961050027

On ALL the sets

Accuracy on all the set for every iteration, for every couple c,sigma (n_of_the_iter, c, sigma):
(0, 0.1, 0.1, 0.3): 0.466666666667
(0, 1.0, 1.0, 0.2): 0.4
(0, 1.0, 1.0, 0.225): 0.533333333333
(0, 1.0, 1.0, 0.25): 0.533333333333
(0, 1.0, 1.0, 0.275): 0.466666666667
(0, 1.0, 1.0, 0.3): 0.6
(0, 10.0, 10.0, 0.2): 0.4
(0, 10.0, 10.0, 0.225): 0.466666666667
(0, 10.0, 10.0, 0.25): 0.6
(0, 10.0, 10.0, 0.275): 0.6
(0, 10.0, 10.0, 0.3): 0.333333333333
(0, 100.0, 100.0, 0.225): 0.4
(0, 100.0, 100.0, 0.25): 0.533333333333
(0, 100.0, 100.0, 0.275): 0.466666666667
(0, 100.0, 100.0, 0.3): 0.466666666667
(1, 0.1, 0.1, 0.275): 0.6
(1, 0.1, 0.1, 0.3): 0.6
(1, 1.0, 1.0, 0.2): 0.6
(1, 1.0, 1.0, 0.225): 0.466666666667
(1, 1.0, 1.0, 0.25): 0.266666666667
(1, 1.0, 1.0, 0.275): 0.466666666667
(1, 1.0, 1.0, 0.3): 0.6
(1, 10.0, 10.0, 0.2): 0.533333333333
(1, 10.0, 10.0, 0.225): 0.6
(1, 10.0, 10.0, 0.25): 0.266666666667
(1, 10.0, 10.0, 0.275): 0.666666666667
(1, 10.0, 10.0, 0.3): 0.533333333333
(1, 100.0, 100.0, 0.2): 0.266666666667
(1, 100.0, 100.0, 0.225): 0.4
(1, 100.0, 100.0, 0.25): 0.266666666667
(1, 100.0, 100.0, 0.275): 0.666666666667
(1, 100.0, 100.0, 0.3): 0.533333333333
(2, 0.1, 0.1, 0.3): 0.533333333333
(2, 1.0, 1.0, 0.225): 0.2
(2, 1.0, 1.0, 0.25): 0.6
(2, 1.0, 1.0, 0.275): 0.4
(2, 1.0, 1.0, 0.3): 0.4
(2, 10.0, 10.0, 0.225): 0.2
(2, 10.0, 10.0, 0.25): 0.4
(2, 10.0, 10.0, 0.275): 0.466666666667
(2, 10.0, 10.0, 0.3): 0.4
(2, 100.0, 100.0, 0.225): 0.2
(2, 100.0, 100.0, 0.25): 0.533333333333
(2, 100.0, 100.0, 0.275): 0.466666666667
(2, 100.0, 100.0, 0.3): 0.533333333333
(3, 0.1, 0.1, 0.25): 0.266666666667
(3, 0.1, 0.1, 0.275): 0.2
(3, 0.1, 0.1, 0.3): 0.466666666667
(3, 1.0, 1.0, 0.2): 0.4
(3, 1.0, 1.0, 0.225): 0.266666666667
(3, 1.0, 1.0, 0.25): 0.333333333333
(3, 1.0, 1.0, 0.275): 0.4
(3, 1.0, 1.0, 0.3): 0.266666666667
(3, 10.0, 10.0, 0.2): 0.266666666667
(3, 10.0, 10.0, 0.225): 0.333333333333
(3, 10.0, 10.0, 0.25): 0.333333333333
(3, 10.0, 10.0, 0.275): 0.4
(3, 10.0, 10.0, 0.3): 0.333333333333
(3, 100.0, 100.0, 0.2): 0.0666666666667
(3, 100.0, 100.0, 0.225): 0.4
(3, 100.0, 100.0, 0.25): 0.2
(3, 100.0, 100.0, 0.275): 0.333333333333
(3, 100.0, 100.0, 0.3): 0.266666666667
(4, 0.1, 0.1, 0.3): 0.266666666667
(4, 1.0, 1.0, 0.2): 0.0666666666667
(4, 1.0, 1.0, 0.225): 0.266666666667
(4, 1.0, 1.0, 0.25): 0.333333333333
(4, 1.0, 1.0, 0.275): 0.266666666667
(4, 1.0, 1.0, 0.3): 0.333333333333
(4, 10.0, 10.0, 0.2): 0.133333333333
(4, 10.0, 10.0, 0.225): 0.266666666667
(4, 10.0, 10.0, 0.25): 0.333333333333
(4, 10.0, 10.0, 0.275): 0.333333333333
(4, 10.0, 10.0, 0.3): 0.333333333333
(4, 100.0, 100.0, 0.2): 0.4
(4, 100.0, 100.0, 0.225): 0.533333333333
(4, 100.0, 100.0, 0.25): 0.4
(4, 100.0, 100.0, 0.275): 0.4
(4, 100.0, 100.0, 0.3): 0.2
(5, 0.1, 0.1, 0.25): 0.4
(5, 0.1, 0.1, 0.3): 0.333333333333
(5, 1.0, 1.0, 0.2): 0.266666666667
(5, 1.0, 1.0, 0.225): 0.333333333333
(5, 1.0, 1.0, 0.25): 0.4
(5, 1.0, 1.0, 0.275): 0.333333333333
(5, 1.0, 1.0, 0.3): 0.333333333333
(5, 10.0, 10.0, 0.2): 0.133333333333
(5, 10.0, 10.0, 0.225): 0.466666666667
(5, 10.0, 10.0, 0.25): 0.266666666667
(5, 10.0, 10.0, 0.275): 0.333333333333
(5, 10.0, 10.0, 0.3): 0.333333333333
(5, 100.0, 100.0, 0.2): 0.133333333333
(5, 100.0, 100.0, 0.225): 0.266666666667
(5, 100.0, 100.0, 0.25): 0.2
(5, 100.0, 100.0, 0.275): 0.333333333333
(5, 100.0, 100.0, 0.3): 0.333333333333
(6, 0.1, 0.1, 0.275): 0.133333333333
(6, 0.1, 0.1, 0.3): 0.266666666667
(6, 1.0, 1.0, 0.225): 0.266666666667
(6, 1.0, 1.0, 0.25): 0.0666666666667
(6, 1.0, 1.0, 0.275): 0.2
(6, 1.0, 1.0, 0.3): 0.133333333333
(6, 10.0, 10.0, 0.2): 0.333333333333
(6, 10.0, 10.0, 0.225): 0.533333333333
(6, 10.0, 10.0, 0.25): 0.266666666667
(6, 10.0, 10.0, 0.275): 0.333333333333
(6, 10.0, 10.0, 0.3): 0.2
(6, 100.0, 100.0, 0.225): 0.266666666667
(6, 100.0, 100.0, 0.25): 0.2
(6, 100.0, 100.0, 0.275): 0.466666666667
(6, 100.0, 100.0, 0.3): 0.266666666667
(7, 0.1, 0.1, 0.3): 0.466666666667
(7, 1.0, 1.0, 0.225): 0.2
(7, 1.0, 1.0, 0.25): 0.333333333333
(7, 1.0, 1.0, 0.275): 0.2
(7, 1.0, 1.0, 0.3): 0.466666666667
(7, 10.0, 10.0, 0.2): 0.2
(7, 10.0, 10.0, 0.225): 0.4
(7, 10.0, 10.0, 0.25): 0.4
(7, 10.0, 10.0, 0.275): 0.2
(7, 10.0, 10.0, 0.3): 0.4
(7, 100.0, 100.0, 0.2): 0.2
(7, 100.0, 100.0, 0.225): 0.466666666667
(7, 100.0, 100.0, 0.25): 0.733333333333
(7, 100.0, 100.0, 0.275): 0.2
(7, 100.0, 100.0, 0.3): 0.533333333333
(8, 0.1, 0.1, 0.25): 0.4
(8, 0.1, 0.1, 0.275): 0.4
(8, 0.1, 0.1, 0.3): 0.4
(8, 1.0, 1.0, 0.225): 0.2
(8, 1.0, 1.0, 0.25): 0.333333333333
(8, 1.0, 1.0, 0.275): 0.533333333333
(8, 1.0, 1.0, 0.3): 0.533333333333
(8, 10.0, 10.0, 0.225): 0.2
(8, 10.0, 10.0, 0.25): 0.4
(8, 10.0, 10.0, 0.275): 0.466666666667
(8, 10.0, 10.0, 0.3): 0.6
(8, 100.0, 100.0, 0.225): 0.4
(8, 100.0, 100.0, 0.25): 0.4
(8, 100.0, 100.0, 0.275): 0.4
(8, 100.0, 100.0, 0.3): 0.4
(9, 0.1, 0.1, 0.275): 0.466666666667
(9, 0.1, 0.1, 0.3): 0.4
(9, 1.0, 1.0, 0.2): 0.4
(9, 1.0, 1.0, 0.225): 0.533333333333
(9, 1.0, 1.0, 0.25): 0.0666666666667
(9, 1.0, 1.0, 0.275): 0.266666666667
(9, 1.0, 1.0, 0.3): 0.466666666667
(9, 10.0, 10.0, 0.2): 0.133333333333
(9, 10.0, 10.0, 0.225): 0.133333333333
(9, 10.0, 10.0, 0.25): 0.4
(9, 10.0, 10.0, 0.275): 0.133333333333
(9, 10.0, 10.0, 0.3): 0.266666666667
(9, 100.0, 100.0, 0.2): 0.2
(9, 100.0, 100.0, 0.225): 0.333333333333
(9, 100.0, 100.0, 0.25): 0.266666666667
(9, 100.0, 100.0, 0.275): 0.333333333333
(9, 100.0, 100.0, 0.3): 0.466666666667

Loss on all the set for every iteration, for every couple c,sigma (n_of_the_iter, best_c, best_sigma):
(0, 0.1, 0.1, 0.3): 0.533333333333
(0, 1.0, 1.0, 0.2): 0.6
(0, 1.0, 1.0, 0.225): 0.466666666667
(0, 1.0, 1.0, 0.25): 0.466666666667
(0, 1.0, 1.0, 0.275): 0.533333333333
(0, 1.0, 1.0, 0.3): 0.4
(0, 10.0, 10.0, 0.2): 0.6
(0, 10.0, 10.0, 0.225): 0.533333333333
(0, 10.0, 10.0, 0.25): 0.4
(0, 10.0, 10.0, 0.275): 0.4
(0, 10.0, 10.0, 0.3): 0.666666666667
(0, 100.0, 100.0, 0.225): 0.6
(0, 100.0, 100.0, 0.25): 0.466666666667
(0, 100.0, 100.0, 0.275): 0.533333333333
(0, 100.0, 100.0, 0.3): 0.533333333333
(1, 0.1, 0.1, 0.275): 0.4
(1, 0.1, 0.1, 0.3): 0.4
(1, 1.0, 1.0, 0.2): 0.4
(1, 1.0, 1.0, 0.225): 0.533333333333
(1, 1.0, 1.0, 0.25): 0.733333333333
(1, 1.0, 1.0, 0.275): 0.533333333333
(1, 1.0, 1.0, 0.3): 0.4
(1, 10.0, 10.0, 0.2): 0.466666666667
(1, 10.0, 10.0, 0.225): 0.4
(1, 10.0, 10.0, 0.25): 0.733333333333
(1, 10.0, 10.0, 0.275): 0.333333333333
(1, 10.0, 10.0, 0.3): 0.466666666667
(1, 100.0, 100.0, 0.2): 0.733333333333
(1, 100.0, 100.0, 0.225): 0.6
(1, 100.0, 100.0, 0.25): 0.733333333333
(1, 100.0, 100.0, 0.275): 0.333333333333
(1, 100.0, 100.0, 0.3): 0.466666666667
(2, 0.1, 0.1, 0.3): 0.466666666667
(2, 1.0, 1.0, 0.225): 0.8
(2, 1.0, 1.0, 0.25): 0.4
(2, 1.0, 1.0, 0.275): 0.6
(2, 1.0, 1.0, 0.3): 0.6
(2, 10.0, 10.0, 0.225): 0.8
(2, 10.0, 10.0, 0.25): 0.6
(2, 10.0, 10.0, 0.275): 0.533333333333
(2, 10.0, 10.0, 0.3): 0.6
(2, 100.0, 100.0, 0.225): 0.8
(2, 100.0, 100.0, 0.25): 0.466666666667
(2, 100.0, 100.0, 0.275): 0.533333333333
(2, 100.0, 100.0, 0.3): 0.466666666667
(3, 0.1, 0.1, 0.25): 0.733333333333
(3, 0.1, 0.1, 0.275): 0.8
(3, 0.1, 0.1, 0.3): 0.533333333333
(3, 1.0, 1.0, 0.2): 0.6
(3, 1.0, 1.0, 0.225): 0.733333333333
(3, 1.0, 1.0, 0.25): 0.666666666667
(3, 1.0, 1.0, 0.275): 0.6
(3, 1.0, 1.0, 0.3): 0.733333333333
(3, 10.0, 10.0, 0.2): 0.733333333333
(3, 10.0, 10.0, 0.225): 0.666666666667
(3, 10.0, 10.0, 0.25): 0.666666666667
(3, 10.0, 10.0, 0.275): 0.6
(3, 10.0, 10.0, 0.3): 0.666666666667
(3, 100.0, 100.0, 0.2): 0.933333333333
(3, 100.0, 100.0, 0.225): 0.6
(3, 100.0, 100.0, 0.25): 0.8
(3, 100.0, 100.0, 0.275): 0.666666666667
(3, 100.0, 100.0, 0.3): 0.733333333333
(4, 0.1, 0.1, 0.3): 0.733333333333
(4, 1.0, 1.0, 0.2): 0.933333333333
(4, 1.0, 1.0, 0.225): 0.733333333333
(4, 1.0, 1.0, 0.25): 0.666666666667
(4, 1.0, 1.0, 0.275): 0.733333333333
(4, 1.0, 1.0, 0.3): 0.666666666667
(4, 10.0, 10.0, 0.2): 0.866666666667
(4, 10.0, 10.0, 0.225): 0.733333333333
(4, 10.0, 10.0, 0.25): 0.666666666667
(4, 10.0, 10.0, 0.275): 0.666666666667
(4, 10.0, 10.0, 0.3): 0.666666666667
(4, 100.0, 100.0, 0.2): 0.6
(4, 100.0, 100.0, 0.225): 0.466666666667
(4, 100.0, 100.0, 0.25): 0.6
(4, 100.0, 100.0, 0.275): 0.6
(4, 100.0, 100.0, 0.3): 0.8
(5, 0.1, 0.1, 0.25): 0.6
(5, 0.1, 0.1, 0.3): 0.666666666667
(5, 1.0, 1.0, 0.2): 0.733333333333
(5, 1.0, 1.0, 0.225): 0.666666666667
(5, 1.0, 1.0, 0.25): 0.6
(5, 1.0, 1.0, 0.275): 0.666666666667
(5, 1.0, 1.0, 0.3): 0.666666666667
(5, 10.0, 10.0, 0.2): 0.866666666667
(5, 10.0, 10.0, 0.225): 0.533333333333
(5, 10.0, 10.0, 0.25): 0.733333333333
(5, 10.0, 10.0, 0.275): 0.666666666667
(5, 10.0, 10.0, 0.3): 0.666666666667
(5, 100.0, 100.0, 0.2): 0.866666666667
(5, 100.0, 100.0, 0.225): 0.733333333333
(5, 100.0, 100.0, 0.25): 0.8
(5, 100.0, 100.0, 0.275): 0.666666666667
(5, 100.0, 100.0, 0.3): 0.666666666667
(6, 0.1, 0.1, 0.275): 0.866666666667
(6, 0.1, 0.1, 0.3): 0.733333333333
(6, 1.0, 1.0, 0.225): 0.733333333333
(6, 1.0, 1.0, 0.25): 0.933333333333
(6, 1.0, 1.0, 0.275): 0.8
(6, 1.0, 1.0, 0.3): 0.866666666667
(6, 10.0, 10.0, 0.2): 0.666666666667
(6, 10.0, 10.0, 0.225): 0.466666666667
(6, 10.0, 10.0, 0.25): 0.733333333333
(6, 10.0, 10.0, 0.275): 0.666666666667
(6, 10.0, 10.0, 0.3): 0.8
(6, 100.0, 100.0, 0.225): 0.733333333333
(6, 100.0, 100.0, 0.25): 0.8
(6, 100.0, 100.0, 0.275): 0.533333333333
(6, 100.0, 100.0, 0.3): 0.733333333333
(7, 0.1, 0.1, 0.3): 0.533333333333
(7, 1.0, 1.0, 0.225): 0.8
(7, 1.0, 1.0, 0.25): 0.666666666667
(7, 1.0, 1.0, 0.275): 0.8
(7, 1.0, 1.0, 0.3): 0.533333333333
(7, 10.0, 10.0, 0.2): 0.8
(7, 10.0, 10.0, 0.225): 0.6
(7, 10.0, 10.0, 0.25): 0.6
(7, 10.0, 10.0, 0.275): 0.8
(7, 10.0, 10.0, 0.3): 0.6
(7, 100.0, 100.0, 0.2): 0.8
(7, 100.0, 100.0, 0.225): 0.533333333333
(7, 100.0, 100.0, 0.25): 0.266666666667
(7, 100.0, 100.0, 0.275): 0.8
(7, 100.0, 100.0, 0.3): 0.466666666667
(8, 0.1, 0.1, 0.25): 0.6
(8, 0.1, 0.1, 0.275): 0.6
(8, 0.1, 0.1, 0.3): 0.6
(8, 1.0, 1.0, 0.225): 0.8
(8, 1.0, 1.0, 0.25): 0.666666666667
(8, 1.0, 1.0, 0.275): 0.466666666667
(8, 1.0, 1.0, 0.3): 0.466666666667
(8, 10.0, 10.0, 0.225): 0.8
(8, 10.0, 10.0, 0.25): 0.6
(8, 10.0, 10.0, 0.275): 0.533333333333
(8, 10.0, 10.0, 0.3): 0.4
(8, 100.0, 100.0, 0.225): 0.6
(8, 100.0, 100.0, 0.25): 0.6
(8, 100.0, 100.0, 0.275): 0.6
(8, 100.0, 100.0, 0.3): 0.6
(9, 0.1, 0.1, 0.275): 0.533333333333
(9, 0.1, 0.1, 0.3): 0.6
(9, 1.0, 1.0, 0.2): 0.6
(9, 1.0, 1.0, 0.225): 0.466666666667
(9, 1.0, 1.0, 0.25): 0.933333333333
(9, 1.0, 1.0, 0.275): 0.733333333333
(9, 1.0, 1.0, 0.3): 0.533333333333
(9, 10.0, 10.0, 0.2): 0.866666666667
(9, 10.0, 10.0, 0.225): 0.866666666667
(9, 10.0, 10.0, 0.25): 0.6
(9, 10.0, 10.0, 0.275): 0.866666666667
(9, 10.0, 10.0, 0.3): 0.733333333333
(9, 100.0, 100.0, 0.2): 0.8
(9, 100.0, 100.0, 0.225): 0.666666666667
(9, 100.0, 100.0, 0.25): 0.733333333333
(9, 100.0, 100.0, 0.275): 0.666666666667
(9, 100.0, 100.0, 0.3): 0.533333333333

Accuracy mean :0.3620253164556962
Std deviation :0.13979432120919677
Loss mean :0.6379746835443038
Std deviation :0.13979432120919677



QuantileLinPiecewise_LinearMu_2dim, number of iterations: 10

Parameters info
Tradeoffs parameter: [  0.1   1.   10.  100. ]
Using same parameters for all the clusterization: True
Gaussian kernel sigmas: [0.2   0.225 0.25  0.275 0.3  ]
Number of principal dimension considerated: 2
Dataset length: 150

Clusterization info
Minimum size of a cluster: in perc: 0.01, in int: 2
Type of mu: Linear Muzzifier
Fuzzifier: QuantileLinPiecewise
Cluster graph uses all connections: False
Cluster to label info
Force the number of cluster to be the number of different labels: False
Force the labels to be always represent by a cluster: False

Validation info
Conflict resolution: random
Number of cluster considerated for validation (top_k): None


RESULTS

On TEST set

Accuracy on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 1.0, 0.25): 0.866666666667
(1, 100.0, 100.0, 0.225): 0.933333333333
(2, 1.0, 1.0, 0.2): 0.4
(3, 1.0, 1.0, 0.2): 0.933333333333
(4, 1.0, 1.0, 0.2): 0.8
(5, 100.0, 100.0, 0.2): 0.733333333333
(6, 10.0, 10.0, 0.2): 0.6
(7, 1.0, 1.0, 0.2): 1.0
(8, 1.0, 1.0, 0.275): 0.733333333333
(9, 100.0, 100.0, 0.225): 0.866666666667

Loss on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 1.0, 0.25): 0.133333333333
(1, 100.0, 100.0, 0.225): 0.0666666666667
(2, 1.0, 1.0, 0.2): 0.6
(3, 1.0, 1.0, 0.2): 0.0666666666667
(4, 1.0, 1.0, 0.2): 0.2
(5, 100.0, 100.0, 0.2): 0.266666666667
(6, 10.0, 10.0, 0.2): 0.4
(7, 1.0, 1.0, 0.2): 0.0
(8, 1.0, 1.0, 0.275): 0.266666666667
(9, 100.0, 100.0, 0.225): 0.133333333333

Accuracy mean :0.7866666666666667
Std deviation :0.17074997966487598
Loss mean :0.21333333333333332
Std deviation :0.17074997966487596

On TRAINING set

Accuracy on training set for every iteration (n_of_the_iter, c, sigma):
(0, 1.0, 1.0, 0.25): 0.918518518519
(1, 100.0, 100.0, 0.225): 0.785185185185
(2, 1.0, 1.0, 0.2): 0.451851851852
(3, 1.0, 1.0, 0.2): 0.866666666667
(4, 1.0, 1.0, 0.2): 0.844444444444
(5, 100.0, 100.0, 0.2): 0.851851851852
(6, 10.0, 10.0, 0.2): 0.674074074074
(7, 1.0, 1.0, 0.2): 0.866666666667
(8, 1.0, 1.0, 0.275): 0.866666666667
(9, 100.0, 100.0, 0.225): 0.837037037037
Loss on training set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 1.0, 0.25): 0.0814814814815
(1, 100.0, 100.0, 0.225): 0.214814814815
(2, 1.0, 1.0, 0.2): 0.548148148148
(3, 1.0, 1.0, 0.2): 0.133333333333
(4, 1.0, 1.0, 0.2): 0.155555555556
(5, 100.0, 100.0, 0.2): 0.148148148148
(6, 10.0, 10.0, 0.2): 0.325925925926
(7, 1.0, 1.0, 0.2): 0.133333333333
(8, 1.0, 1.0, 0.275): 0.133333333333
(9, 100.0, 100.0, 0.225): 0.162962962963

Accuracy mean :0.7962962962962964
Std deviation :0.13059954604290094
Loss mean :0.20370370370370372
Std deviation :0.13059954604290094

On ALL the sets

Accuracy on all the set for every iteration, for every couple c,sigma (n_of_the_iter, c, sigma):
(0, 0.1, 0.1, 0.25): 0.733333333333
(0, 0.1, 0.1, 0.3): 0.4
(0, 1.0, 1.0, 0.2): 0.933333333333
(0, 1.0, 1.0, 0.225): 0.4
(0, 1.0, 1.0, 0.25): 0.933333333333
(0, 1.0, 1.0, 0.275): 0.8
(0, 1.0, 1.0, 0.3): 0.4
(0, 10.0, 10.0, 0.2): 0.733333333333
(0, 10.0, 10.0, 0.225): 0.866666666667
(0, 10.0, 10.0, 0.25): 0.933333333333
(0, 10.0, 10.0, 0.275): 0.733333333333
(0, 10.0, 10.0, 0.3): 0.733333333333
(0, 100.0, 100.0, 0.2): 0.933333333333
(0, 100.0, 100.0, 0.225): 0.866666666667
(0, 100.0, 100.0, 0.25): 0.866666666667
(0, 100.0, 100.0, 0.275): 0.8
(0, 100.0, 100.0, 0.3): 0.8
(1, 1.0, 1.0, 0.2): 0.666666666667
(1, 1.0, 1.0, 0.225): 0.666666666667
(1, 1.0, 1.0, 0.25): 0.666666666667
(1, 1.0, 1.0, 0.275): 0.666666666667
(1, 1.0, 1.0, 0.3): 0.666666666667
(1, 10.0, 10.0, 0.2): 0.6
(1, 10.0, 10.0, 0.225): 0.666666666667
(1, 10.0, 10.0, 0.25): 0.666666666667
(1, 10.0, 10.0, 0.275): 0.666666666667
(1, 10.0, 10.0, 0.3): 0.666666666667
(1, 100.0, 100.0, 0.2): 0.666666666667
(1, 100.0, 100.0, 0.225): 0.733333333333
(1, 100.0, 100.0, 0.25): 0.333333333333
(1, 100.0, 100.0, 0.275): 0.666666666667
(1, 100.0, 100.0, 0.3): 0.666666666667
(2, 1.0, 1.0, 0.2): 0.866666666667
(2, 1.0, 1.0, 0.225): 0.866666666667
(2, 1.0, 1.0, 0.25): 0.6
(2, 1.0, 1.0, 0.275): 0.6
(2, 1.0, 1.0, 0.3): 0.6
(2, 10.0, 10.0, 0.2): 0.733333333333
(2, 10.0, 10.0, 0.225): 0.733333333333
(2, 10.0, 10.0, 0.25): 0.6
(2, 10.0, 10.0, 0.275): 0.6
(2, 10.0, 10.0, 0.3): 0.6
(2, 100.0, 100.0, 0.2): 0.8
(2, 100.0, 100.0, 0.225): 0.8
(2, 100.0, 100.0, 0.25): 0.6
(2, 100.0, 100.0, 0.275): 0.6
(2, 100.0, 100.0, 0.3): 0.6
(3, 0.1, 0.1, 0.225): 0.733333333333
(3, 0.1, 0.1, 0.275): 0.666666666667
(3, 0.1, 0.1, 0.3): 0.266666666667
(3, 1.0, 1.0, 0.2): 0.866666666667
(3, 1.0, 1.0, 0.225): 0.666666666667
(3, 1.0, 1.0, 0.25): 0.733333333333
(3, 1.0, 1.0, 0.275): 0.666666666667
(3, 1.0, 1.0, 0.3): 0.666666666667
(3, 10.0, 10.0, 0.2): 0.733333333333
(3, 10.0, 10.0, 0.225): 0.666666666667
(3, 10.0, 10.0, 0.25): 0.666666666667
(3, 10.0, 10.0, 0.275): 0.666666666667
(3, 10.0, 10.0, 0.3): 0.266666666667
(3, 100.0, 100.0, 0.2): 0.4
(3, 100.0, 100.0, 0.225): 0.733333333333
(3, 100.0, 100.0, 0.25): 0.733333333333
(3, 100.0, 100.0, 0.275): 0.666666666667
(3, 100.0, 100.0, 0.3): 0.266666666667
(4, 0.1, 0.1, 0.275): 0.866666666667
(4, 1.0, 1.0, 0.2): 0.933333333333
(4, 1.0, 1.0, 0.225): 0.933333333333
(4, 1.0, 1.0, 0.25): 0.866666666667
(4, 1.0, 1.0, 0.275): 0.866666666667
(4, 1.0, 1.0, 0.3): 0.733333333333
(4, 10.0, 10.0, 0.2): 0.8
(4, 10.0, 10.0, 0.225): 0.8
(4, 10.0, 10.0, 0.25): 0.666666666667
(4, 10.0, 10.0, 0.275): 0.8
(4, 10.0, 10.0, 0.3): 0.733333333333
(4, 100.0, 100.0, 0.2): 0.8
(4, 100.0, 100.0, 0.225): 0.866666666667
(4, 100.0, 100.0, 0.25): 0.866666666667
(4, 100.0, 100.0, 0.275): 0.733333333333
(4, 100.0, 100.0, 0.3): 0.733333333333
(5, 0.1, 0.1, 0.3): 0.2
(5, 1.0, 1.0, 0.2): 0.933333333333
(5, 1.0, 1.0, 0.225): 0.866666666667
(5, 1.0, 1.0, 0.25): 0.866666666667
(5, 1.0, 1.0, 0.275): 0.533333333333
(5, 1.0, 1.0, 0.3): 0.2
(5, 10.0, 10.0, 0.2): 0.866666666667
(5, 10.0, 10.0, 0.225): 0.866666666667
(5, 10.0, 10.0, 0.25): 0.8
(5, 10.0, 10.0, 0.275): 0.6
(5, 10.0, 10.0, 0.3): 0.2
(5, 100.0, 100.0, 0.2): 1.0
(5, 100.0, 100.0, 0.225): 0.8
(5, 100.0, 100.0, 0.25): 0.733333333333
(5, 100.0, 100.0, 0.275): 0.533333333333
(5, 100.0, 100.0, 0.3): 0.8
(6, 1.0, 1.0, 0.2): 0.733333333333
(6, 1.0, 1.0, 0.225): 0.733333333333
(6, 1.0, 1.0, 0.25): 0.866666666667
(6, 1.0, 1.0, 0.275): 1.0
(6, 1.0, 1.0, 0.3): 0.666666666667
(6, 10.0, 10.0, 0.2): 1.0
(6, 10.0, 10.0, 0.225): 0.733333333333
(6, 10.0, 10.0, 0.25): 0.733333333333
(6, 10.0, 10.0, 0.275): 0.733333333333
(6, 10.0, 10.0, 0.3): 0.666666666667
(6, 100.0, 100.0, 0.2): 0.8
(6, 100.0, 100.0, 0.225): 0.733333333333
(6, 100.0, 100.0, 0.25): 0.733333333333
(6, 100.0, 100.0, 0.275): 0.866666666667
(6, 100.0, 100.0, 0.3): 0.666666666667
(7, 1.0, 1.0, 0.2): 0.866666666667
(7, 1.0, 1.0, 0.225): 0.6
(7, 1.0, 1.0, 0.25): 0.666666666667
(7, 1.0, 1.0, 0.275): 0.666666666667
(7, 1.0, 1.0, 0.3): 0.666666666667
(7, 10.0, 10.0, 0.2): 0.733333333333
(7, 10.0, 10.0, 0.225): 0.8
(7, 10.0, 10.0, 0.25): 0.666666666667
(7, 10.0, 10.0, 0.275): 0.666666666667
(7, 10.0, 10.0, 0.3): 0.533333333333
(7, 100.0, 100.0, 0.2): 0.8
(7, 100.0, 100.0, 0.225): 0.6
(7, 100.0, 100.0, 0.25): 0.666666666667
(7, 100.0, 100.0, 0.275): 0.666666666667
(7, 100.0, 100.0, 0.3): 0.666666666667
(8, 1.0, 1.0, 0.2): 0.933333333333
(8, 1.0, 1.0, 0.225): 0.666666666667
(8, 1.0, 1.0, 0.25): 0.8
(8, 1.0, 1.0, 0.275): 1.0
(8, 1.0, 1.0, 0.3): 0.866666666667
(8, 10.0, 10.0, 0.2): 0.933333333333
(8, 10.0, 10.0, 0.225): 0.866666666667
(8, 10.0, 10.0, 0.25): 0.8
(8, 10.0, 10.0, 0.275): 0.933333333333
(8, 10.0, 10.0, 0.3): 0.666666666667
(8, 100.0, 100.0, 0.2): 0.933333333333
(8, 100.0, 100.0, 0.225): 0.666666666667
(8, 100.0, 100.0, 0.25): 0.866666666667
(8, 100.0, 100.0, 0.275): 0.8
(8, 100.0, 100.0, 0.3): 0.666666666667
(9, 0.1, 0.1, 0.3): 0.666666666667
(9, 1.0, 1.0, 0.2): 0.666666666667
(9, 1.0, 1.0, 0.225): 0.6
(9, 1.0, 1.0, 0.25): 0.533333333333
(9, 1.0, 1.0, 0.275): 0.666666666667
(9, 1.0, 1.0, 0.3): 0.666666666667
(9, 10.0, 10.0, 0.2): 0.733333333333
(9, 10.0, 10.0, 0.225): 0.6
(9, 10.0, 10.0, 0.25): 0.666666666667
(9, 10.0, 10.0, 0.275): 0.666666666667
(9, 10.0, 10.0, 0.3): 0.666666666667
(9, 100.0, 100.0, 0.2): 0.666666666667
(9, 100.0, 100.0, 0.225): 0.8
(9, 100.0, 100.0, 0.25): 0.666666666667
(9, 100.0, 100.0, 0.275): 0.666666666667
(9, 100.0, 100.0, 0.3): 0.666666666667

Loss on all the set for every iteration, for every couple c,sigma (n_of_the_iter, best_c, best_sigma):
(0, 0.1, 0.1, 0.25): 0.266666666667
(0, 0.1, 0.1, 0.3): 0.6
(0, 1.0, 1.0, 0.2): 0.0666666666667
(0, 1.0, 1.0, 0.225): 0.6
(0, 1.0, 1.0, 0.25): 0.0666666666667
(0, 1.0, 1.0, 0.275): 0.2
(0, 1.0, 1.0, 0.3): 0.6
(0, 10.0, 10.0, 0.2): 0.266666666667
(0, 10.0, 10.0, 0.225): 0.133333333333
(0, 10.0, 10.0, 0.25): 0.0666666666667
(0, 10.0, 10.0, 0.275): 0.266666666667
(0, 10.0, 10.0, 0.3): 0.266666666667
(0, 100.0, 100.0, 0.2): 0.0666666666667
(0, 100.0, 100.0, 0.225): 0.133333333333
(0, 100.0, 100.0, 0.25): 0.133333333333
(0, 100.0, 100.0, 0.275): 0.2
(0, 100.0, 100.0, 0.3): 0.2
(1, 1.0, 1.0, 0.2): 0.333333333333
(1, 1.0, 1.0, 0.225): 0.333333333333
(1, 1.0, 1.0, 0.25): 0.333333333333
(1, 1.0, 1.0, 0.275): 0.333333333333
(1, 1.0, 1.0, 0.3): 0.333333333333
(1, 10.0, 10.0, 0.2): 0.4
(1, 10.0, 10.0, 0.225): 0.333333333333
(1, 10.0, 10.0, 0.25): 0.333333333333
(1, 10.0, 10.0, 0.275): 0.333333333333
(1, 10.0, 10.0, 0.3): 0.333333333333
(1, 100.0, 100.0, 0.2): 0.333333333333
(1, 100.0, 100.0, 0.225): 0.266666666667
(1, 100.0, 100.0, 0.25): 0.666666666667
(1, 100.0, 100.0, 0.275): 0.333333333333
(1, 100.0, 100.0, 0.3): 0.333333333333
(2, 1.0, 1.0, 0.2): 0.133333333333
(2, 1.0, 1.0, 0.225): 0.133333333333
(2, 1.0, 1.0, 0.25): 0.4
(2, 1.0, 1.0, 0.275): 0.4
(2, 1.0, 1.0, 0.3): 0.4
(2, 10.0, 10.0, 0.2): 0.266666666667
(2, 10.0, 10.0, 0.225): 0.266666666667
(2, 10.0, 10.0, 0.25): 0.4
(2, 10.0, 10.0, 0.275): 0.4
(2, 10.0, 10.0, 0.3): 0.4
(2, 100.0, 100.0, 0.2): 0.2
(2, 100.0, 100.0, 0.225): 0.2
(2, 100.0, 100.0, 0.25): 0.4
(2, 100.0, 100.0, 0.275): 0.4
(2, 100.0, 100.0, 0.3): 0.4
(3, 0.1, 0.1, 0.225): 0.266666666667
(3, 0.1, 0.1, 0.275): 0.333333333333
(3, 0.1, 0.1, 0.3): 0.733333333333
(3, 1.0, 1.0, 0.2): 0.133333333333
(3, 1.0, 1.0, 0.225): 0.333333333333
(3, 1.0, 1.0, 0.25): 0.266666666667
(3, 1.0, 1.0, 0.275): 0.333333333333
(3, 1.0, 1.0, 0.3): 0.333333333333
(3, 10.0, 10.0, 0.2): 0.266666666667
(3, 10.0, 10.0, 0.225): 0.333333333333
(3, 10.0, 10.0, 0.25): 0.333333333333
(3, 10.0, 10.0, 0.275): 0.333333333333
(3, 10.0, 10.0, 0.3): 0.733333333333
(3, 100.0, 100.0, 0.2): 0.6
(3, 100.0, 100.0, 0.225): 0.266666666667
(3, 100.0, 100.0, 0.25): 0.266666666667
(3, 100.0, 100.0, 0.275): 0.333333333333
(3, 100.0, 100.0, 0.3): 0.733333333333
(4, 0.1, 0.1, 0.275): 0.133333333333
(4, 1.0, 1.0, 0.2): 0.0666666666667
(4, 1.0, 1.0, 0.225): 0.0666666666667
(4, 1.0, 1.0, 0.25): 0.133333333333
(4, 1.0, 1.0, 0.275): 0.133333333333
(4, 1.0, 1.0, 0.3): 0.266666666667
(4, 10.0, 10.0, 0.2): 0.2
(4, 10.0, 10.0, 0.225): 0.2
(4, 10.0, 10.0, 0.25): 0.333333333333
(4, 10.0, 10.0, 0.275): 0.2
(4, 10.0, 10.0, 0.3): 0.266666666667
(4, 100.0, 100.0, 0.2): 0.2
(4, 100.0, 100.0, 0.225): 0.133333333333
(4, 100.0, 100.0, 0.25): 0.133333333333
(4, 100.0, 100.0, 0.275): 0.266666666667
(4, 100.0, 100.0, 0.3): 0.266666666667
(5, 0.1, 0.1, 0.3): 0.8
(5, 1.0, 1.0, 0.2): 0.0666666666667
(5, 1.0, 1.0, 0.225): 0.133333333333
(5, 1.0, 1.0, 0.25): 0.133333333333
(5, 1.0, 1.0, 0.275): 0.466666666667
(5, 1.0, 1.0, 0.3): 0.8
(5, 10.0, 10.0, 0.2): 0.133333333333
(5, 10.0, 10.0, 0.225): 0.133333333333
(5, 10.0, 10.0, 0.25): 0.2
(5, 10.0, 10.0, 0.275): 0.4
(5, 10.0, 10.0, 0.3): 0.8
(5, 100.0, 100.0, 0.2): 0.0
(5, 100.0, 100.0, 0.225): 0.2
(5, 100.0, 100.0, 0.25): 0.266666666667
(5, 100.0, 100.0, 0.275): 0.466666666667
(5, 100.0, 100.0, 0.3): 0.2
(6, 1.0, 1.0, 0.2): 0.266666666667
(6, 1.0, 1.0, 0.225): 0.266666666667
(6, 1.0, 1.0, 0.25): 0.133333333333
(6, 1.0, 1.0, 0.275): 0.0
(6, 1.0, 1.0, 0.3): 0.333333333333
(6, 10.0, 10.0, 0.2): 0.0
(6, 10.0, 10.0, 0.225): 0.266666666667
(6, 10.0, 10.0, 0.25): 0.266666666667
(6, 10.0, 10.0, 0.275): 0.266666666667
(6, 10.0, 10.0, 0.3): 0.333333333333
(6, 100.0, 100.0, 0.2): 0.2
(6, 100.0, 100.0, 0.225): 0.266666666667
(6, 100.0, 100.0, 0.25): 0.266666666667
(6, 100.0, 100.0, 0.275): 0.133333333333
(6, 100.0, 100.0, 0.3): 0.333333333333
(7, 1.0, 1.0, 0.2): 0.133333333333
(7, 1.0, 1.0, 0.225): 0.4
(7, 1.0, 1.0, 0.25): 0.333333333333
(7, 1.0, 1.0, 0.275): 0.333333333333
(7, 1.0, 1.0, 0.3): 0.333333333333
(7, 10.0, 10.0, 0.2): 0.266666666667
(7, 10.0, 10.0, 0.225): 0.2
(7, 10.0, 10.0, 0.25): 0.333333333333
(7, 10.0, 10.0, 0.275): 0.333333333333
(7, 10.0, 10.0, 0.3): 0.466666666667
(7, 100.0, 100.0, 0.2): 0.2
(7, 100.0, 100.0, 0.225): 0.4
(7, 100.0, 100.0, 0.25): 0.333333333333
(7, 100.0, 100.0, 0.275): 0.333333333333
(7, 100.0, 100.0, 0.3): 0.333333333333
(8, 1.0, 1.0, 0.2): 0.0666666666667
(8, 1.0, 1.0, 0.225): 0.333333333333
(8, 1.0, 1.0, 0.25): 0.2
(8, 1.0, 1.0, 0.275): 0.0
(8, 1.0, 1.0, 0.3): 0.133333333333
(8, 10.0, 10.0, 0.2): 0.0666666666667
(8, 10.0, 10.0, 0.225): 0.133333333333
(8, 10.0, 10.0, 0.25): 0.2
(8, 10.0, 10.0, 0.275): 0.0666666666667
(8, 10.0, 10.0, 0.3): 0.333333333333
(8, 100.0, 100.0, 0.2): 0.0666666666667
(8, 100.0, 100.0, 0.225): 0.333333333333
(8, 100.0, 100.0, 0.25): 0.133333333333
(8, 100.0, 100.0, 0.275): 0.2
(8, 100.0, 100.0, 0.3): 0.333333333333
(9, 0.1, 0.1, 0.3): 0.333333333333
(9, 1.0, 1.0, 0.2): 0.333333333333
(9, 1.0, 1.0, 0.225): 0.4
(9, 1.0, 1.0, 0.25): 0.466666666667
(9, 1.0, 1.0, 0.275): 0.333333333333
(9, 1.0, 1.0, 0.3): 0.333333333333
(9, 10.0, 10.0, 0.2): 0.266666666667
(9, 10.0, 10.0, 0.225): 0.4
(9, 10.0, 10.0, 0.25): 0.333333333333
(9, 10.0, 10.0, 0.275): 0.333333333333
(9, 10.0, 10.0, 0.3): 0.333333333333
(9, 100.0, 100.0, 0.2): 0.333333333333
(9, 100.0, 100.0, 0.225): 0.2
(9, 100.0, 100.0, 0.25): 0.333333333333
(9, 100.0, 100.0, 0.275): 0.333333333333
(9, 100.0, 100.0, 0.3): 0.333333333333

Accuracy mean :0.7122362869198312
Std deviation :0.1562731583009092
Loss mean :0.2877637130801688
Std deviation :0.1562731583009092



QuantileLinPiecewise_LinearMu_3dim, number of iterations: 10

Parameters info
Tradeoffs parameter: [  0.1   1.   10.  100. ]
Using same parameters for all the clusterization: True
Gaussian kernel sigmas: [0.2   0.225 0.25  0.275 0.3  ]
Number of principal dimension considerated: 3
Dataset length: 150

Clusterization info
Minimum size of a cluster: in perc: 0.01, in int: 2
Type of mu: Linear Muzzifier
Fuzzifier: QuantileLinPiecewise
Cluster graph uses all connections: False
Cluster to label info
Force the number of cluster to be the number of different labels: False
Force the labels to be always represent by a cluster: False

Validation info
Conflict resolution: random
Number of cluster considerated for validation (top_k): None


RESULTS

On TEST set

Accuracy on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 1.0, 0.25): 0.866666666667
(1, 100.0, 100.0, 0.2): 0.8
(2, 100.0, 100.0, 0.2): 0.866666666667
(3, 100.0, 100.0, 0.3): 0.6
(4, 1.0, 1.0, 0.275): 0.8
(5, 1.0, 1.0, 0.2): 0.2
(6, 1.0, 1.0, 0.25): 0.8
(7, 1.0, 1.0, 0.2): 0.733333333333
(8, 100.0, 100.0, 0.2): 1.0
(9, 100.0, 100.0, 0.2): 0.933333333333

Loss on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 1.0, 0.25): 0.133333333333
(1, 100.0, 100.0, 0.2): 0.2
(2, 100.0, 100.0, 0.2): 0.133333333333
(3, 100.0, 100.0, 0.3): 0.4
(4, 1.0, 1.0, 0.275): 0.2
(5, 1.0, 1.0, 0.2): 0.8
(6, 1.0, 1.0, 0.25): 0.2
(7, 1.0, 1.0, 0.2): 0.266666666667
(8, 100.0, 100.0, 0.2): 0.0
(9, 100.0, 100.0, 0.2): 0.0666666666667

Accuracy mean :0.76
Std deviation :0.21333333333333335
Loss mean :0.24000000000000005
Std deviation :0.21333333333333335

On TRAINING set

Accuracy on training set for every iteration (n_of_the_iter, c, sigma):
(0, 1.0, 1.0, 0.25): 0.82962962963
(1, 100.0, 100.0, 0.2): 0.881481481481
(2, 100.0, 100.0, 0.2): 0.911111111111
(3, 100.0, 100.0, 0.3): 0.674074074074
(4, 1.0, 1.0, 0.275): 0.651851851852
(5, 1.0, 1.0, 0.2): 0.348148148148
(6, 1.0, 1.0, 0.25): 0.940740740741
(7, 1.0, 1.0, 0.2): 0.918518518519
(8, 100.0, 100.0, 0.2): 0.918518518519
(9, 100.0, 100.0, 0.2): 0.859259259259
Loss on training set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 1.0, 0.25): 0.17037037037
(1, 100.0, 100.0, 0.2): 0.118518518519
(2, 100.0, 100.0, 0.2): 0.0888888888889
(3, 100.0, 100.0, 0.3): 0.325925925926
(4, 1.0, 1.0, 0.275): 0.348148148148
(5, 1.0, 1.0, 0.2): 0.651851851852
(6, 1.0, 1.0, 0.25): 0.0592592592593
(7, 1.0, 1.0, 0.2): 0.0814814814815
(8, 100.0, 100.0, 0.2): 0.0814814814815
(9, 100.0, 100.0, 0.2): 0.140740740741

Accuracy mean :0.7933333333333332
Std deviation :0.17686338919124553
Loss mean :0.2066666666666667
Std deviation :0.17686338919124556

On ALL the sets

Accuracy on all the set for every iteration, for every couple c,sigma (n_of_the_iter, c, sigma):
(0, 1.0, 1.0, 0.2): 0.2
(0, 1.0, 1.0, 0.225): 0.2
(0, 1.0, 1.0, 0.25): 0.933333333333
(0, 1.0, 1.0, 0.275): 0.866666666667
(0, 1.0, 1.0, 0.3): 0.8
(0, 10.0, 10.0, 0.2): 0.2
(0, 10.0, 10.0, 0.225): 0.2
(0, 10.0, 10.0, 0.25): 0.933333333333
(0, 10.0, 10.0, 0.275): 0.866666666667
(0, 10.0, 10.0, 0.3): 0.666666666667
(0, 100.0, 100.0, 0.2): 0.2
(0, 100.0, 100.0, 0.225): 0.2
(0, 100.0, 100.0, 0.25): 0.8
(0, 100.0, 100.0, 0.275): 0.8
(0, 100.0, 100.0, 0.3): 0.6
(1, 1.0, 1.0, 0.2): 1.0
(1, 1.0, 1.0, 0.225): 0.866666666667
(1, 1.0, 1.0, 0.25): 0.733333333333
(1, 1.0, 1.0, 0.275): 0.733333333333
(1, 1.0, 1.0, 0.3): 0.666666666667
(1, 10.0, 10.0, 0.2): 1.0
(1, 10.0, 10.0, 0.225): 0.933333333333
(1, 10.0, 10.0, 0.25): 0.733333333333
(1, 10.0, 10.0, 0.275): 0.733333333333
(1, 10.0, 10.0, 0.3): 0.6
(1, 100.0, 100.0, 0.2): 1.0
(1, 100.0, 100.0, 0.225): 0.933333333333
(1, 100.0, 100.0, 0.25): 0.733333333333
(1, 100.0, 100.0, 0.275): 0.733333333333
(1, 100.0, 100.0, 0.3): 0.533333333333
(2, 1.0, 1.0, 0.2): 0.733333333333
(2, 1.0, 1.0, 0.225): 0.266666666667
(2, 1.0, 1.0, 0.25): 0.866666666667
(2, 1.0, 1.0, 0.275): 0.6
(2, 1.0, 1.0, 0.3): 0.6
(2, 10.0, 10.0, 0.2): 0.8
(2, 10.0, 10.0, 0.225): 0.266666666667
(2, 10.0, 10.0, 0.25): 0.266666666667
(2, 10.0, 10.0, 0.275): 0.6
(2, 10.0, 10.0, 0.3): 0.6
(2, 100.0, 100.0, 0.2): 0.866666666667
(2, 100.0, 100.0, 0.225): 0.266666666667
(2, 100.0, 100.0, 0.25): 0.8
(2, 100.0, 100.0, 0.275): 0.6
(2, 100.0, 100.0, 0.3): 0.666666666667
(3, 1.0, 1.0, 0.2): 0.2
(3, 1.0, 1.0, 0.225): 0.2
(3, 1.0, 1.0, 0.25): 0.2
(3, 1.0, 1.0, 0.275): 0.533333333333
(3, 1.0, 1.0, 0.3): 0.533333333333
(3, 10.0, 10.0, 0.2): 0.2
(3, 10.0, 10.0, 0.225): 0.2
(3, 10.0, 10.0, 0.25): 0.533333333333
(3, 10.0, 10.0, 0.275): 0.533333333333
(3, 10.0, 10.0, 0.3): 0.533333333333
(3, 100.0, 100.0, 0.2): 0.2
(3, 100.0, 100.0, 0.225): 0.2
(3, 100.0, 100.0, 0.25): 0.533333333333
(3, 100.0, 100.0, 0.275): 0.533333333333
(3, 100.0, 100.0, 0.3): 0.666666666667
(4, 1.0, 1.0, 0.2): 0.333333333333
(4, 1.0, 1.0, 0.225): 0.866666666667
(4, 1.0, 1.0, 0.25): 0.933333333333
(4, 1.0, 1.0, 0.275): 0.933333333333
(4, 1.0, 1.0, 0.3): 0.866666666667
(4, 10.0, 10.0, 0.225): 0.8
(4, 10.0, 10.0, 0.25): 0.733333333333
(4, 10.0, 10.0, 0.275): 0.866666666667
(4, 10.0, 10.0, 0.3): 0.733333333333
(4, 100.0, 100.0, 0.225): 0.8
(4, 100.0, 100.0, 0.25): 0.8
(4, 100.0, 100.0, 0.275): 0.733333333333
(4, 100.0, 100.0, 0.3): 0.533333333333
(5, 1.0, 1.0, 0.2): 1.0
(5, 1.0, 1.0, 0.225): 1.0
(5, 1.0, 1.0, 0.25): 0.666666666667
(5, 1.0, 1.0, 0.275): 0.666666666667
(5, 1.0, 1.0, 0.3): 0.866666666667
(5, 10.0, 10.0, 0.2): 0.0666666666667
(5, 10.0, 10.0, 0.225): 1.0
(5, 10.0, 10.0, 0.25): 0.666666666667
(5, 10.0, 10.0, 0.275): 0.733333333333
(5, 10.0, 10.0, 0.3): 0.733333333333
(5, 100.0, 100.0, 0.2): 0.0666666666667
(5, 100.0, 100.0, 0.225): 0.333333333333
(5, 100.0, 100.0, 0.25): 0.666666666667
(5, 100.0, 100.0, 0.275): 0.0666666666667
(5, 100.0, 100.0, 0.3): 0.933333333333
(6, 0.1, 0.1, 0.275): 1.0
(6, 0.1, 0.1, 0.3): 1.0
(6, 1.0, 1.0, 0.2): 0.266666666667
(6, 1.0, 1.0, 0.225): 0.266666666667
(6, 1.0, 1.0, 0.25): 1.0
(6, 1.0, 1.0, 0.275): 0.4
(6, 1.0, 1.0, 0.3): 0.866666666667
(6, 10.0, 10.0, 0.2): 0.266666666667
(6, 10.0, 10.0, 0.225): 0.933333333333
(6, 10.0, 10.0, 0.25): 1.0
(6, 10.0, 10.0, 0.275): 0.933333333333
(6, 10.0, 10.0, 0.3): 0.8
(6, 100.0, 100.0, 0.2): 0.266666666667
(6, 100.0, 100.0, 0.225): 0.266666666667
(6, 100.0, 100.0, 0.25): 0.866666666667
(6, 100.0, 100.0, 0.275): 0.866666666667
(6, 100.0, 100.0, 0.3): 0.866666666667
(7, 0.1, 0.1, 0.3): 0.266666666667
(7, 1.0, 1.0, 0.2): 1.0
(7, 1.0, 1.0, 0.225): 1.0
(7, 1.0, 1.0, 0.25): 0.266666666667
(7, 1.0, 1.0, 0.275): 1.0
(7, 1.0, 1.0, 0.3): 0.266666666667
(7, 10.0, 10.0, 0.2): 1.0
(7, 10.0, 10.0, 0.225): 1.0
(7, 10.0, 10.0, 0.25): 0.933333333333
(7, 10.0, 10.0, 0.275): 0.8
(7, 10.0, 10.0, 0.3): 0.266666666667
(7, 100.0, 100.0, 0.2): 0.933333333333
(7, 100.0, 100.0, 0.225): 1.0
(7, 100.0, 100.0, 0.25): 0.8
(7, 100.0, 100.0, 0.275): 0.866666666667
(7, 100.0, 100.0, 0.3): 0.266666666667
(8, 1.0, 1.0, 0.2): 0.866666666667
(8, 1.0, 1.0, 0.225): 0.733333333333
(8, 1.0, 1.0, 0.25): 0.533333333333
(8, 1.0, 1.0, 0.275): 0.533333333333
(8, 1.0, 1.0, 0.3): 0.8
(8, 10.0, 10.0, 0.2): 0.866666666667
(8, 10.0, 10.0, 0.225): 0.866666666667
(8, 10.0, 10.0, 0.25): 0.866666666667
(8, 10.0, 10.0, 0.275): 0.466666666667
(8, 10.0, 10.0, 0.3): 0.733333333333
(8, 100.0, 100.0, 0.2): 0.933333333333
(8, 100.0, 100.0, 0.225): 0.733333333333
(8, 100.0, 100.0, 0.25): 0.533333333333
(8, 100.0, 100.0, 0.275): 0.533333333333
(8, 100.0, 100.0, 0.3): 0.8
(9, 1.0, 1.0, 0.2): 0.8
(9, 1.0, 1.0, 0.225): 0.8
(9, 1.0, 1.0, 0.25): 0.666666666667
(9, 1.0, 1.0, 0.275): 0.866666666667
(9, 1.0, 1.0, 0.3): 0.666666666667
(9, 10.0, 10.0, 0.2): 0.866666666667
(9, 10.0, 10.0, 0.225): 0.8
(9, 10.0, 10.0, 0.25): 0.733333333333
(9, 10.0, 10.0, 0.275): 0.8
(9, 10.0, 10.0, 0.3): 0.8
(9, 100.0, 100.0, 0.2): 0.933333333333
(9, 100.0, 100.0, 0.225): 0.866666666667
(9, 100.0, 100.0, 0.25): 0.733333333333
(9, 100.0, 100.0, 0.275): 0.666666666667
(9, 100.0, 100.0, 0.3): 0.8

Loss on all the set for every iteration, for every couple c,sigma (n_of_the_iter, best_c, best_sigma):
(0, 1.0, 1.0, 0.2): 0.8
(0, 1.0, 1.0, 0.225): 0.8
(0, 1.0, 1.0, 0.25): 0.0666666666667
(0, 1.0, 1.0, 0.275): 0.133333333333
(0, 1.0, 1.0, 0.3): 0.2
(0, 10.0, 10.0, 0.2): 0.8
(0, 10.0, 10.0, 0.225): 0.8
(0, 10.0, 10.0, 0.25): 0.0666666666667
(0, 10.0, 10.0, 0.275): 0.133333333333
(0, 10.0, 10.0, 0.3): 0.333333333333
(0, 100.0, 100.0, 0.2): 0.8
(0, 100.0, 100.0, 0.225): 0.8
(0, 100.0, 100.0, 0.25): 0.2
(0, 100.0, 100.0, 0.275): 0.2
(0, 100.0, 100.0, 0.3): 0.4
(1, 1.0, 1.0, 0.2): 0.0
(1, 1.0, 1.0, 0.225): 0.133333333333
(1, 1.0, 1.0, 0.25): 0.266666666667
(1, 1.0, 1.0, 0.275): 0.266666666667
(1, 1.0, 1.0, 0.3): 0.333333333333
(1, 10.0, 10.0, 0.2): 0.0
(1, 10.0, 10.0, 0.225): 0.0666666666667
(1, 10.0, 10.0, 0.25): 0.266666666667
(1, 10.0, 10.0, 0.275): 0.266666666667
(1, 10.0, 10.0, 0.3): 0.4
(1, 100.0, 100.0, 0.2): 0.0
(1, 100.0, 100.0, 0.225): 0.0666666666667
(1, 100.0, 100.0, 0.25): 0.266666666667
(1, 100.0, 100.0, 0.275): 0.266666666667
(1, 100.0, 100.0, 0.3): 0.466666666667
(2, 1.0, 1.0, 0.2): 0.266666666667
(2, 1.0, 1.0, 0.225): 0.733333333333
(2, 1.0, 1.0, 0.25): 0.133333333333
(2, 1.0, 1.0, 0.275): 0.4
(2, 1.0, 1.0, 0.3): 0.4
(2, 10.0, 10.0, 0.2): 0.2
(2, 10.0, 10.0, 0.225): 0.733333333333
(2, 10.0, 10.0, 0.25): 0.733333333333
(2, 10.0, 10.0, 0.275): 0.4
(2, 10.0, 10.0, 0.3): 0.4
(2, 100.0, 100.0, 0.2): 0.133333333333
(2, 100.0, 100.0, 0.225): 0.733333333333
(2, 100.0, 100.0, 0.25): 0.2
(2, 100.0, 100.0, 0.275): 0.4
(2, 100.0, 100.0, 0.3): 0.333333333333
(3, 1.0, 1.0, 0.2): 0.8
(3, 1.0, 1.0, 0.225): 0.8
(3, 1.0, 1.0, 0.25): 0.8
(3, 1.0, 1.0, 0.275): 0.466666666667
(3, 1.0, 1.0, 0.3): 0.466666666667
(3, 10.0, 10.0, 0.2): 0.8
(3, 10.0, 10.0, 0.225): 0.8
(3, 10.0, 10.0, 0.25): 0.466666666667
(3, 10.0, 10.0, 0.275): 0.466666666667
(3, 10.0, 10.0, 0.3): 0.466666666667
(3, 100.0, 100.0, 0.2): 0.8
(3, 100.0, 100.0, 0.225): 0.8
(3, 100.0, 100.0, 0.25): 0.466666666667
(3, 100.0, 100.0, 0.275): 0.466666666667
(3, 100.0, 100.0, 0.3): 0.333333333333
(4, 1.0, 1.0, 0.2): 0.666666666667
(4, 1.0, 1.0, 0.225): 0.133333333333
(4, 1.0, 1.0, 0.25): 0.0666666666667
(4, 1.0, 1.0, 0.275): 0.0666666666667
(4, 1.0, 1.0, 0.3): 0.133333333333
(4, 10.0, 10.0, 0.225): 0.2
(4, 10.0, 10.0, 0.25): 0.266666666667
(4, 10.0, 10.0, 0.275): 0.133333333333
(4, 10.0, 10.0, 0.3): 0.266666666667
(4, 100.0, 100.0, 0.225): 0.2
(4, 100.0, 100.0, 0.25): 0.2
(4, 100.0, 100.0, 0.275): 0.266666666667
(4, 100.0, 100.0, 0.3): 0.466666666667
(5, 1.0, 1.0, 0.2): 0.0
(5, 1.0, 1.0, 0.225): 0.0
(5, 1.0, 1.0, 0.25): 0.333333333333
(5, 1.0, 1.0, 0.275): 0.333333333333
(5, 1.0, 1.0, 0.3): 0.133333333333
(5, 10.0, 10.0, 0.2): 0.933333333333
(5, 10.0, 10.0, 0.225): 0.0
(5, 10.0, 10.0, 0.25): 0.333333333333
(5, 10.0, 10.0, 0.275): 0.266666666667
(5, 10.0, 10.0, 0.3): 0.266666666667
(5, 100.0, 100.0, 0.2): 0.933333333333
(5, 100.0, 100.0, 0.225): 0.666666666667
(5, 100.0, 100.0, 0.25): 0.333333333333
(5, 100.0, 100.0, 0.275): 0.933333333333
(5, 100.0, 100.0, 0.3): 0.0666666666667
(6, 0.1, 0.1, 0.275): 0.0
(6, 0.1, 0.1, 0.3): 0.0
(6, 1.0, 1.0, 0.2): 0.733333333333
(6, 1.0, 1.0, 0.225): 0.733333333333
(6, 1.0, 1.0, 0.25): 0.0
(6, 1.0, 1.0, 0.275): 0.6
(6, 1.0, 1.0, 0.3): 0.133333333333
(6, 10.0, 10.0, 0.2): 0.733333333333
(6, 10.0, 10.0, 0.225): 0.0666666666667
(6, 10.0, 10.0, 0.25): 0.0
(6, 10.0, 10.0, 0.275): 0.0666666666667
(6, 10.0, 10.0, 0.3): 0.2
(6, 100.0, 100.0, 0.2): 0.733333333333
(6, 100.0, 100.0, 0.225): 0.733333333333
(6, 100.0, 100.0, 0.25): 0.133333333333
(6, 100.0, 100.0, 0.275): 0.133333333333
(6, 100.0, 100.0, 0.3): 0.133333333333
(7, 0.1, 0.1, 0.3): 0.733333333333
(7, 1.0, 1.0, 0.2): 0.0
(7, 1.0, 1.0, 0.225): 0.0
(7, 1.0, 1.0, 0.25): 0.733333333333
(7, 1.0, 1.0, 0.275): 0.0
(7, 1.0, 1.0, 0.3): 0.733333333333
(7, 10.0, 10.0, 0.2): 0.0
(7, 10.0, 10.0, 0.225): 0.0
(7, 10.0, 10.0, 0.25): 0.0666666666667
(7, 10.0, 10.0, 0.275): 0.2
(7, 10.0, 10.0, 0.3): 0.733333333333
(7, 100.0, 100.0, 0.2): 0.0666666666667
(7, 100.0, 100.0, 0.225): 0.0
(7, 100.0, 100.0, 0.25): 0.2
(7, 100.0, 100.0, 0.275): 0.133333333333
(7, 100.0, 100.0, 0.3): 0.733333333333
(8, 1.0, 1.0, 0.2): 0.133333333333
(8, 1.0, 1.0, 0.225): 0.266666666667
(8, 1.0, 1.0, 0.25): 0.466666666667
(8, 1.0, 1.0, 0.275): 0.466666666667
(8, 1.0, 1.0, 0.3): 0.2
(8, 10.0, 10.0, 0.2): 0.133333333333
(8, 10.0, 10.0, 0.225): 0.133333333333
(8, 10.0, 10.0, 0.25): 0.133333333333
(8, 10.0, 10.0, 0.275): 0.533333333333
(8, 10.0, 10.0, 0.3): 0.266666666667
(8, 100.0, 100.0, 0.2): 0.0666666666667
(8, 100.0, 100.0, 0.225): 0.266666666667
(8, 100.0, 100.0, 0.25): 0.466666666667
(8, 100.0, 100.0, 0.275): 0.466666666667
(8, 100.0, 100.0, 0.3): 0.2
(9, 1.0, 1.0, 0.2): 0.2
(9, 1.0, 1.0, 0.225): 0.2
(9, 1.0, 1.0, 0.25): 0.333333333333
(9, 1.0, 1.0, 0.275): 0.133333333333
(9, 1.0, 1.0, 0.3): 0.333333333333
(9, 10.0, 10.0, 0.2): 0.133333333333
(9, 10.0, 10.0, 0.225): 0.2
(9, 10.0, 10.0, 0.25): 0.266666666667
(9, 10.0, 10.0, 0.275): 0.2
(9, 10.0, 10.0, 0.3): 0.2
(9, 100.0, 100.0, 0.2): 0.0666666666667
(9, 100.0, 100.0, 0.225): 0.133333333333
(9, 100.0, 100.0, 0.25): 0.266666666667
(9, 100.0, 100.0, 0.275): 0.333333333333
(9, 100.0, 100.0, 0.3): 0.2

Accuracy mean :0.6657836644591612
Std deviation :0.26622333512086777
Loss mean :0.33421633554083885
Std deviation :0.26622333512086777



QuantileLinPiecewise_LinearMu_4dim, number of iterations: 10

Parameters info
Tradeoffs parameter: [  0.1   1.   10.  100. ]
Using same parameters for all the clusterization: True
Gaussian kernel sigmas: [0.2   0.225 0.25  0.275 0.3  ]
Number of principal dimension considerated: 4
Dataset length: 150

Clusterization info
Minimum size of a cluster: in perc: 0.01, in int: 2
Type of mu: Linear Muzzifier
Fuzzifier: QuantileLinPiecewise
Cluster graph uses all connections: False
Cluster to label info
Force the number of cluster to be the number of different labels: False
Force the labels to be always represent by a cluster: False

Validation info
Conflict resolution: random
Number of cluster considerated for validation (top_k): None


RESULTS

On TEST set

Accuracy on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 100.0, 100.0, 0.3): 0.466666666667
(1, 100.0, 100.0, 0.25): 0.666666666667
(2, 1.0, 1.0, 0.25): 0.666666666667
(3, 100.0, 100.0, 0.225): 0.2
(4, 10.0, 10.0, 0.225): 0.733333333333
(5, 10.0, 10.0, 0.275): 0.933333333333
(6, 100.0, 100.0, 0.225): 0.8
(7, 0.1, 0.1, 0.275): 0.666666666667
(8, 10.0, 10.0, 0.225): 0.6
(9, 100.0, 100.0, 0.25): 0.8

Loss on test set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 100.0, 100.0, 0.3): 0.533333333333
(1, 100.0, 100.0, 0.25): 0.333333333333
(2, 1.0, 1.0, 0.25): 0.333333333333
(3, 100.0, 100.0, 0.225): 0.8
(4, 10.0, 10.0, 0.225): 0.266666666667
(5, 10.0, 10.0, 0.275): 0.0666666666667
(6, 100.0, 100.0, 0.225): 0.2
(7, 0.1, 0.1, 0.275): 0.333333333333
(8, 10.0, 10.0, 0.225): 0.4
(9, 100.0, 100.0, 0.25): 0.2

Accuracy mean :0.6533333333333334
Std deviation :0.19275776393067948
Loss mean :0.3466666666666667
Std deviation :0.1927577639306795

On TRAINING set

Accuracy on training set for every iteration (n_of_the_iter, c, sigma):
(0, 100.0, 100.0, 0.3): 0.688888888889
(1, 100.0, 100.0, 0.25): 0.666666666667
(2, 1.0, 1.0, 0.25): 0.874074074074
(3, 100.0, 100.0, 0.225): 0.348148148148
(4, 10.0, 10.0, 0.225): 0.659259259259
(5, 10.0, 10.0, 0.275): 0.807407407407
(6, 100.0, 100.0, 0.225): 0.77037037037
(7, 0.1, 0.1, 0.275): 0.666666666667
(8, 10.0, 10.0, 0.225): 0.674074074074
(9, 100.0, 100.0, 0.25): 0.666666666667
Loss on training set for every iteration (n_of_the_iter, best_c, best_sigma):
(0, 100.0, 100.0, 0.3): 0.311111111111
(1, 100.0, 100.0, 0.25): 0.333333333333
(2, 1.0, 1.0, 0.25): 0.125925925926
(3, 100.0, 100.0, 0.225): 0.651851851852
(4, 10.0, 10.0, 0.225): 0.340740740741
(5, 10.0, 10.0, 0.275): 0.192592592593
(6, 100.0, 100.0, 0.225): 0.22962962963
(7, 0.1, 0.1, 0.275): 0.333333333333
(8, 10.0, 10.0, 0.225): 0.325925925926
(9, 100.0, 100.0, 0.25): 0.333333333333

Accuracy mean :0.6822222222222223
Std deviation :0.13161235010282463
Loss mean :0.31777777777777777
Std deviation :0.13161235010282463

On ALL the sets

Accuracy on all the set for every iteration, for every couple c,sigma (n_of_the_iter, c, sigma):
(0, 0.1, 0.1, 0.25): 0.866666666667
(0, 0.1, 0.1, 0.275): 0.866666666667
(0, 1.0, 1.0, 0.225): 0.266666666667
(0, 1.0, 1.0, 0.25): 0.866666666667
(0, 1.0, 1.0, 0.275): 0.866666666667
(0, 1.0, 1.0, 0.3): 0.866666666667
(0, 10.0, 10.0, 0.225): 0.866666666667
(0, 10.0, 10.0, 0.25): 0.866666666667
(0, 10.0, 10.0, 0.275): 0.866666666667
(0, 10.0, 10.0, 0.3): 0.866666666667
(0, 100.0, 100.0, 0.225): 0.8
(0, 100.0, 100.0, 0.25): 0.866666666667
(0, 100.0, 100.0, 0.275): 0.866666666667
(0, 100.0, 100.0, 0.3): 0.866666666667
(1, 0.1, 0.1, 0.275): 0.466666666667
(1, 1.0, 1.0, 0.2): 0.466666666667
(1, 1.0, 1.0, 0.225): 0.666666666667
(1, 1.0, 1.0, 0.25): 0.8
(1, 1.0, 1.0, 0.275): 0.466666666667
(1, 1.0, 1.0, 0.3): 0.466666666667
(1, 10.0, 10.0, 0.2): 0.466666666667
(1, 10.0, 10.0, 0.225): 0.733333333333
(1, 10.0, 10.0, 0.25): 0.8
(1, 10.0, 10.0, 0.275): 0.466666666667
(1, 10.0, 10.0, 0.3): 0.466666666667
(1, 100.0, 100.0, 0.2): 0.333333333333
(1, 100.0, 100.0, 0.225): 0.733333333333
(1, 100.0, 100.0, 0.25): 0.8
(1, 100.0, 100.0, 0.275): 0.466666666667
(1, 100.0, 100.0, 0.3): 0.466666666667
(2, 0.1, 0.1, 0.275): 0.533333333333
(2, 0.1, 0.1, 0.3): 0.533333333333
(2, 1.0, 1.0, 0.2): 0.2
(2, 1.0, 1.0, 0.225): 0.733333333333
(2, 1.0, 1.0, 0.25): 0.866666666667
(2, 1.0, 1.0, 0.275): 0.533333333333
(2, 1.0, 1.0, 0.3): 0.533333333333
(2, 10.0, 10.0, 0.2): 0.2
(2, 10.0, 10.0, 0.225): 0.866666666667
(2, 10.0, 10.0, 0.25): 0.866666666667
(2, 10.0, 10.0, 0.275): 0.533333333333
(2, 10.0, 10.0, 0.3): 0.533333333333
(2, 100.0, 100.0, 0.2): 0.2
(2, 100.0, 100.0, 0.225): 0.866666666667
(2, 100.0, 100.0, 0.25): 0.6
(2, 100.0, 100.0, 0.275): 0.533333333333
(2, 100.0, 100.0, 0.3): 0.533333333333
(3, 0.1, 0.1, 0.25): 0.866666666667
(3, 0.1, 0.1, 0.275): 0.866666666667
(3, 0.1, 0.1, 0.3): 0.266666666667
(3, 1.0, 1.0, 0.2): 0.866666666667
(3, 1.0, 1.0, 0.225): 0.933333333333
(3, 1.0, 1.0, 0.25): 0.866666666667
(3, 1.0, 1.0, 0.275): 0.866666666667
(3, 1.0, 1.0, 0.3): 0.866666666667
(3, 10.0, 10.0, 0.2): 0.866666666667
(3, 10.0, 10.0, 0.225): 0.266666666667
(3, 10.0, 10.0, 0.25): 0.866666666667
(3, 10.0, 10.0, 0.275): 0.866666666667
(3, 10.0, 10.0, 0.3): 0.266666666667
(3, 100.0, 100.0, 0.2): 0.866666666667
(3, 100.0, 100.0, 0.225): 1.0
(3, 100.0, 100.0, 0.25): 0.933333333333
(3, 100.0, 100.0, 0.275): 0.866666666667
(3, 100.0, 100.0, 0.3): 0.266666666667
(4, 0.1, 0.1, 0.275): 0.733333333333
(4, 1.0, 1.0, 0.2): 0.133333333333
(4, 1.0, 1.0, 0.225): 0.8
(4, 1.0, 1.0, 0.25): 0.733333333333
(4, 1.0, 1.0, 0.275): 0.733333333333
(4, 1.0, 1.0, 0.3): 0.133333333333
(4, 10.0, 10.0, 0.2): 0.733333333333
(4, 10.0, 10.0, 0.225): 0.933333333333
(4, 10.0, 10.0, 0.25): 0.733333333333
(4, 10.0, 10.0, 0.275): 0.733333333333
(4, 10.0, 10.0, 0.3): 0.133333333333
(4, 100.0, 100.0, 0.225): 0.133333333333
(4, 100.0, 100.0, 0.25): 0.733333333333
(4, 100.0, 100.0, 0.275): 0.266666666667
(4, 100.0, 100.0, 0.3): 0.133333333333
(5, 0.1, 0.1, 0.25): 0.6
(5, 0.1, 0.1, 0.3): 0.6
(5, 1.0, 1.0, 0.2): 0.6
(5, 1.0, 1.0, 0.225): 0.8
(5, 1.0, 1.0, 0.25): 0.6
(5, 1.0, 1.0, 0.275): 1.0
(5, 1.0, 1.0, 0.3): 0.933333333333
(5, 10.0, 10.0, 0.2): 0.6
(5, 10.0, 10.0, 0.225): 0.8
(5, 10.0, 10.0, 0.25): 0.6
(5, 10.0, 10.0, 0.275): 1.0
(5, 10.0, 10.0, 0.3): 0.6
(5, 100.0, 100.0, 0.2): 0.533333333333
(5, 100.0, 100.0, 0.225): 0.8
(5, 100.0, 100.0, 0.25): 0.6
(5, 100.0, 100.0, 0.275): 0.333333333333
(5, 100.0, 100.0, 0.3): 0.6
(6, 0.1, 0.1, 0.275): 0.666666666667
(6, 1.0, 1.0, 0.2): 0.133333333333
(6, 1.0, 1.0, 0.225): 0.933333333333
(6, 1.0, 1.0, 0.25): 0.133333333333
(6, 1.0, 1.0, 0.275): 0.666666666667
(6, 1.0, 1.0, 0.3): 0.666666666667
(6, 10.0, 10.0, 0.2): 0.133333333333
(6, 10.0, 10.0, 0.225): 0.933333333333
(6, 10.0, 10.0, 0.25): 0.933333333333
(6, 10.0, 10.0, 0.275): 0.666666666667
(6, 10.0, 10.0, 0.3): 0.133333333333
(6, 100.0, 100.0, 0.2): 0.133333333333
(6, 100.0, 100.0, 0.225): 1.0
(6, 100.0, 100.0, 0.25): 0.666666666667
(6, 100.0, 100.0, 0.275): 0.666666666667
(6, 100.0, 100.0, 0.3): 0.666666666667
(7, 0.1, 0.1, 0.25): 0.533333333333
(7, 0.1, 0.1, 0.275): 0.533333333333
(7, 0.1, 0.1, 0.3): 0.533333333333
(7, 1.0, 1.0, 0.2): 0.466666666667
(7, 1.0, 1.0, 0.225): 0.2
(7, 1.0, 1.0, 0.25): 0.533333333333
(7, 1.0, 1.0, 0.275): 0.533333333333
(7, 1.0, 1.0, 0.3): 0.533333333333
(7, 10.0, 10.0, 0.2): 0.533333333333
(7, 10.0, 10.0, 0.225): 0.2
(7, 10.0, 10.0, 0.25): 0.533333333333
(7, 10.0, 10.0, 0.275): 0.533333333333
(7, 10.0, 10.0, 0.3): 0.533333333333
(7, 100.0, 100.0, 0.2): 0.466666666667
(7, 100.0, 100.0, 0.225): 0.2
(7, 100.0, 100.0, 0.25): 0.533333333333
(7, 100.0, 100.0, 0.275): 0.533333333333
(7, 100.0, 100.0, 0.3): 0.533333333333
(8, 0.1, 0.1, 0.25): 0.6
(8, 0.1, 0.1, 0.275): 0.6
(8, 1.0, 1.0, 0.2): 0.2
(8, 1.0, 1.0, 0.225): 0.333333333333
(8, 1.0, 1.0, 0.25): 0.6
(8, 1.0, 1.0, 0.275): 0.6
(8, 1.0, 1.0, 0.3): 0.333333333333
(8, 10.0, 10.0, 0.2): 0.266666666667
(8, 10.0, 10.0, 0.225): 0.733333333333
(8, 10.0, 10.0, 0.25): 0.6
(8, 10.0, 10.0, 0.275): 0.6
(8, 10.0, 10.0, 0.3): 0.333333333333
(8, 100.0, 100.0, 0.2): 0.333333333333
(8, 100.0, 100.0, 0.225): 0.733333333333
(8, 100.0, 100.0, 0.25): 0.6
(8, 100.0, 100.0, 0.275): 0.6
(8, 100.0, 100.0, 0.3): 0.6
(9, 1.0, 1.0, 0.2): 0.0666666666667
(9, 1.0, 1.0, 0.225): 0.8
(9, 1.0, 1.0, 0.25): 0.8
(9, 1.0, 1.0, 0.275): 0.266666666667
(9, 1.0, 1.0, 0.3): 0.333333333333
(9, 10.0, 10.0, 0.2): 0.266666666667
(9, 10.0, 10.0, 0.225): 0.8
(9, 10.0, 10.0, 0.25): 0.8
(9, 10.0, 10.0, 0.275): 0.8
(9, 10.0, 10.0, 0.3): 0.266666666667
(9, 100.0, 100.0, 0.225): 0.8
(9, 100.0, 100.0, 0.25): 0.866666666667
(9, 100.0, 100.0, 0.275): 0.266666666667
(9, 100.0, 100.0, 0.3): 0.266666666667

Loss on all the set for every iteration, for every couple c,sigma (n_of_the_iter, best_c, best_sigma):
(0, 0.1, 0.1, 0.25): 0.133333333333
(0, 0.1, 0.1, 0.275): 0.133333333333
(0, 1.0, 1.0, 0.225): 0.733333333333
(0, 1.0, 1.0, 0.25): 0.133333333333
(0, 1.0, 1.0, 0.275): 0.133333333333
(0, 1.0, 1.0, 0.3): 0.133333333333
(0, 10.0, 10.0, 0.225): 0.133333333333
(0, 10.0, 10.0, 0.25): 0.133333333333
(0, 10.0, 10.0, 0.275): 0.133333333333
(0, 10.0, 10.0, 0.3): 0.133333333333
(0, 100.0, 100.0, 0.225): 0.2
(0, 100.0, 100.0, 0.25): 0.133333333333
(0, 100.0, 100.0, 0.275): 0.133333333333
(0, 100.0, 100.0, 0.3): 0.133333333333
(1, 0.1, 0.1, 0.275): 0.533333333333
(1, 1.0, 1.0, 0.2): 0.533333333333
(1, 1.0, 1.0, 0.225): 0.333333333333
(1, 1.0, 1.0, 0.25): 0.2
(1, 1.0, 1.0, 0.275): 0.533333333333
(1, 1.0, 1.0, 0.3): 0.533333333333
(1, 10.0, 10.0, 0.2): 0.533333333333
(1, 10.0, 10.0, 0.225): 0.266666666667
(1, 10.0, 10.0, 0.25): 0.2
(1, 10.0, 10.0, 0.275): 0.533333333333
(1, 10.0, 10.0, 0.3): 0.533333333333
(1, 100.0, 100.0, 0.2): 0.666666666667
(1, 100.0, 100.0, 0.225): 0.266666666667
(1, 100.0, 100.0, 0.25): 0.2
(1, 100.0, 100.0, 0.275): 0.533333333333
(1, 100.0, 100.0, 0.3): 0.533333333333
(2, 0.1, 0.1, 0.275): 0.466666666667
(2, 0.1, 0.1, 0.3): 0.466666666667
(2, 1.0, 1.0, 0.2): 0.8
(2, 1.0, 1.0, 0.225): 0.266666666667
(2, 1.0, 1.0, 0.25): 0.133333333333
(2, 1.0, 1.0, 0.275): 0.466666666667
(2, 1.0, 1.0, 0.3): 0.466666666667
(2, 10.0, 10.0, 0.2): 0.8
(2, 10.0, 10.0, 0.225): 0.133333333333
(2, 10.0, 10.0, 0.25): 0.133333333333
(2, 10.0, 10.0, 0.275): 0.466666666667
(2, 10.0, 10.0, 0.3): 0.466666666667
(2, 100.0, 100.0, 0.2): 0.8
(2, 100.0, 100.0, 0.225): 0.133333333333
(2, 100.0, 100.0, 0.25): 0.4
(2, 100.0, 100.0, 0.275): 0.466666666667
(2, 100.0, 100.0, 0.3): 0.466666666667
(3, 0.1, 0.1, 0.25): 0.133333333333
(3, 0.1, 0.1, 0.275): 0.133333333333
(3, 0.1, 0.1, 0.3): 0.733333333333
(3, 1.0, 1.0, 0.2): 0.133333333333
(3, 1.0, 1.0, 0.225): 0.0666666666667
(3, 1.0, 1.0, 0.25): 0.133333333333
(3, 1.0, 1.0, 0.275): 0.133333333333
(3, 1.0, 1.0, 0.3): 0.133333333333
(3, 10.0, 10.0, 0.2): 0.133333333333
(3, 10.0, 10.0, 0.225): 0.733333333333
(3, 10.0, 10.0, 0.25): 0.133333333333
(3, 10.0, 10.0, 0.275): 0.133333333333
(3, 10.0, 10.0, 0.3): 0.733333333333
(3, 100.0, 100.0, 0.2): 0.133333333333
(3, 100.0, 100.0, 0.225): 0.0
(3, 100.0, 100.0, 0.25): 0.0666666666667
(3, 100.0, 100.0, 0.275): 0.133333333333
(3, 100.0, 100.0, 0.3): 0.733333333333
(4, 0.1, 0.1, 0.275): 0.266666666667
(4, 1.0, 1.0, 0.2): 0.866666666667
(4, 1.0, 1.0, 0.225): 0.2
(4, 1.0, 1.0, 0.25): 0.266666666667
(4, 1.0, 1.0, 0.275): 0.266666666667
(4, 1.0, 1.0, 0.3): 0.866666666667
(4, 10.0, 10.0, 0.2): 0.266666666667
(4, 10.0, 10.0, 0.225): 0.0666666666667
(4, 10.0, 10.0, 0.25): 0.266666666667
(4, 10.0, 10.0, 0.275): 0.266666666667
(4, 10.0, 10.0, 0.3): 0.866666666667
(4, 100.0, 100.0, 0.225): 0.866666666667
(4, 100.0, 100.0, 0.25): 0.266666666667
(4, 100.0, 100.0, 0.275): 0.733333333333
(4, 100.0, 100.0, 0.3): 0.866666666667
(5, 0.1, 0.1, 0.25): 0.4
(5, 0.1, 0.1, 0.3): 0.4
(5, 1.0, 1.0, 0.2): 0.4
(5, 1.0, 1.0, 0.225): 0.2
(5, 1.0, 1.0, 0.25): 0.4
(5, 1.0, 1.0, 0.275): 0.0
(5, 1.0, 1.0, 0.3): 0.0666666666667
(5, 10.0, 10.0, 0.2): 0.4
(5, 10.0, 10.0, 0.225): 0.2
(5, 10.0, 10.0, 0.25): 0.4
(5, 10.0, 10.0, 0.275): 0.0
(5, 10.0, 10.0, 0.3): 0.4
(5, 100.0, 100.0, 0.2): 0.466666666667
(5, 100.0, 100.0, 0.225): 0.2
(5, 100.0, 100.0, 0.25): 0.4
(5, 100.0, 100.0, 0.275): 0.666666666667
(5, 100.0, 100.0, 0.3): 0.4
(6, 0.1, 0.1, 0.275): 0.333333333333
(6, 1.0, 1.0, 0.2): 0.866666666667
(6, 1.0, 1.0, 0.225): 0.0666666666667
(6, 1.0, 1.0, 0.25): 0.866666666667
(6, 1.0, 1.0, 0.275): 0.333333333333
(6, 1.0, 1.0, 0.3): 0.333333333333
(6, 10.0, 10.0, 0.2): 0.866666666667
(6, 10.0, 10.0, 0.225): 0.0666666666667
(6, 10.0, 10.0, 0.25): 0.0666666666667
(6, 10.0, 10.0, 0.275): 0.333333333333
(6, 10.0, 10.0, 0.3): 0.866666666667
(6, 100.0, 100.0, 0.2): 0.866666666667
(6, 100.0, 100.0, 0.225): 0.0
(6, 100.0, 100.0, 0.25): 0.333333333333
(6, 100.0, 100.0, 0.275): 0.333333333333
(6, 100.0, 100.0, 0.3): 0.333333333333
(7, 0.1, 0.1, 0.25): 0.466666666667
(7, 0.1, 0.1, 0.275): 0.466666666667
(7, 0.1, 0.1, 0.3): 0.466666666667
(7, 1.0, 1.0, 0.2): 0.533333333333
(7, 1.0, 1.0, 0.225): 0.8
(7, 1.0, 1.0, 0.25): 0.466666666667
(7, 1.0, 1.0, 0.275): 0.466666666667
(7, 1.0, 1.0, 0.3): 0.466666666667
(7, 10.0, 10.0, 0.2): 0.466666666667
(7, 10.0, 10.0, 0.225): 0.8
(7, 10.0, 10.0, 0.25): 0.466666666667
(7, 10.0, 10.0, 0.275): 0.466666666667
(7, 10.0, 10.0, 0.3): 0.466666666667
(7, 100.0, 100.0, 0.2): 0.533333333333
(7, 100.0, 100.0, 0.225): 0.8
(7, 100.0, 100.0, 0.25): 0.466666666667
(7, 100.0, 100.0, 0.275): 0.466666666667
(7, 100.0, 100.0, 0.3): 0.466666666667
(8, 0.1, 0.1, 0.25): 0.4
(8, 0.1, 0.1, 0.275): 0.4
(8, 1.0, 1.0, 0.2): 0.8
(8, 1.0, 1.0, 0.225): 0.666666666667
(8, 1.0, 1.0, 0.25): 0.4
(8, 1.0, 1.0, 0.275): 0.4
(8, 1.0, 1.0, 0.3): 0.666666666667
(8, 10.0, 10.0, 0.2): 0.733333333333
(8, 10.0, 10.0, 0.225): 0.266666666667
(8, 10.0, 10.0, 0.25): 0.4
(8, 10.0, 10.0, 0.275): 0.4
(8, 10.0, 10.0, 0.3): 0.666666666667
(8, 100.0, 100.0, 0.2): 0.666666666667
(8, 100.0, 100.0, 0.225): 0.266666666667
(8, 100.0, 100.0, 0.25): 0.4
(8, 100.0, 100.0, 0.275): 0.4
(8, 100.0, 100.0, 0.3): 0.4
(9, 1.0, 1.0, 0.2): 0.933333333333
(9, 1.0, 1.0, 0.225): 0.2
(9, 1.0, 1.0, 0.25): 0.2
(9, 1.0, 1.0, 0.275): 0.733333333333
(9, 1.0, 1.0, 0.3): 0.666666666667
(9, 10.0, 10.0, 0.2): 0.733333333333
(9, 10.0, 10.0, 0.225): 0.2
(9, 10.0, 10.0, 0.25): 0.2
(9, 10.0, 10.0, 0.275): 0.2
(9, 10.0, 10.0, 0.3): 0.733333333333
(9, 100.0, 100.0, 0.225): 0.2
(9, 100.0, 100.0, 0.25): 0.133333333333
(9, 100.0, 100.0, 0.275): 0.733333333333
(9, 100.0, 100.0, 0.3): 0.733333333333

Accuracy mean :0.597119341563786
Std deviation :0.24749485762427087
Loss mean :0.402880658436214
Std deviation :0.24749485762427087



